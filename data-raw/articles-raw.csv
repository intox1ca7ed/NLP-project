"Brand","GPU Model","Article Title","Article URL","Article Text"
"NVIDIA","RTX 5060","NVIDIA GeForce RTX 5060: All We Know So Far","https://www.xtremegaminerd.com/nvidia-geforce-rtx-5060/","NVIDIA’s Blackwell architecture is powering both the gaming and data center GPUs. While the data center Blackwell GPUs have already hit the market, it will still take time for all the GeForce RTX 50 GPUs to roll out in the market. In the RTX 50-series lineup, you will find 60-class cards similar to the previous generation and today we are taking a detailed look at the upcoming GeForce RTX 5060.
The GeForce RTX 5060 will be available in both Ti and non-Ti variants, and this particular post is about the latter. I have gathered all the early information about this GPU from various leaks/reports and for both the desktop and laptop variants. Here you will find all the crucial details about the RTX 5060, including specs, performance, pricing, and release date.NVIDIA Blackwell Architecture and GeForce RTX 5060The NVIDIA Blackwell is the latest architecture for the company’s mainstream gaming and data center GPUs. It’s powering chips like B200, which are being used in data centers for intensive workloads. The same architecture powers the mainstream desktop graphics cards, which are used for gaming, content creation, and other such operations where a GPU can speed up the process.Related ArticlesNVIDIA GeForce RTX 5060 Laptop Outperforms RTX 4070 Laptop GPU, Leaked Time Spy Score Appears OnlineJanuary 1, 20255 Best Graphics Cards for Intel Core i3 12100FJuly 27, 2023NVIDIA hasn’t revealed much about the architecture and we will have to wait till CES to know more about it. What we actually know is that the GPUs based on the Blackwell architecture will belong to the GeForce RTX 50 or RTX 5000 series. This succeeds the Ada RTX 40 lineup and will be NVIDIA’s best gaming GPUs till now.The Geforce RTX 5060 belongs to the budget or lower mid-range category, replacing the RTX 4060 from the previous lineup. The 60-class cards are generally offered for smooth 1080p and 1440p gaming performance. Since the card will be available on both desktop and laptop platforms, the specs will vary, which we will talk about in detail in the next section.SpecificationsThe GeForce RTX 5060 is reportedly going to utilize the GB206 GPU die and a PG151 board. No solid leak has been reported about the Cuda Core count or the core clocks. As far as other specs go, the RTX 5060 is expected to bring the same VRAM capacity as the RTX 4060 but a faster memory type. This means an 8GB GDDR7 memory configuration, which means the GPU is likely going with a 128-bit memory bus.With that in mind, the memory bandwidth will go up to 448 GB/s with 28 Gbps of memory speed, which is about 65% faster compared to 272 GB/s bandwidth on the RTX 4060. The PCI-E interface will also be upgraded from PCI-E 4.0 to PCI-E 5.0 and whether the GPU will bring an increased TDP or not is still unclear.The GeForce RTX 5060 Laptop GPU is expected to feature the same VRAM configuration but a nerfed GPU die. This means fewer Cuda Cores and also lower power consumption. Since, we don’t have solid leaks about the desktop GPU, the information on the RTX 5060 laptop GPU is pretty scarce.Right now almost every other RTX 50-series GPU is reported to be more power-hungry than their predecessors, which means we could see TDP higher than 115W on the desktop variant. We will update this page once newer reports about its specs emerge.Specs Summary:Architecture: BlackwellGPU: GB206Cuda Cores: TBDVRAM: 8GB GDDR6, 128-bitMemory Bandwidth: 448 GB/sBase Clock: TBDBoost Clock: TBDTDP: TBDPerformanceAs far as the performance goes, we expect the RTX 5060 to provide about 30% uplifts over the RTX 4060. Some leaks did appear recently which suggested that the RTX 5060 laptop GPU is 32% faster than the RTX 4060 laptop GPU and so will be the desktop version but this leak seems to be unreliable. However, going by the generational uplifts NVIDIA offers in every new GPU generation, 30% is what we expect.The RTX 5060 should be targeting 1080p ultra settings but should be able to play most games at 1440p with ultra graphics presets as well. With just 8GB of memory, the GPU would throttle performance in some titles but in case there is a 12GB variant, this could be solved. That said, the ray tracing performance is expected to be enhanced noticeably as well.With DLSS 4.0 support, the performance could be boosted even further but raster performance on native resolution will determine its position in the GPU hierarchy. We expect the RTX 5060 to go on par with the RTX 4070 or RX 7800 XT but will still be somehow inferior in various circumstances, particularly due to 8GB VRAM.Price and Release DateThe GeForce RTX 5060 could be priced between $299 and $349. This is based on the previous pricing trends of the 60-class GPUs. The RTX 4060 was launched at $299 and NVIDIA is known for increasing the price with every generation. As far as the release goes, the RTX 5060 is reportedly coming in the first quarter of 2025 but the exact month and date are unknown.GeForce RTX 5060: What to Expect?The Geforce RTX 5060 should be seen as an ideal GPU for 1080p gaming but could also deliver excellent performance at 1440p on ultra settings. If the GPU ever gets a 12GB variant, the performance will be significantly better than the predecessors. With 8GB of VRAM, it won’t stand a chance in some modern titles but should do well in the majority of the games.The RTX 5060 is expected to deliver a solid raster and RT performance but will still likely fall in the budget GPU category. To reach the mid-range category, the VRAM and memory bus should be increased and uncompromised 1440p gaming will be its ultimate test.
The NVIDIA Blackwell is the latest architecture for the company’s mainstream gaming and data center GPUs. It’s powering chips like B200, which are being used in data centers for intensive workloads. The same architecture powers the mainstream desktop graphics cards, which are used for gaming, content creation, and other such operations where a GPU can speed up the process.
NVIDIA hasn’t revealed much about the architecture and we will have to wait till CES to know more about it. What we actually know is that the GPUs based on the Blackwell architecture will belong to the GeForce RTX 50 or RTX 5000 series. This succeeds the Ada RTX 40 lineup and will be NVIDIA’s best gaming GPUs till now.The Geforce RTX 5060 belongs to the budget or lower mid-range category, replacing the RTX 4060 from the previous lineup. The 60-class cards are generally offered for smooth 1080p and 1440p gaming performance. Since the card will be available on both desktop and laptop platforms, the specs will vary, which we will talk about in detail in the next section.SpecificationsThe GeForce RTX 5060 is reportedly going to utilize the GB206 GPU die and a PG151 board. No solid leak has been reported about the Cuda Core count or the core clocks. As far as other specs go, the RTX 5060 is expected to bring the same VRAM capacity as the RTX 4060 but a faster memory type. This means an 8GB GDDR7 memory configuration, which means the GPU is likely going with a 128-bit memory bus.With that in mind, the memory bandwidth will go up to 448 GB/s with 28 Gbps of memory speed, which is about 65% faster compared to 272 GB/s bandwidth on the RTX 4060. The PCI-E interface will also be upgraded from PCI-E 4.0 to PCI-E 5.0 and whether the GPU will bring an increased TDP or not is still unclear.The GeForce RTX 5060 Laptop GPU is expected to feature the same VRAM configuration but a nerfed GPU die. This means fewer Cuda Cores and also lower power consumption. Since, we don’t have solid leaks about the desktop GPU, the information on the RTX 5060 laptop GPU is pretty scarce.Right now almost every other RTX 50-series GPU is reported to be more power-hungry than their predecessors, which means we could see TDP higher than 115W on the desktop variant. We will update this page once newer reports about its specs emerge.Specs Summary:Architecture: BlackwellGPU: GB206Cuda Cores: TBDVRAM: 8GB GDDR6, 128-bitMemory Bandwidth: 448 GB/sBase Clock: TBDBoost Clock: TBDTDP: TBDPerformanceAs far as the performance goes, we expect the RTX 5060 to provide about 30% uplifts over the RTX 4060. Some leaks did appear recently which suggested that the RTX 5060 laptop GPU is 32% faster than the RTX 4060 laptop GPU and so will be the desktop version but this leak seems to be unreliable. However, going by the generational uplifts NVIDIA offers in every new GPU generation, 30% is what we expect.The RTX 5060 should be targeting 1080p ultra settings but should be able to play most games at 1440p with ultra graphics presets as well. With just 8GB of memory, the GPU would throttle performance in some titles but in case there is a 12GB variant, this could be solved. That said, the ray tracing performance is expected to be enhanced noticeably as well.With DLSS 4.0 support, the performance could be boosted even further but raster performance on native resolution will determine its position in the GPU hierarchy. We expect the RTX 5060 to go on par with the RTX 4070 or RX 7800 XT but will still be somehow inferior in various circumstances, particularly due to 8GB VRAM.Price and Release DateThe GeForce RTX 5060 could be priced between $299 and $349. This is based on the previous pricing trends of the 60-class GPUs. The RTX 4060 was launched at $299 and NVIDIA is known for increasing the price with every generation. As far as the release goes, the RTX 5060 is reportedly coming in the first quarter of 2025 but the exact month and date are unknown.GeForce RTX 5060: What to Expect?The Geforce RTX 5060 should be seen as an ideal GPU for 1080p gaming but could also deliver excellent performance at 1440p on ultra settings. If the GPU ever gets a 12GB variant, the performance will be significantly better than the predecessors. With 8GB of VRAM, it won’t stand a chance in some modern titles but should do well in the majority of the games.The RTX 5060 is expected to deliver a solid raster and RT performance but will still likely fall in the budget GPU category. To reach the mid-range category, the VRAM and memory bus should be increased and uncompromised 1440p gaming will be its ultimate test.
The Geforce RTX 5060 belongs to the budget or lower mid-range category, replacing the RTX 4060 from the previous lineup. The 60-class cards are generally offered for smooth 1080p and 1440p gaming performance. Since the card will be available on both desktop and laptop platforms, the specs will vary, which we will talk about in detail in the next section.
The GeForce RTX 5060 is reportedly going to utilize the GB206 GPU die and a PG151 board. No solid leak has been reported about the Cuda Core count or the core clocks. As far as other specs go, the RTX 5060 is expected to bring the same VRAM capacity as the RTX 4060 but a faster memory type. This means an 8GB GDDR7 memory configuration, which means the GPU is likely going with a 128-bit memory bus.
With that in mind, the memory bandwidth will go up to 448 GB/s with 28 Gbps of memory speed, which is about 65% faster compared to 272 GB/s bandwidth on the RTX 4060. The PCI-E interface will also be upgraded from PCI-E 4.0 to PCI-E 5.0 and whether the GPU will bring an increased TDP or not is still unclear.
The GeForce RTX 5060 Laptop GPU is expected to feature the same VRAM configuration but a nerfed GPU die. This means fewer Cuda Cores and also lower power consumption. Since, we don’t have solid leaks about the desktop GPU, the information on the RTX 5060 laptop GPU is pretty scarce.Right now almost every other RTX 50-series GPU is reported to be more power-hungry than their predecessors, which means we could see TDP higher than 115W on the desktop variant. We will update this page once newer reports about its specs emerge.Specs Summary:Architecture: BlackwellGPU: GB206Cuda Cores: TBDVRAM: 8GB GDDR6, 128-bitMemory Bandwidth: 448 GB/sBase Clock: TBDBoost Clock: TBDTDP: TBDPerformanceAs far as the performance goes, we expect the RTX 5060 to provide about 30% uplifts over the RTX 4060. Some leaks did appear recently which suggested that the RTX 5060 laptop GPU is 32% faster than the RTX 4060 laptop GPU and so will be the desktop version but this leak seems to be unreliable. However, going by the generational uplifts NVIDIA offers in every new GPU generation, 30% is what we expect.The RTX 5060 should be targeting 1080p ultra settings but should be able to play most games at 1440p with ultra graphics presets as well. With just 8GB of memory, the GPU would throttle performance in some titles but in case there is a 12GB variant, this could be solved. That said, the ray tracing performance is expected to be enhanced noticeably as well.With DLSS 4.0 support, the performance could be boosted even further but raster performance on native resolution will determine its position in the GPU hierarchy. We expect the RTX 5060 to go on par with the RTX 4070 or RX 7800 XT but will still be somehow inferior in various circumstances, particularly due to 8GB VRAM.Price and Release DateThe GeForce RTX 5060 could be priced between $299 and $349. This is based on the previous pricing trends of the 60-class GPUs. The RTX 4060 was launched at $299 and NVIDIA is known for increasing the price with every generation. As far as the release goes, the RTX 5060 is reportedly coming in the first quarter of 2025 but the exact month and date are unknown.GeForce RTX 5060: What to Expect?The Geforce RTX 5060 should be seen as an ideal GPU for 1080p gaming but could also deliver excellent performance at 1440p on ultra settings. If the GPU ever gets a 12GB variant, the performance will be significantly better than the predecessors. With 8GB of VRAM, it won’t stand a chance in some modern titles but should do well in the majority of the games.The RTX 5060 is expected to deliver a solid raster and RT performance but will still likely fall in the budget GPU category. To reach the mid-range category, the VRAM and memory bus should be increased and uncompromised 1440p gaming will be its ultimate test.
Right now almost every other RTX 50-series GPU is reported to be more power-hungry than their predecessors, which means we could see TDP higher than 115W on the desktop variant. We will update this page once newer reports about its specs emerge.Specs Summary:Architecture: BlackwellGPU: GB206Cuda Cores: TBDVRAM: 8GB GDDR6, 128-bitMemory Bandwidth: 448 GB/sBase Clock: TBDBoost Clock: TBDTDP: TBDPerformanceAs far as the performance goes, we expect the RTX 5060 to provide about 30% uplifts over the RTX 4060. Some leaks did appear recently which suggested that the RTX 5060 laptop GPU is 32% faster than the RTX 4060 laptop GPU and so will be the desktop version but this leak seems to be unreliable. However, going by the generational uplifts NVIDIA offers in every new GPU generation, 30% is what we expect.The RTX 5060 should be targeting 1080p ultra settings but should be able to play most games at 1440p with ultra graphics presets as well. With just 8GB of memory, the GPU would throttle performance in some titles but in case there is a 12GB variant, this could be solved. That said, the ray tracing performance is expected to be enhanced noticeably as well.With DLSS 4.0 support, the performance could be boosted even further but raster performance on native resolution will determine its position in the GPU hierarchy. We expect the RTX 5060 to go on par with the RTX 4070 or RX 7800 XT but will still be somehow inferior in various circumstances, particularly due to 8GB VRAM.Price and Release DateThe GeForce RTX 5060 could be priced between $299 and $349. This is based on the previous pricing trends of the 60-class GPUs. The RTX 4060 was launched at $299 and NVIDIA is known for increasing the price with every generation. As far as the release goes, the RTX 5060 is reportedly coming in the first quarter of 2025 but the exact month and date are unknown.GeForce RTX 5060: What to Expect?The Geforce RTX 5060 should be seen as an ideal GPU for 1080p gaming but could also deliver excellent performance at 1440p on ultra settings. If the GPU ever gets a 12GB variant, the performance will be significantly better than the predecessors. With 8GB of VRAM, it won’t stand a chance in some modern titles but should do well in the majority of the games.The RTX 5060 is expected to deliver a solid raster and RT performance but will still likely fall in the budget GPU category. To reach the mid-range category, the VRAM and memory bus should be increased and uncompromised 1440p gaming will be its ultimate test.
Specs Summary:Architecture: BlackwellGPU: GB206Cuda Cores: TBDVRAM: 8GB GDDR6, 128-bitMemory Bandwidth: 448 GB/sBase Clock: TBDBoost Clock: TBDTDP: TBDPerformanceAs far as the performance goes, we expect the RTX 5060 to provide about 30% uplifts over the RTX 4060. Some leaks did appear recently which suggested that the RTX 5060 laptop GPU is 32% faster than the RTX 4060 laptop GPU and so will be the desktop version but this leak seems to be unreliable. However, going by the generational uplifts NVIDIA offers in every new GPU generation, 30% is what we expect.The RTX 5060 should be targeting 1080p ultra settings but should be able to play most games at 1440p with ultra graphics presets as well. With just 8GB of memory, the GPU would throttle performance in some titles but in case there is a 12GB variant, this could be solved. That said, the ray tracing performance is expected to be enhanced noticeably as well.With DLSS 4.0 support, the performance could be boosted even further but raster performance on native resolution will determine its position in the GPU hierarchy. We expect the RTX 5060 to go on par with the RTX 4070 or RX 7800 XT but will still be somehow inferior in various circumstances, particularly due to 8GB VRAM.Price and Release DateThe GeForce RTX 5060 could be priced between $299 and $349. This is based on the previous pricing trends of the 60-class GPUs. The RTX 4060 was launched at $299 and NVIDIA is known for increasing the price with every generation. As far as the release goes, the RTX 5060 is reportedly coming in the first quarter of 2025 but the exact month and date are unknown.GeForce RTX 5060: What to Expect?The Geforce RTX 5060 should be seen as an ideal GPU for 1080p gaming but could also deliver excellent performance at 1440p on ultra settings. If the GPU ever gets a 12GB variant, the performance will be significantly better than the predecessors. With 8GB of VRAM, it won’t stand a chance in some modern titles but should do well in the majority of the games.The RTX 5060 is expected to deliver a solid raster and RT performance but will still likely fall in the budget GPU category. To reach the mid-range category, the VRAM and memory bus should be increased and uncompromised 1440p gaming will be its ultimate test.
As far as the performance goes, we expect the RTX 5060 to provide about 30% uplifts over the RTX 4060. Some leaks did appear recently which suggested that the RTX 5060 laptop GPU is 32% faster than the RTX 4060 laptop GPU and so will be the desktop version but this leak seems to be unreliable. However, going by the generational uplifts NVIDIA offers in every new GPU generation, 30% is what we expect.
The RTX 5060 should be targeting 1080p ultra settings but should be able to play most games at 1440p with ultra graphics presets as well. With just 8GB of memory, the GPU would throttle performance in some titles but in case there is a 12GB variant, this could be solved. That said, the ray tracing performance is expected to be enhanced noticeably as well.With DLSS 4.0 support, the performance could be boosted even further but raster performance on native resolution will determine its position in the GPU hierarchy. We expect the RTX 5060 to go on par with the RTX 4070 or RX 7800 XT but will still be somehow inferior in various circumstances, particularly due to 8GB VRAM.Price and Release DateThe GeForce RTX 5060 could be priced between $299 and $349. This is based on the previous pricing trends of the 60-class GPUs. The RTX 4060 was launched at $299 and NVIDIA is known for increasing the price with every generation. As far as the release goes, the RTX 5060 is reportedly coming in the first quarter of 2025 but the exact month and date are unknown.GeForce RTX 5060: What to Expect?The Geforce RTX 5060 should be seen as an ideal GPU for 1080p gaming but could also deliver excellent performance at 1440p on ultra settings. If the GPU ever gets a 12GB variant, the performance will be significantly better than the predecessors. With 8GB of VRAM, it won’t stand a chance in some modern titles but should do well in the majority of the games.The RTX 5060 is expected to deliver a solid raster and RT performance but will still likely fall in the budget GPU category. To reach the mid-range category, the VRAM and memory bus should be increased and uncompromised 1440p gaming will be its ultimate test.
With DLSS 4.0 support, the performance could be boosted even further but raster performance on native resolution will determine its position in the GPU hierarchy. We expect the RTX 5060 to go on par with the RTX 4070 or RX 7800 XT but will still be somehow inferior in various circumstances, particularly due to 8GB VRAM.Price and Release DateThe GeForce RTX 5060 could be priced between $299 and $349. This is based on the previous pricing trends of the 60-class GPUs. The RTX 4060 was launched at $299 and NVIDIA is known for increasing the price with every generation. As far as the release goes, the RTX 5060 is reportedly coming in the first quarter of 2025 but the exact month and date are unknown.GeForce RTX 5060: What to Expect?The Geforce RTX 5060 should be seen as an ideal GPU for 1080p gaming but could also deliver excellent performance at 1440p on ultra settings. If the GPU ever gets a 12GB variant, the performance will be significantly better than the predecessors. With 8GB of VRAM, it won’t stand a chance in some modern titles but should do well in the majority of the games.The RTX 5060 is expected to deliver a solid raster and RT performance but will still likely fall in the budget GPU category. To reach the mid-range category, the VRAM and memory bus should be increased and uncompromised 1440p gaming will be its ultimate test.
The GeForce RTX 5060 could be priced between $299 and $349. This is based on the previous pricing trends of the 60-class GPUs. The RTX 4060 was launched at $299 and NVIDIA is known for increasing the price with every generation. As far as the release goes, the RTX 5060 is reportedly coming in the first quarter of 2025 but the exact month and date are unknown.
The Geforce RTX 5060 should be seen as an ideal GPU for 1080p gaming but could also deliver excellent performance at 1440p on ultra settings. If the GPU ever gets a 12GB variant, the performance will be significantly better than the predecessors. With 8GB of VRAM, it won’t stand a chance in some modern titles but should do well in the majority of the games.The RTX 5060 is expected to deliver a solid raster and RT performance but will still likely fall in the budget GPU category. To reach the mid-range category, the VRAM and memory bus should be increased and uncompromised 1440p gaming will be its ultimate test.
The RTX 5060 is expected to deliver a solid raster and RT performance but will still likely fall in the budget GPU category. To reach the mid-range category, the VRAM and memory bus should be increased and uncompromised 1440p gaming will be its ultimate test.
Your email address will not be published. Required fields are marked *
This site uses Akismet to reduce spam. Learn how your comment data is processed.
Xtremegaminerd.com is a participant in Amazon Associate Program and is supported by the readers. The qualifying purchase you make through our links may get us some commission and doesn't cost you an extra penny.
A publication dedicated to PC hardware, gaming, and general tech."
"NVIDIA","RTX 5060","NVIDIA RTX 5060 - Everything We Know So Far","https://www.overclockers.co.uk/blog/nvidia-rtx-5060-everything-we-know-so-far/",""
"NVIDIA","RTX 5060","Nvidia GeForce RTX 5060 Laptop Performance Preview","https://www.notebookcheck.net/Nvidia-GeForce-RTX-5060-Laptop-Performance-Preview-Almost-at-the-level-of-the-RTX-5070-Laptop.1023497.0.html",""
"NVIDIA","RTX 5060","Nvidia confirms RTX 5060 black screens and compatibility issues with certain motherboards","https://www.neowin.net/news/nvidia-confirms-rtx-5060-black-screens-and-compatibility-issues-with-certain-motherboards/","Taras Buria

Neowin
@TarasBuria        ·
    

    May 24, 2025 05:46 EDT
    

with 23 comments


The recent launch of the RTX 5060 was quite controversial, with independent media accusing the company of ""rug pulling"" its new budget graphics card by launching it during Computex and withholding the review driver. Add to that user discontent about 8GB of video memory (AMD has a few words about that) and multi-frame generation, high prices, and overall, quite a disastrous state of the recent driver releases. Not the way it's meant to be played...
Now, we have yet another issue with the RTX 5060 graphics card. This time, Nvidia confirmed compatibility issues with certain motherboards, which cause black screens on reboot. The company says this happens with ""legacy motherboards,"" and a firmware update is required to resolve the problem. Nvidia published a support page on its official website with detailed steps to perform if your system experiences black screens during reboots.
Nvidia suggests doing one of the following steps to mitigate the issue:
After booting to the desktop, you can download the Nvidia GPU UEFI Firmware Update Tool and get your RTX 5060 to the latest firmware with the necessary fixes. This applies to the RTX 5060 and the RTX 5060 Ti. Nvidia adds that the firmware update is only required on systems that exhibit black screen issues on reboots. There is no need to update GPU firmware if your system restarts without issues.
You can find all the details and links to the firmware update tool in a post on the official Nvidia support website.


Taras Buria
                                                ·
                    
Apr 25, 2025

with
10
                            comment
                                s
                                                            





Taras Buria
                                                ·
                    
Apr 15, 2025

with
6
                            comment
                                s
                                                            





Sayan Sen
                                                ·
                    
Apr 15, 2025

with
0
                            comment
                                s
                                                            





Taras Buria
                                                ·
                    
Mar 12, 2025

with
14
                            comment
                                s
                                                            



Login or Sign Up to read and post a comment.
Please enter your reason for reporting this comment.

                                software
                                

                                sharge icemag 2
                                

                                keychron k2 he
                                

                                engwe l20 3.0 boost
                                

                                maono wave t5
                                

                                build 2025
                                

                                quntis  light bar
                                

                        windows 11 unamused
                        

                        teamgroup
                        

                        windows 11
                        

                        bob and brad m7 plus massager gun
                        

© Since 2000 Neowin®
                                    All trademarks mentioned are the property of their respective owners."
"NVIDIA","RTX 5060","Nvidia's RTX 5060 Review Debacle Should Be a Wake-Up Call","https://tech.slashdot.org/story/25/05/22/2025241/nvidias-rtx-5060-review-debacle-should-be-a-wake-up-call","Follow Slashdot blog updates by subscribing to our blog RSS feed


The Fine Print: The following comments are owned by whoever posted them.  We are not responsible for them in any way.

A 3080 is superior to a 4070 and DEFINITELY a 4060... but NVidia doesn't enable half the goodies inside the 4XXX driver's DLSS set (despite the 3XXX series being fully capable of frame generation and other artificially ""restricted"" functions). The only reason the 4070 and 4060 look ""better on paper"" is the fact that the drivers have been sabotaged for the 3XXX and 2XXX.


And you can see this clearly when you enable FSR on a 3XXX and 2XXX card.


NVidia is going to piss away their fanbase at this rate.
They'll piss away their fan(atics) base, but they'll continue to maintain the dominant marketshare unless Intel continues to make big strides in their drivers and is willing to take a few more years of deficit spending for their GPU section.
The GamersNexus video ""NVIDIA's Dirty Manipulation of Reviews""  https://www.youtube.com/watch?... [youtube.com]  (mentioned in TFA) is def worth watching.
Nvidia are on top of the world ... and instead of just doing what originally got them there... they're going the enshitification route.  (in graphics cards, anyway). ... i heart marketing.
Nvidia are on top of the world ... and instead of just doing what originally got them there... they're going the enshitification route. (in graphics cards, anyway). ... i heart marketing.
Why? Gaming graphics cards are a drop in the bucket for their revenues these days. They really don't care - they can drop all the GPUs on the market today and there would only be a slight blip on the revenue. Their main profit center is compute cards they sell for AI and other purposes.
That's why they don't care there's a GPU shortage, or they're basically handing GPUs to AMD. They have a loyal fanbase who will buy the 5090 GTX TI SUPER OC OMGSWTFBBQ edition for $10,000. (Heck, Asus I think is selling a gold plated ""Dubai Edition"" or something)
Exactly. NVidia has a long history of cheating. Everybody knows they cannot be trusted.
None of that matters as long as they dominate the GPGPU market. Gaming is a side hustle now.
NVidia is going to piss away their fanbase at this rate.Huh? Why would AI companies care about this? - Snide comment aside NVIDIA has absolutely fucked over budget gamers. The fact that they are still fans at all is mindblowing. NVIDIA has one gaming market left - the mid-high end where they lack competition. The low-end gamers have been pissed away a while back.
NVidia is going to piss away their fanbase at this rate.
Huh? Why would AI companies care about this? - Snide comment aside NVIDIA has absolutely fucked over budget gamers. The fact that they are still fans at all is mindblowing. NVIDIA has one gaming market left - the mid-high end where they lack competition. The low-end gamers have been pissed away a while back.
Just do the reviews after the cards and drivers are released, and buyers can wait until they see the reviews. I don't think Nvidia can prevent that.
People would like to have reviews on launch day, so they can know on the day the cards are available whether they are worth buying, because they tend to sell out and they want to know if they should try to get one.
If the manufacturer is going to fail to produce enough units for initial demand, they ought to at least make sure the devices get reviewed before release, instead of doing exactly the opposite and making sure they don't.
If you know how to make people behave rationally, please let us know.
The ""previews"" described above were one issue, that the only press coverage out there was giving nVidia's exact test suite and basically parroting their cherrypicked bullshit numbers.
The related issue is that the sites that agreed to do previews could use the same drivers to prepare their own reviews in advance of launch, while nobody else had drivers, so that at launch they would have the only reviews out there. It's nice that Hardware Unboxed managed to dash out a review the same day of release out of she 
That would end reviewers getting free shit.
It's not about reviewers ""getting free shit."" They need to get the cards (AND DRIVERS) in advance of the release date so that their reviews go live at a point in time when consumers actually care about the review. They also need access to technical support to get help when the buggy pre-release drivers don't work. The reviewers can easily afford to buy the stuff if they need to; they're running an actual business with employees and rent and all kinds of other costs. Having to buy a $100 case or a $500 GPU i 
for big enough reviewers (those that are more than one men show), paying for cards if needed is not a issue. When a video generates between 1k to 100k, paying for the card is still profit and you keep the card to compare it later on (when some free stuff you have to actually return it later on)
That's not even considering someone like LTT, who could by the card at retail, test it, scribble a sharpie signature on the fan shroud, and sell it for more than it was bought for - ignoring profit from the review itself.
Exactly!!free stuff is great when you are small, after a certain size, it doesn't matter anymore. More important is getting the card before other people, but that is a different battle!

""Buyers should wait instead of buying a product on release day"".
You can also know it's crap because it's got 8GB on it. 12GB is a practical minimum today, and 16GB is the new 8GB anyway.
Nvidia clearly doesn't even need this market, and the only reason we really need them in it is to keep AMD from raising their prices. I don't suggest that it would make sense for them to stop making consumer GPUs (it's not a small amount of income or profit to be made there) but they're clearly acting like they don't need us, and it's a weird time for that since we've never needed them less — in particular in this low end of the market where they are pulling the review sample/driver availability shenanigans.
I don't suggest that it would make sense for them to stop making consumer GPUs (it's not a small amount of income or profit to be made there) Looking at opportunity costs, each gaming GPU costs Nvidia thousands of dollars because that silicon could have been made into a data center chip that sells for 10-20x the price.
I don't suggest that it would make sense for them to stop making consumer GPUs (it's not a small amount of income or profit to be made there) 
Looking at opportunity costs, each gaming GPU costs Nvidia thousands of dollars because that silicon could have been made into a data center chip that sells for 10-20x the price.
That's assuming that they are cannibalizing sales of those devices. They can only sell so many of them.
That's assuming that they are cannibalizing sales of those devices. They can only sell so many of them.Only Nvidia knows for sure, but Jensen Huang has been known to say internally that data center demand is 2-3x current sales and that sales are mostly supply constrained (mostly due to CoWoS packaging). Even with increasing constraints on China sales, total sales continue to increase, which lends some credence to the large demand theory.
That's assuming that they are cannibalizing sales of those devices. They can only sell so many of them.
Only Nvidia knows for sure, but Jensen Huang has been known to say internally that data center demand is 2-3x current sales and that sales are mostly supply constrained (mostly due to CoWoS packaging). Even with increasing constraints on China sales, total sales continue to increase, which lends some credence to the large demand theory.
Sure, as long as you purposely disable swap on your system as you recommend, everything should be fine. /s
4k. Older titles are great at 4k even on a 4060 Ti 16GB, and with improved textures they can easily go past 8GB. Also, it's nice to just not have to care at all what else is running. There's no good reason to supply less than 12GB. I do use lower resolutions for some titles (Satisfactory with any kind of quality will punch this card in the nuts at 4k) but 4k displays are now pretty cheap and that's the direction people are going to be moving.
Also, if everybody is going to have AI everything (like it or not) 
Probably the ill begotten scheme of some cockamamie fascist wannabe middle manager, rather than a company wide policy.
I don't know, after having seeing how the CEO acts on stage during product reveals, I can believe the bullshitting and censoring has his full support, or was his idea.
It really warmed my heart to see the crowd's lackluster response to his captain america bullshit. It was a collective ""what?""
GamersNexus brought this up in their video (the one linked in the summary here) and from their own contacts say the decisions are coming from higher up, and when they try to get info on it NVidia tries to have a lower level/PR/marketting person handle answering any questions, essentially leaving the upper management a way to escape any responsibility by not being on record.
Demonizing a hypothetical ""cockamamie fascist wannabe middle manager"" is dancing to NVidia upper-management's tune.
Except it's happening globally, which suggests it's coming from the top.
The reviewers do massive amounts of work to run all these cards through the ridiculous amounts of tests. They REALLY put work in... but almost universally they also refuse to give any realistic time talking about the big selling point of the last two generations of cards: Frame Generation. Instead, they focus almost entirely on raster performance. They also focus heavily on the most modern and demanding of AAA games instead of the games that that budget buyers play-- LAST YEAR'S AAA games.
Nobody buying an RTX 4060 or RTX 5060 gives a damn about 4K performance with Ultra settings an Max Ray Tracing with all DLSS turned off. That segment doesn't even own 4K monitors. It's a completely unrealistic scenario. But not only do they do those tests for shits and giggles, but they FOCUS on them. Here's are test that the GPU manufacturers want done with their budget tier of cards:
1a. What texture quality do I need to set [THIS GAME] to at 1080p to get 70 FPS so that frame gen can work really well?1b. What does it look like before and after DLSS is enabled?
2a. What texture quality do I need to set [THIS GAME] to at 1440p to get 70 FPS so that frame gen can work really well?2b. What does it look like before and after DLSS is enabled?
Because that's EXACTLY what the budget segment does. They buy the card they can afford and they figure out how to make it work best for them.
Of course, the reviewers won't do that. Instead, they'll put the Honda Civic on the drag strip and complain about its 0-60 speed. They'll hook up 2-ton trailer and complain about towing capacity. What they WON'T do is test the card how it's intended to be used and provide good buying advice within the budget range.
Thats because frame gen is only any good if the raster performance is already good. If the card is running at 20fps, throwing in a bunch of extra frames makes it appear to be running more smoothly, but the actual experience is terrible.
And if raster frames are high enough for a good experience, then you don't need frame gen.
It's just not that useful in most scenarios.Sorry - last post incomplete :/
The reviewers do tend to use high settings, but most use 1080p & 1440p.
They also can't know what everyones setups are like, so try to isolate performance to the gpu only.
Then they put it in a chart with other cards.

They're not trying to optimize a game for getting to 70fps. They're showing the cards relative strength.
Thats because frame gen is only any good if the raster performance is already good. If the card is running at 20fps, throwing in a bunch of extra frames makes it appear to be running more smoothly, but the actual experience is terrible.
And if raster frames are high enough for a good experience, then you don't need frame gen.
It's just not that useful in most scenarios.
Sorry - last post incomplete :/
The reviewers do tend to use high settings, but most use 1080p & 1440p.
They also can't know what everyones setups are like, so try to isolate performance to the gpu only.
Then they put it in a chart with other cards.

They're not trying to optimize a game for getting to 70fps. They're showing the cards relative strength.
They also can't know what everyones setups are like, so try to isolate performance to the gpu only. Then they put it in a chart with other cards. They're not trying to optimize a game for getting to 70fps. They're showing the cards relative strength.And that's exactly the problem. They're reviewing what the card can do in extreme circumstances and basing their ""buy/don't buy"" advice based on that. Those extreme circumstances, though, are in no way demonstrative of how the card will be used and thus should not be used as the basis for advice.If you tested multiple commuter vehicles on a race track and critiqued them based on their ability to handle turns at 70mph, you would be providing zero benefit to those in the market for a commuter vehicle. And if 
They also can't know what everyones setups are like, so try to isolate performance to the gpu only. Then they put it in a chart with other cards. They're not trying to optimize a game for getting to 70fps. They're showing the cards relative strength.
And that's exactly the problem. They're reviewing what the card can do in extreme circumstances and basing their ""buy/don't buy"" advice based on that. Those extreme circumstances, though, are in no way demonstrative of how the card will be used and thus should not be used as the basis for advice.
If you tested multiple commuter vehicles on a race track and critiqued them based on their ability to handle turns at 70mph, you would be providing zero benefit to those in the market for a commuter vehicle. And if 
Thats because frame gen is only any good if the raster performance is already good.Which is exactly why I gave the suggested methodology of...What texture quality do I need to set [THIS GAME] to at 1080p to get 70 FPS so that frame gen can work really well?
Thats because frame gen is only any good if the raster performance is already good.
Which is exactly why I gave the suggested methodology of...
What texture quality do I need to set [THIS GAME] to at 1080p to get 70 FPS so that frame gen can work really well?
What texture quality do I need to set [THIS GAME] to at 1080p to get 70 FPS so that frame gen can work really well?
1a. What texture quality do I need to set [THIS GAME] to at 1080p to get 70 FPS so that frame gen can work really well?1b. What does it look like before and after DLSS is enabled?As for frame generation, unless you're playing a game which is nothing more than an interactive movie, response time matters, and if the game is natively 20 fps and you're using frame generation to turn it into 80 fps, the game will be sluggish as all shit. ...I think he means get it to 70 FPS natively so that it can  frame gen up to 200+  but  once you get up to 70 FPS native (esp if the 1% doesn't suck) then the casuals he talks about won't really care about the 200 fake FPS anyway.overall it's just sad t 
1a. What texture quality do I need to set [THIS GAME] to at 1080p to get 70 FPS so that frame gen can work really well?1b. What does it look like before and after DLSS is enabled?
As for frame generation, unless you're playing a game which is nothing more than an interactive movie, response time matters, and if the game is natively 20 fps and you're using frame generation to turn it into 80 fps, the game will be sluggish as all shit. ...
I think he means get it to 70 FPS natively so that it can  frame gen up to 200+  but  once you get up to 70 FPS native (esp if the 1% doesn't suck) then the casuals he talks about won't really care about the 200 fake FPS anyway.
 unless you're playing a game which is nothing more than an interactive movie, response time matters100% correct. Additionally, it matters subjectively to the person experiencing the response time. Most people buying budget cards won't notice the latency increase of going from 70fps on pure raster to 120fps with DLSS/FSR. They're going to be freaking out over how smooth everything is.
 unless you're playing a game which is nothing more than an interactive movie, response time matters
100% correct. Additionally, it matters subjectively to the person experiencing the response time. Most people buying budget cards won't notice the latency increase of going from 70fps on pure raster to 120fps with DLSS/FSR. They're going to be freaking out over how smooth everything is.
What are you talking about dude. The issue is simple - modern games BARELY fit within 8 GB (and sometimes don't even fit into 8 GB) and when you enable the AI-based enhancement features (DLSS, frame generation)That's exactly what was said about the 8GB cards released LAST generation and people SOMEHOW continue to use 8GB cards to play modern video games. They simply reduce texture quality a little bit. Almost every single game can be played with 8GB of VRAM, you just have to adjust the texture quality and, most importantly, the people who are buying entry-level cards aren't concerned about seeing a couple more polygons in their environmental foliage.
What are you talking about dude. The issue is simple - modern games BARELY fit within 8 GB (and sometimes don't even fit into 8 GB) and when you enable the AI-based enhancement features (DLSS, frame generation)
That's exactly what was said about the 8GB cards released LAST generation and people SOMEHOW continue to use 8GB cards to play modern video games. They simply reduce texture quality a little bit. Almost every single game can be played with 8GB of VRAM, you just have to adjust the texture quality and, most importantly, the people who are buying entry-level cards aren't concerned about seeing a couple more polygons in their environmental foliage.
instead of the games that that budget buyers play-- LAST YEAR'S AAA games.That is some seriously strange gatekeeping. Sorry but there's no clear guidance on what players with certain graphics cards play, nor is there any true comparison between the year and performance requirement of a AAA game. Virtually every gamer I know varies their games and performance requirements wildly. And games seem to have no consistency in max performance even related to visual play. Some games look phenomenal and run well. Others look poor and run like shit. Some modern games run better than last ye 
instead of the games that that budget buyers play-- LAST YEAR'S AAA games.
That is some seriously strange gatekeeping. Sorry but there's no clear guidance on what players with certain graphics cards play, nor is there any true comparison between the year and performance requirement of a AAA game. Virtually every gamer I know varies their games and performance requirements wildly. And games seem to have no consistency in max performance even related to visual play. Some games look phenomenal and run well. Others look poor and run like shit. Some modern games run better than last ye 
I hope Intel overall survive their current CPU tribulations so they can stay in the game.
I have only owned NVIDIA cards since a GeForce4 MX but when my current GTX 1660 Super gets replaced (which probably won't be until it fails given the high price of GPUs in Australia) my next card won't be NVIDIA due to all the crap they have pulled as of late.
The original article comments ""But the unspoken covenant of product reviews is that the press, as a whole, gets a chance to warn the public if a movie, video game, or GPU is not worth their money"".
It is very obvious that film reviews have not worked this way for a long time.
the solution is simple, don't buy NVIDIA! the top hardware may be good, but today, things are getting worse and very overpricedAMD have good cards, Intel new cards are also very good, specially for their cost. Both work great in linux with open drivers, while nvidia is still on the closed ones and a pain to install, maintain and full of bugs (not in games, but in X11/Wayland stuff)
The power of nvidia is their market share, drop it and games will start to work better in other cards too. With lower market share, nvidia will either lower prices or build better hardware.
TLDR: don't buy nvidia, demand AMD or Intel GPUs
That only sucks relative to their own products. The problem is if you want top cards you are going NVIDIA regardless of what your exact performance requirements are. AMD has abandoned this segment, and Intel never played in it.
exactly, they build this stupid top cards, that cost a arm and a leg, consume enormous amount of power , just to claim they have the best, the true one TOP GPU... reviewers go crazy,  a few whales buy them (ie: rich people, just to show and mostly not even used and some ""gamers"", with either rich parents or high sponsors, many times to play little demand games) and them sell a huge amount of XX50 or XX60 or XX70 cards because people read the review of the top card and believe that the lower ones are good en 
There may be more comments in this discussion. Without JavaScript enabled, you might want to turn on Classic Discussion System in your preferences instead.
Internet Archive Now Livestreams History As It's Being Preserved
Destructive Malware Available In NPM Repo Went Unnoticed For 2 Years
Some programming languages manage to absorb change, but withstand progress.
		-- Epigrams in Programming, ACM SIGPLAN Sept. 1982"
"NVIDIA","RTX 5060","I just reviewed the Nvidia GeForce RTX 5060, and it has one major flaw that severely hurts it","https://www.pcgamesn.com/nvidia/geforce-rtx-5060-review","We run the RTX 5060 benchmarks through our graphics card test suite, to see if 8GB of VRAM can still carry this new $299 GPU to victory.


                    Ben Hardwidge                

If it weren’t for its limited memory, the Nvidia GeForce RTX 5060 would be a decent 1080p gaming GPU, with solid rendering performance that’s close to the RTX 4060 Ti and a fair bit quicker than the 4060. As it is, however, the 5060’s lack of VRAM means it already struggles with some games, and will find it harder to run more demanding titles in the future.
I almost feel a bit sorry for the Nvidia GeForce RTX 5060. My review has been delayed because Nvidia decided not to release a press driver so I could test it before release, and there's now a very obvious reason why. The Nvidia GeForce RTX 5060 still only has 8GB of VRAM, and that's proving to be a major restriction on its capabilities. It's a baffling decision that means the RTX 5060 falls over in one of our test games if you run it above the Medium graphics preset, even at 1,920 x 1,080.

All of which is a shame because, as a GPU, the Nvidia GeForce RTX 5060 isn't actually bad, with a decent step up in rendering power over its predecessor, the RTX 4060. With a wider memory interface and 12GB of VRAM, it could easily justify its $299 asking price, even if Nvidia decided to use older GDDR6 memory to keep down the price. As it is, the RTX 5060 has an uphill battle to justify itself as the best graphics card in this price range, particularly with the aggressively priced AMD Radeon RX 9060 XT release date on the horizon.


Why you can trust our advice ✔ At PCGamesN, our experts spend hours testing hardware and reviewing games and VPNs. We share honest, unbiased opinions to help you buy the best. Find out how we test.


If we ignore the VRAM elephant in the room for the moment, the RTX 5060 has a solid uplift in specs compared to the RTX 4060. The new GPU contains 3,840 CUDA cores, which are the tiny processors that work in parallel to render your games. As a general rule, the more the better when it comes to CUDA cores, and the RTX 5060 has a solid 25% more than the RTX 4060, which only has 3,072 of them, meaning it should be significantly quicker when it comes to raw frame rates.


Meanwhile, the RTX 5060's 30 RT cores will make this budget gaming GPU better at handling ray tracing than the 24 in the RTX 4060, especially as they're also based on the new and improved Blackwell architecture. There are also 120 Tensor cores for AI workloads, such as DLSS upscaling and frame gen, as well as new neural rendering techniques such as neural radiance cache. Again, that's a decent step up from the 96 found in the RTX 4060.


Nvidia has also furnished its latest Tensor cores with the ability to run its multi-frame gen tech, which uses AI to insert up to three extra frames between each pair of frames genuinely rendered by the GPU. In my tests, I've found that this tech can't fix a low frame rate – if your game is running at 15fps, then using multi-frame gen to boost that to 50fps will still be horrible to play. However, if your game is already running at 60fps, multi-frame gen can be a useful tool in your GPU's arsenal to boost that figure to around 200fps, smoothing out the motion if you have a monitor with a high refresh rate.


Then we come to the RTX 5060's big stumbling block, which is the VRAM. On the plus side, it uses fast GDDR7 memory running at 1,750MHz (28Gbps effective), which results in a peak memory bandwidth figure of 448GB/s with the card's 128-bit bus. That basically means the GPU can retrieve data from the VRAM remarkably quickly for a card at this price – as a point of comparison, the RTX 4060's total memory bandwidth is just 272GB/s, so the RTX 5060's memory system is significantly faster.


That's where the good news ends, though, because the big problem is that there's only 8GB of this fast memory on this card. That's the same capacity as the RTX 4060 before it, and even less than the 12GB RTX 3060 that came out several years ago. As a result, many of our test games showed us VRAM warnings in the graphics settings during testing, even when running some of the games at 1080p, some games would clearly be quicker with more memory, and some games just flat-out turned into a horrible stuttering mess.


You can see how Nvidia ended up in this situation. After all, 12GB of the latest GDDR7 VRAM tech is going to bump up the price, and the yields on new 3GB GDDR7 chips are probably nowhere near high enough to make a mass-produced graphics card based on them yet. That means Nvidia needs to use 2GB chips at the moment, and the structure of the RTX 5060 gives it a 128-bit bus, which means it can either have 8GB or 16GB of VRAM made with standard 2GB chips – 16GB would be too expensive, so we've ended up with an 8GB card.


Arguably, Nvidia would have been better off having a different layout for this chip, with a 192-bit bus allowing for a 12GB setup, or even just a 160-bit bus allowing for 10GB – I'd argue that just using more GDDR6 memory would have been better than using less GDDR7 RAM. As it is, though, the RTX 5060 has really fast memory, but not enough of it, which puts it in a difficult competitive position against the $349 AMD Radeon RX 9060 XT 16GB, if it's actually available at that price.


Another area where the Radeon RX 9060 XT has an advantage is the PCIe interface, as it uses the full 16 PCIe 5.0 lanes, while the RTX 5060 only uses eight of them. That's plenty of bandwidth if you're using a PCIe 5.0 motherboard, but it means you'll still only get eight lanes if you put this GPU in a PCIe 3.0 or 4.0 motherboard, perhaps if you're upgrading an Intel 10th-gen Comet Lake system, for example, which only supports PCIe 3.0, and using just eight PCIe 3.0 lanes is likely to start restricting performance of this GPU.


Finally, I'm pleased to see that the RTX 5060 still only requires a single 8-pin power connector, meaning it will work fine with most people's existing PSUs, with the total graphics power rated at just 145W. That means you'll be fine running this GPU with a standard 500-550W PSU.

Nvidia isn't making its own RTX 5060 Founders Edition card, which means all 5060 cards are made by third-party board partners. Gigabyte sent me its GeForce RTX 5060 Gaming OC card for this review, which keeps remarkably quiet with its three fans spinning at low speeds, and also tightly squeezes into just two slots.


Gigabyte has also given its card a small overclock of 95MHz, which takes the boost clock from 2.5GHz to 2.9GHz. As such, bear in mind that our benchmark results will be very slightly quicker than those of a stock speed card, though the difference will be very small (only 1-2fps here and there).


There's also a large metal backplate on the rear of the card, which doesn't make a direct connection with any of the silicon, but does help prevent the card from sagging, and the RTX 5060 doesn't run particularly hot anyway. Meanwhile, a large cutout on the right of the backplate allows hot air to be pushed straight through the top of the card in a standard case configuration, where your case's fans can then push it out the back.


It's a big beast for a budget gaming GPU, but it was also exceptionally quiet during testing, and you also get a bit of RGB lighting on the right-hand side of the top edge, beneath the Gigabyte logo. One other interesting part of the design is the positioning of the 8-pin power connector on the far left of the card. This can make for tidy cable routing if your case has a routing hole for your motherboard's front panel audio header, but it can look a bit messy if you're dragging cables over from the right.

To assess the gaming performance of the GeForce RTX 5060, I'm running a number of benchmarks using real games, rather than synthetic benchmarks. Each test is run three times, recorded with Nvidia FrameView, and I report the mean average of the results, discarding any obvious anomalies. I report two figures for frame rates – firstly, the average, which gives you an idea of the general frame rate you will achieve. Secondly, I report the 1% low, which is an average of the lowest one percent of results recorded during the benchmark.


The latter is a more reliable indicator of performance than the outright minimum, as it removes outliers, such as moments where a Windows system event causes the game to stutter, which is unrelated to the performance of the GPU. The 1% low is what you can expect the actual typical minimum frame rate to be in these games.


I'm also now reporting the latency in GPU reviews, which is expressed in milliseconds (ms), and refers to the delay between an action being executed in a game and that action being displayed by your graphics card – the lower the latency, the more responsive your game will feel in action. These figures aren't always featured in the graphs, as I don't have the data for all the other GPUs, but they're discussed in the review.


I'm going to kick off the RTX 5060 benchmarks with a look at a great showcase title for this new budget gaming GPU, Doom The Dark Ages. The latest hellscape shooter from id features loads of cool ray tracing tech, as well as implementing full support for Nvidia DLSS 4, including multi-frame gen. I ran the new game on the RTX 5060 at various settings, and it coped remarkably well with them.


Even without any help from DLSS, the RTX 5060 can cope with this game at 1080p using the Ultra Nightmare preset, where it averages 70fps, with the 1% low dropping to 54fps. Enable DLSS 4 on the Quality setting, and these figures go up to 93fps and 75fps respectively, plus the game still looks really good at these settings, thanks to the new DLSS 4 Transformer model, which makes for stable motion and much less blurriness than DLSS 3.


That 93fps average is also a solid enough foundation to enable frame gen, meaning the RTX 5060 can spread its AI wings and soar to an average of 238fps. Impressively, while enabling multi frame gen does increase input lag, this game's support for Nvidia Reflex means the impact is minimal – I measured the input latency at 24ms without multi frame gen, and 35ms with it, and the game was perfectly fine with frame gen enabled during gameplay, with the action looking super smooth.


However, if you then up the settings to 2,560 x 1,440, you already start to bump up against the VRAM limitation, with the memory usage turning red in the graphics menu. You can run the game reasonably well at 1440p with DLSS on the Quality setting, where it averages 66fps with 31ms latency.


However, enabling frame gen at these settings saturates the VRAM, with the latency going up to 74ms. While the average frame rate of 92fps looks higher with frame gen enabled, the game was actually unplayable in action, with occasional stuttering and sudden acceleration as the frame rate caught up. If you're only gaming at 1080p, though, the RTX 5060 handles this game really well.


Another good showcase for the RTX 5060 is Cyberpunk 2077, which again supports all the latest Nvidia DLSS 4 tech, and looks fantastic on the new GPU. If you run the game with the Ultra ray tracing preset at 1080p, you can see that the RTX 5060's 45fps average is significantly quicker than the 37fps from the RTX 4060, and only 3fps behind the RTX 4060 Ti.


It's still not playable at these settings, but setting DLSS upscaling to Balanced (which still looks solid with the new Transformer model), pushes this up to 85fps, with a 68fps 1% low, which is a solid enough. You can then enable 4x multi-frame gen, and the game averages 231fps with a 126fps 1% low – the game looks fantastic at these settings, and it's smooth as well. There is an increase in latency here, going from 26ms to 40ms, but I found the game was still responsive enough to play in action.


I then had a go at enabling path tracing using the Cyberpunk 2077 Overdrive mode, just to see if this new budget GPU could cope with the intensive demands of this ray tracing technique. You need to engage DLSS to get a decent frame rate, of course, but if you enable upscaling on the Performance setting (which, again, doesn't look terrible anymore, thanks to the new Transformer model), the game averages 66fps with a 50fps 1% low and 31ms latency.


If you then enable multi-frame gen, the average frame rate goes up to 202fps, but the 1% low is just 70fps, and the latency increases to 46ms. In action, I found the game occasionally stuttered with frame gen enabled here, with sudden fast movements occurring when the frame rate caught up again. Basically, path tracing is technically possible on this GPU, but I wouldn't recommend it, even if you throw all the DLSS magic Nvidia can muster at it.


Now for the big fly in the RTX 5060 ointment, a super-demanding game that makes the RTX 5060 struggle, simply because it doesn't have enough VRAM to cope with it. This game has two tiers of settings – the standard one ranges from Low to Supreme and has ray tracing as standard, and you can then also engage a further tier of Full RT settings at various levels to enable path tracing features.


As standard, I run this game at the Ultra preset with no path tracing on all GPUs, as it still looks really good at these settings, and most GPUs can cope with it. The RTX 5060 Ti 16GB, for example, averages a smooth 91fps in this game at these settings, with a 1% low of 72fps.


However, 8GB of VRAM simply isn't enough to cope with this preset, with the RTX 5060's performance falling off a cliff edge – the average was just 23fps, and the 1% low dropped right down to 9fps – it's completely unplayable. Even dropping to the next setting down (High) pushed this GPU's memory to the limit, with a 41fps average and stuttery 1% low of 13fps. You also still get a warning about not having enough VRAM if you drop down to the Medium setting, but I found the card could still run the game using this preset, averaging 92fps with a 71fps 1% low.


In the end, you have to drop down to the Low preset to stop getting VRAM warnings in the graphics menu, where the RTX 5060 averages 110fps. That's a decent frame rate, but while the game still looks OK at these settings, it's clear that a bit more memory would enable you to run this game with much more detail and eye candy on the RTX 5060.


Next up is Call of Duty Black Ops 6, a AAA shooter that doesn't feature any ray tracing tech, and also doesn't support multi-frame gen. This test gives us a good indication of a GPU's raw rendering capabilities, although it does generally prefer AMD's GPU architectures to those of Nvidia. As a case in point, this is the only game in our test suite where the aging Radeon RX 7600 is faster than the RTX 5060.


Even so, the difference is marginal, and the RTX 5060 can clearly cope with this game fine at 1080p, even when it's maxed out with no DLSS. Its average of 94fps and 68fps 1% low is fine, with the former being a good 12fps quicker than the RTX 4060. If you then enable DLSS on the Quality setting, the average frame rate goes up to 100fps, and the latency remains the same, averaging around the 22ms mark.


You can also then enable frame gen, allowing the average frame rate to pull away to 147fps, although the latency then goes up a little to 32ms. However, the 1% low was surprisingly low here at 69fps, particularly at the start of the benchmark. If you're playing this game on the RTX 5060, I'd avoid enabling frame gen and just stick with DLSS on the Quality setting with the Extreme preset or, even better, drop down to lower graphics settings.


While the RTX 5060 can cope with Black Ops 6 at 1080p, the graphics menu flags up a red VRAM warning when you up the resolution to 2,560 x 1,440 with the Extreme preset. It's still playable, despite this, averaging 68fps, with the scope for a higher frame rate if you enable DLSS or drop the settings, but this does again show that this card is right up against it when it comes to memory. Enabling frame gen at this resolution pushes the average up to 104fps, but the 1% low still stumbles at 50fps, and the latency goes right up to 45ms – your best bet is to run this game at 1080p if you're using the RTX 5060.


While we wait for F1 25 to come out, we're currently still testing with F1 24, which is still a great benchmark with its fast pace and gorgeous ray tracing effects at the Ultra preset. This benchmark also really shows the increase in rendering power you get from stepping up from the RTX 4060 to the 5060. The former averages 53fps with a 38fps 1% low at 1080p, but the RTX 5060 takes the average all the way up to 72fps. That puts the average on a par with the RTX 4060 Ti, and the 1% low on the RTX 5060 was actually faster than the 4060 Ti at 56fps.


That's already playable without any help from DLSS, but you can get some more performance from the RTX 5060 if you enable DLSS upscaling on the Quality setting and enable frame gen too. This pushes the average right up to 120fps with a 74fps 1% low – a great result, and with only a modest increase in latency from 35ms to 38ms. Comparatively, I couldn't even get the Radeon RX 7600 to run this game with FSR upscaling and frame gen enabled, as it kept falling over.


Moving up to 1440p is a bit much for the RTX 5060 at these settings, though, where it averages just 48fps with a clunky 1% low of 34fps, and high input latency of 52ms. As with most of the above tests, 1080p is definitely the sweet spot for the RTX 5060 if you're running games at high settings.


Finally, our aging Doom Eternal benchmark is still a good test of the latest GPUs, particularly if you enable ray tracing. Even at 1080p, maxing out this game with the Ultra Nightmare settings with ray tracing pushes the VRAM really hard, but can give you great results if the GPU is up to the job. I've seen several 8GB cards fall over in this test, including the RTX 4060, so I was intrigued to see how the RTX 5060 coped.


Remarkably, the RTX 5060 managed to clock up a 185fps average at these settings, with a 96fps 1% low, offering similar performance to the RTX 4060 Ti 8GB. However, you can see from the RTX 5060 Ti 16GB result that the extra VRAM really enables this GPU to pull away by a huge margin, despite only having a modest amount of extra GPU power.


Disable ray tracing, however, and the RTX 5060 can really churn out the frame rates in this old game, with its 336fps average at 1080p being 31fps in front of the RTX 4060, and its 247fps average at 1440p being a decent result too.

One area where the RTX 5060 really excels is power draw, with our Ryzen 7 7800X3D test rig only drawing a peak of 274W from the mains with the RTX 5060 running at full load. Again, that's a similar result to the RTX 4060 Ti, and only marginally higher than the Radeon RX 7600 – a difference that would very probably disappear if you were using a stock speed version of the card, rather than our overclocked one.


I'll be interested to see the power draw of the Radeon RX 9060 XT in comparison, but given the relatively high power consumption of the Radeon RX 9070 cards, this is a battle that may end up being won by the RTX 5060. It may only have 8GB of VRAM, but most PC gamers will easily be able to upgrade to this card without needing to buy a new PSU.

The Nvidia GeForce RTX 5060 price is $299, which matches the price of its predecessor, the RTX 4060, while overclocked versions, such as the Gigabyte Gaming OC card we tested, cost a little extra. At face value, the price doesn't look too bad, given that the new GPU offers a solid improvement in rendering performance over its predecessor.


However, at this price, this GPU really needs more than 8GB of VRAM now, especially when the AMD Radeon RX 9060 XT 16GB is lined up to cost $349. We'll have to see how real-world prices look once all the GPUs have been released and the prices have settled down, but $299 is too expensive for a GPU with this level of performance and memory in 2025, even if it is better than its predecessor.

AMD's competing GPU may not be out yet, but the Radeon RX 9060 XT release date is coming very soon, and I advise holding off on purchasing the RTX 5060 before you know how this new AMD GPU performs. Two versions will be available, with AMD saying the 8GB card will have the same price as the RTX 5060, with the 16GB card costing a very reasonable $349 at MSRP. It's likely that the Radeon RX 9060 XT 16GB will be a much better buy than the RTX 5060, but only if AMD's pricing holds up in the real world, which hasn't been the case with the 9070 cards so far.


This Intel Battlemage GPU has one major benefit over the RTX 5060, which is an extra 4GB of VRAM, giving it a total of 12GB. This gives it much more headroom to run the latest games at high settings without completely falling over, although Intel's supporting suite of AI tech is a fair way behind Nvidia DLSS when it comes to game support. Thanks to scarce supply, however, Intel Arc B580 prices are all over the place, particularly in the US. At its $259 MSRP, the Arc B580 is a good buy, but it makes less sense when it costs over $300.

Oh, Nvidia, why do you do this to us? You give us a decent boost in rendering performance for the same price as your old card, but then you only give it 8GB of VRAM on a $299 card in 2025 – it's like building a nice house, then making all the doors not quite big enough to avoid scraping your butt and bashing your head. Nvidia could have definitely made a better offering here, perhaps using a wider bus and 12GB of VRAM, even if it wasn't GDDR7 memory. I'd also be surprised if we didn't see a 12GB RTX 5060 Super coming out next year, using 3GB GDDR7 chips.


It's unfortunate in the meantime, though, because there are other areas where the RTX 5060 is a decent GPU. Its rendering and ray tracing performance is a fair bit quicker than the RTX 4060, and not far off the RTX 4060 Ti 8GB, which isn't bad for the $299 asking price. Even at this level, multi frame gen can still be a useful tool as well – you just need to make sure your starting frame rate is high enough, and you can then enjoy super-smooth frame rates in games that support it – in particular, Doom The Dark Ages and Cyberpunk 2077 run well on this new GPU with DLSS 4.


But then we have other games, such as Indiana Jones and the Great Circle, that fall over if you go above the Medium graphics preset, even at 1080p, and several of our test games gave us warnings about VRAM capacity in the graphics menu as we increased the settings. To be fair, this card did still run most of our test games at 1,920 x 1,080 with its 8GB of VRAM, but a lot of them were right on the limit, which raises questions about future proofing and the lifespan of this GPU.


I also advise waiting for our AMD Radeon RX 9060 XT review before purchasing the RTX 5060, as the 16GB version of this graphics card could well turn out to be a better buy, but only if real-world pricing doesn't spiral out of control. In the meantime, the RTX 5060 isn't a terrible $299 gaming GPU, but it could have been made so much better if Nvidia addressed its one major flaw.


If you're planning an upgrade, check out our guide to the best graphics card to see which GPU best suits your needs, as well as our tutorial on how to install a GPU, which takes you through the whole process.


You can also follow us on Google News for daily PC games news, reviews, and guides. In addition, we have a vibrant community Discord server, where you can chat about this story with members of the team and fellow readers.





                    Ben Hardwidge                 A tech journalist since 1999, and a PC hardware enthusiast since 1989, Ben has seen it all, from the horrors of CGA graphics to the awesome power of the RTX 5090 today. Ben is mainly interested in the latest CPU and graphics tech, and currently spends most of his evenings playing Oblivion Remastered, while marveling at how much better it looks than the original game did on his old GeForce 6600 GT."
"NVIDIA","RTX 5060","4 graphics cards you should consider instead of the RTX 5060","https://www.digitaltrends.com/computing/4-gpus-to-buy-instead-of-rtx-5060/","Digital Trends may earn a commission when you buy through links on our site. Why trust us?
Nvidia’s RTX 5060 is finally here, and many people hoped it’d put up a fight against some of the best graphics cards. Does it really, though? Reviewers are split on the matter. Alas, I’m not here to judge the card. I’m here to show you some alternatives.
While Nvidia’s xx60 cards typically become some of the most popular GPUs of any given generation, they’re not the only option you have right now. The RTX 5060 might not even be the best option at that price point. Below, I’ll walk you through four GPUs that I think you should buy instead of the RTX 5060.
I’m not sure whether this will come as a surprise or not, but based on current pricing and benchmarks, the GPU I recommend buying instead of the RTX 5060 is its last-gen equivalent. 
The RTX 4060 is one of the last RTX 40-series graphics cards that are still readily available around MSRP. I found one for $329 at Newegg, and it’s an overclocked model, meaning slightly faster performance than the base version. However, you might as well just buy a used RTX 4060 if you find it from a trustworthy source, as that’ll cost you a whole lot less.
The RTX 5060 and the RTX 4060 have a lot in common. Spec-wise, they’re not at all far apart, although Nvidia’s newer Blackwell architecture and the switch to GDDR7 VRAM give the newer GPU a bit more oomph. But, unfortunately, both cards share the same 8GB RAM — an increasingly small amount in today’s gaming world — and the same narrow 128-bit bus.
Some reviewers note that the RTX 5060 isn’t far ahead of the RTX 4060 in raw performance. The newer card gets the full benefit of Nvidia’s Multi-Frame Generation, though. Overall, they’re pretty comparable, but if you can score a used RTX 4060 for cheap, I’d go for it.
I wasn’t a big fan of the RX 7600 XT 16GB upon launch, and I still have some beef with that card. Much like Nvidia’s options, AMD equipped its mainstream GPU with a really narrow memory interface, stifling the bandwidth and holding back its performance. Still, in the current climate, I’ll take that 16GB with the 128-bit bus over a card that has the same interface and only sports 8GB VRAM.
The cheapest RX 7600 XT 16GB costs around $360, and you can find it on the shelves with ease. But it’s the same scenario here — if you can find it used from a trustworthy source, it might be worth it, assuming you’re on a tight budget. The state of the GPU market as of late has made me appreciate second-hand GPUs a lot more.
The RX 7600 XT is slower than the RTX 5060, and it’ll fall behind in ray tracing, but it gives you plenty of RAM where Nvidia’s card offers very little. That alone makes it worthy of your consideration.
AMD’s upcoming RX 9060 XT could be a great option here, too. I expect it to offer better ray tracing capabilities than the RX 7600 XT, and it’ll have the same $300 price tag as Nvidia’s RTX 5060.
If your budget is a little bit flexible, you could go one level up and get the RTX 5060 Ti with 16GB of RAM. Unfortunately, the cheapest options are at around $479 right now, which is well over the MSRP and a whopping $180 more than the RTX 5060. However, for that price, you’ll get yourself a GPU that’s better suited to stand the test of time.
With 16GB of video memory and the full benefit of GDDR7 RAM, the RTX 5060 Ti 16GB offers an upgrade over the last-gen version. It’s not perfect by any stretch, though. Reviewers put the GPU below the RX 9070 non-XT, the RTX 5070, and even the RTX 4070 when you consider pure rasterization. This means no so-called “fake frames,” which is what Nvidia’s DLSS 4 delivers.
 That leaves the RTX 5060 Ti in an odd spot. Basically, if your budget can stretch to it, the RX 9070 and the RTX 5070 are both better cards; they’re also a lot more expensive.
Less demanding gamers might find an option in Intel’s Arc B580. Upon launch, the GPU surprised pretty much everyone with its excellent performance-per-dollar ratio. The downside? That ratio is now a lot less impressive, because unexpected demand and low stock levels brought the price of the Arc B580 far above its $250 recommended list price (MSRP).
The Arc B580 is a little bit slower than the RTX 4060 Ti, so it’ll be slower than the RTX 5060, too. It also can’t put up a fight as far as ray tracing goes. But it’s a budget-friendly GPU and a solid alternative to the RTX 5060 if you’d rather pick up something else this time around.
The more successful and impressive cards from this generation, such as AMD’s RX 9070 XT or Nvidia’s RTX 5070 Ti, keep selling above MSRP. Those that aren’t quite as exciting may stick around MSRP (which is where the RTX 5060 sits right now, mere days after launch) … but that doesn’t make up for their shortcomings.
Given the fact that reviews of the RTX 5060 are still pretty scarce, I’d wait it out for a week or two. Read some comparisons, check out the prices, and then decide. Gambling on a GPU just because the previous generations were solid doesn’t work anymore, and that’s now clearer than ever.
It's official: Nvidia has just announced three new GPUs, and some of them will be competing against the best graphics cards in the mainstream segment. There are also new laptops coming right up. Nvidia's announcement tells us what to expect from the new additions to the RTX 50-series lineup.
Team Green is keeping the pricing the same for its (historically) most popular GPU, the RTX 5060. Starting at $299, the card has the same recommended list price (MSRP) as the RTX 4060. No surprise there, really, and it's mostly a question of whether it'll actually be readily available at MSRP -- most RTX 50-series models are decidedly not.
Nvidia's upcoming RTX 5060 Ti 16GB graphics card has once again surfaced in leaked benchmarks, showcasing a decent performance uplift over its predecessor. According to data obtained by VideoCardz, the RTX 5060 Ti 16GB outpaces the RTX 4060 Ti 16GB by approximately 20% in synthetic tests, while trailing the RTX 5070 by nearly 33%.
The leaked benchmarks results are primarily from 3DMark's suite of tests including SpeedWay, Steel Nomad, Port Royal, Fire Strike and Time Spy, suggesting that the RTX 5060 Ti 16GB offers a generational improvement over the RTX 4060 Ti.  
If the leaks are to be believed, Nvidia's RTX 5060 Ti is mere days away from launching, set to rival some of the best graphics cards. Today's leaked benchmarks serve as proof that the GPU might indeed be on the imminent horizon, because it showed up in two tests. Both results make me feel a little more optimistic about the RTX 5060 Ti.
VideoCardz spotted Geekbench scores of the RTX 5060 Ti, and while these are synthetic benchmarks that don't say much about the card's ability to handle AAA games, they give us some insight into its specs and overall capability.
Upgrade your lifestyleDigital Trends helps readers keep tabs on the fast-paced world of tech with all the latest news, fun product reviews, insightful editorials, and one-of-a-kind sneak peeks."
"NVIDIA","RTX 5060","Nvidia GeForce RTX 5060 Review: They Didn't Want This Out","https://www.techspot.com/review/2992-nvidia-geforce-rtx-5060/","The Nvidia RTX 5060 is based on the same GB206 silicon as the more expensive 5060 Ti series. This means the 181 mm² die contains 21.9 billion transistors – though not all are active in this model, as the core count has been reduced by 17%.
Still, the RTX 5060 features 25% more cores than the previous-gen 4060. Not only that, but thanks to the use of 28 Gbps GDDR7 memory, bandwidth has increased by 65% to 448 GB/s.
On paper, this is a $299 graphics card (MSRP) but we'll have to see what's the retail pricing looks like post-launch. This model is also limited to just 8 GB of VRAM, which is bad, however on the upside  there is only one configuration available, making it far less of a trap for gamers compared to the more expensive 8GB RTX 5060 Ti.
Making the VRAM limitation even more problematic is the use of a PCIe 5.0 x8 bus interface. While not ideal even for PCIe 5.0-enabled systems, it becomes a major issue for those on older hardware, especially when restricted to PCIe 3.0. We can explore this further on a later extended review.
Believe it or not, we set up an AM5 test system in our hotel room while attending Computex 2025 in Taiwan. We're using a Ryzen 7 9800X3D processor we brought along, paired with a G.Skill Trident Z5 DDR5-6000 CL30 memory kit and our usual test SSD. A big thank you to MSI for providing the additional components needed to make this review possible.
It's been a challenging but worthwhile process to put this together – so let's dive into the data…
First up is Clair Obscur: Expedition 33, where the RTX 5060 averaged just 48 fps at 1080p. That's the same level of performance seen with the much older RTX 3060 Ti, making it only 17% faster than the RTX 4060.
Increasing the resolution using the Epic preset isn't really viable. The RTX 5060 simply isn't powerful enough and, with just 8 GB of VRAM, quickly runs out of memory. For example, the 5060 Ti 16 GB model is usable here, while the 8 GB version struggles significantly.
In Oblivion Remastered, the RTX 5060 delivered an average of 45 fps at 1080p, which is roughly on par with the RTX 4060 Ti and RTX 3070 – about 25% faster than the RTX 4060.
At 1440p, VRAM limitations show up again. While 34 fps is technically playable, 1% lows drop to just 18 fps, resulting in a very choppy experience.
Performance in Delta Force looks much better, with the RTX 5060 hitting 138 fps at 1080p. However, this is still in line with the RTX 3060 Ti and actually a bit slower than the RTX 4060 Ti.
At 1440p, the RTX 5060 again lands between the 3060 Ti and 3070, though this time its performance is closer to the 3070.
In Stalker 2, the RTX 5060 averaged 47 fps at 1080p – once again similar to the RTX 3060 Ti, and just a 9% improvement over the RTX 4060.
At 1440p, the 8 GB VRAM buffer becomes a major bottleneck, dropping average frame rates to just 6 fps. In this state, the game is essentially unplayable.
The RTX 5060 excels as an esports GPU, delivering strong performance in Counter-Strike 2 at 1080p using the medium preset. Impressively, it offers a 27% increase over the RTX 4060 and matches the performance of the older RTX 3070.
At 1440p, performance remained excellent, averaging 370 fps – slightly ahead of the Radeon RX 7700 XT.
Space Marine 2 ran very well at 1080p, with the RTX 5060 averaging 100 fps, providing a smooth experience and a 23% improvement over the RTX 4060.
At 1440p, performance improved by a massive 33% over the 4060, coming very close to RX 7700 XT levels.
Star Wars Jedi: Survivor also ran smoothly, with 95 fps on average at 1080p. This level of performance puts the RTX 5060 in the same range as the RTX 4060 Ti and RTX 3070.
At 1440p, the trend continues: 57 fps on average matches 4060 Ti and 3070 levels and represents a 30% uplift over the RTX 4060 – an impressive result.
Call of Duty: Black Ops 6 saw the RTX 5060 render 75 fps at 1080p, which was only a 10% uplift over the 4060 and 7% slower than the RTX 3070 – not a great result overall.
It's a similar story at 1440p, where performance is essentially on par with the RTX 3060 Ti. That's a disappointing outcome given how much time has passed since that GPU's release.
In A Plague Tale: Requiem at 1080p, the RTX 5060 delivered just a 5% improvement over the RTX 3070, making it slightly slower than the 7700 XT. However, it was a significant 36% faster than the RTX 4060.
At 1440p, it continued to outperform the 4060 with a 33% advantage, although it only managed to match the performance of the RTX 3070 and 4060 Ti.
Starfield performance was even weaker. At 1080p, the RTX 5060 matched the RTX 3060 Ti with just 55 fps on average.
At 1440p, the margin remained narrow, with the RTX 5060 averaging 44 fps – just 5% faster than the 3060 Ti.
Performance in Cyberpunk 2077 at 1080p was solid. The RTX 5060 averaged 100 fps, putting it on par with the 4060 Ti and 28% ahead of the RTX 4060.
At 1440p, it maintained strong performance with 66 fps on average, again delivering results similar to the 4060 Ti and RTX 3070 – 35% faster than the 4060 in this case.
The RTX 5060 delivered surprisingly strong results in God of War Ragnarök at 1080p, averaging 128 fps – an impressive 45% improvement over the RTX 4060.
That margin was reduced at 1440p, but the 5060 still came in 32% faster than the 4060, again delivering performance comparable to the RTX 3070 and 4060 Ti.
In Dying Light 2, the RTX 5060 effectively matched the RTX 3070 and 4060 Ti at 1080p.
At 1440p, the story remained consistent – though here, the Arc B580 also entered the performance mix.
Interestingly, Dragon Age: The Veilguard proved more difficult. At 1080p, the RTX 5060 rendered just 68 fps, making it only 11% faster than the RTX 4060 and notably slower than both the RTX 3070 and 4060 Ti.
At 1440p, the performance gap narrowed, with the 5060 aligning more closely with the RTX 3070 and 4060 Ti, though average frame rates dropped to 49 fps – not exactly impressive.
Spider-Man Remastered posed no challenge for the RTX 5060 at 1080p. It averaged 159 fps, narrowly edging out the 4060 Ti and RTX 3070, and delivering a 25% uplift over the RTX 4060.
At 1440p, performance remained strong with 110 fps on average. Relative performance was typical, closely matching the Arc B580, 4060 Ti, and RTX 3070.
Performance in Hogwarts Legacy at 1080p was also impressive. The RTX 5060 averaged 101 fps – slightly faster than the RTX 3070, significantly ahead of the 4060 Ti, and 44% faster than the RTX 4060. This game is very memory-intensive, so the high-speed GDDR7 memory is well utilized here.
At 1440p, a different bottleneck appears. The RTX 5060 only matched the 4060 Ti, resulting in a 13% performance uplift over the 4060.
In The Last of Us Part I, the RTX 5060 delivered 85 fps at 1080p, putting it on par with the 4060 Ti and 25% faster than the RTX 4060.
However, at 1440p, performance fell apart. With just 8 GB of VRAM, the 5060 couldn't maintain consistent frame times using ultra-quality settings.
Finally, in Star Wars Outlaws, the RTX 5060 struggled. At 1080p, it rendered only 42 fps – a mere 8% improvement over the RTX 4060.
At 1440p, frame rates dropped further to 31 fps, making it 19% faster than the 4060 but still delivering very weak performance overall.
Across the 18 games tested, the RTX 5060 matched the performance of the RTX 4060 Ti and RTX 3070, while coming in 6% slower than the 7700 XT. It was also 22% faster than the RTX 4060, which aligns closely with Nvidia's official claims.
At 1440p, we saw several examples where 8 GB GPUs began to fall apart. In some cases, performance appeared acceptable, but the visual quality suffered due to missing textures that couldn't fit into local video memory.
Overall, the RTX 5060 remained on par with the RTX 3070 and 4060 Ti, though it was just 6% faster than Intel's Arc B580 and 27% faster than the RTX 4060.
As expected, achieving a high-end ray tracing experience is difficult – if not impossible – with the RTX 5060. The GPU simply doesn't have enough power, and its 8 GB of VRAM is insufficient for ray tracing in modern titles.
For example, in Alan Wake II at 1080p with DLSS Quality enabled, the RTX 5060 averaged just 36 fps. That made it 20% faster than the RTX 4060, but 14% slower than the 4060 Ti.
Those hoping to enable ray tracing at 1440p will be disappointed – it's simply not viable on this GPU.
Thanks to DLSS, it's possible to approach 60 fps at 1080p in Cyberpunk 2077 using the Ultra RT preset. However, since this relies on upscaling, it's not true 1080p rendering. Performance is comparable to the Arc B580 – not a particularly strong result.
At 1440p, ray tracing is off the table. We also encountered VRAM limitations during our brief testing.
Marvel's Spider-Man Remastered is a well-optimized title, and even with ray tracing maxed out, the RTX 5060 managed an impressive 112 fps on average at 1080p – similar to the 4060 Ti.
At 1440p, the RTX 5060 performed even better, averaging 109 fps and pulling well ahead of the 4060 Ti. That's a solid result, nearly matching the RTX 4070. Still, you'd typically expect a product labeled ""5060"" to at least match the previous-generation GPU positioned one tier higher.
At upscaled 1080p with the high ray tracing preset, Dying Light 2 ran at 72 fps on the RTX 5060 – matching the 7700 XT and 4060 Ti. This is usable performance, though not outstanding, despite being almost 30% faster than the RTX 4060.
At 1440p, the RTX 5060 struggled more, delivering just 48 fps on average. This result was still similar to the 7700 XT and 4060 Ti.
We know MSRP isn't always reflective of reality, especially at launch, but it's still useful for establishing a baseline. If all GPUs were sold at their suggested prices, the Arc B580 would offer the best value, followed by the RX 9070, and then the RTX 5060.
At MSRP, the 5060 comes in at a cost per frame of $5.35 – a 21% improvement over the RTX 4060 and a 30% improvement over the RTX 3060. That sounds solid, or at least it would be if the card had more VRAM. We'll come back to that shortly. For now, let's take a look at real retail pricing.
At the time of writing this review, the RTX 5060 was available and in stock on Newegg for $330 – about 10% over MSRP. In the current market, that makes it relatively decent value, assuming you ignore the elephant in the room: the 8 GB VRAM buffer.
Even when factoring in the poor results seen in some VRAM-limited games, particularly in titles where missing textures or inconsistent frame times become an issue, the RTX 5060 still manages to be 8% better value than the 7800 XT.
That's not a strong showing considering the Radeon GPU comes with 16 GB of VRAM. Compared to the outgoing RX 7600, it's 14% better value, and 21% better value than remaining RTX 4060 stock.
So, given today's market conditions, the RTX 5060 offers decent value for buyers looking to purchase a brand-new GPU. But with just 8 GB of VRAM, it's not a product we can recommend at that price.
So there you have it. On the surface, the RTX 5060 appears to stack up fairly well. If you don't look too closely, you might even call it good value. But deeper analysis reveals a troubled product that will almost certainly age incredibly poorly.
As we've clearly established by now, 8 GB of VRAM is simply not enough – and in 2025, it should not exist on any GPU priced above $200. As an esports card, it holds up reasonably well, though if that's your target use case, we'd suggest exploring the second-hand market instead.
It's frustrating how good the RTX 5060 could have been. Even with just 12 GB of VRAM, we might have been able to tentatively recommend it at its current price. With 16 GB, it could have been a genuinely solid product.
As it stands, the RTX 5060 is effectively a discounted RTX 4060 Ti – offering about 25% savings. That might sound appealing, but nearly two years after the 4060 Ti's release, it's hardly exciting. Looking further back, the 5060 essentially offers RTX 3070-like performance at a 40% discount – but nearly five years have passed since Ampere launched.
In our opinion, Nvidia had a clear opportunity to deliver a meaningful upgrade here. Instead, they've recycled the same class of GPU for five years, offering incremental discounts with each release.
The real challenge for Nvidia will be the incoming Radeon RX 9060 XT series. If AMD's numbers prove accurate, the RTX 5060 won't be worth considering – it's dead on arrival, at least for buyers who follow real, independent reviews. On that note, Nvidia has handled this launch very poorly. It's been a PR disaster. Ironically, Nvidia's marketing may be more effective than AMD's at convincing GeForce owners to switch to Radeon. In fact, this might be the only way that shift was ever going to happen.
That's going to wrap up our review of the RTX 5060. We debated calling it a preview rather than a full review – but claiming an ""RTX 5060 preview"" feels like a bit of a self-own at this point. So let's call it a quick review.
We'll cover more details, including ray tracing performance, power consumption, and overclocking, once we're back from Computex. Needless to say, Steve has outdone himself to deliver a comprehensive and honest look at this GPU under the circumstances. If you've found it helpful, we appreciate your support.

About
Ethics Statement
Terms of Use
Privacy Policy
Manage Ad Consent

TECHSPOT : Tech Enthusiasts, Power Users, Gamers
TechSpot is a registered trademark and may not be used by third parties without express written permission.
TechSpot is represented by Future PLC. Learn about advertising opportunities.
© 1998 - 2025 TechSpot, Inc. All Rights Reserved."
"NVIDIA","RTX 5060","Nvidia's RTX 5060 review debacle should be a wake-up call for gamers and reviewers","https://www.theverge.com/pc-gaming/672637/nvidia-rtx-5060-review-meddling-gamersnexus-wake-up-call","Seems Nvidia didn’t want its most popular GPU raked over the coals this year — and risked its credibility to prevent it.
Seems Nvidia didn’t want its most popular GPU raked over the coals this year — and risked its credibility to prevent it.
If you buy something from a Verge link, Vox Media may earn a commission. See our ethics statement.
This week, the company reportedly attempted to delay, derail, and manipulate reviews of its $299 GeForce RTX 5060 graphics card, which would normally be its bestselling GPU of the generation. Nvidia has repeatedly and publicly said the budget 60-series cards are its most popular, and this year it reportedly tried to ensure it by withholding access and pressuring reviewers to paint them in the best light possible. 
Nvidia might have wanted to prevent a repeat of 2022, when it launched this card’s predecessor. Those reviews were harsh. The 4060 was called a “slap in the face to gamers” and a “wet fart of a GPU.” I had guessed the 5060 was headed for the same fate after seeing how reviewers handled the 5080, which similarly showcased how little Nvidia’s hardware has improved in one generation and relies on software to make up the gaps.
Here are the tactics that Nvidia reportedly just used to throw us off the 5060’s true scent, as individually described by GamersNexus, VideoCardz, Hardware Unboxed, GameStar.de, Digital Foundry, and more:
Some reviewers apparently took Nvidia up on that proposition, leading to day-one “previews” where the charts looked positively stacked in the 5060’s favor:
But the reality, according to reviews that have since hit the web, is that the RTX 5060 often fails to beat a four-year-old RTX 3060 Ti, frequently fails to beat a four-year-old 3070, and can sometimes get upstaged by Intel’s cheaper $250 B580. 
And yet, the 5060’s lackluster improvements are overshadowed by a juicier story: inexplicably, Nvidia decided to threaten GamersNexus’ future access over its GPU coverage. Yes, the same GamersNexus that’s developed a staunch reputation for defending consumers from predatory behavior, and just last month published a report on “GPU shrinkflation” that accused Nvidia of misleading marketing. Bad move!
In a 22-minute video, GN claims Nvidia threatened to cut off access to Nvidia’s cooling and latency experts unless GN agreed to do the thing you see in the charts above — compare cards with fake frames to cards without. GN claims it has the recorded phone conversations to prove it, which are likely legal because Nvidia was recording them too. 
“Just to be clear, Nvidia, I am prepared to release them,” GN editor-in-chief Steve Burke threatened. 
Recording every conversation isn’t how companies and reviewers normally operate. There’s been a serious breakdown in trust if we find ourselves here!
Nvidia is within its rights to withhold access, of course. Nvidia doesn’t have to send out graphics cards or grant interviews. It’ll only do it if it’s good for business.
But the unspoken covenant of product reviews is that the press, as a whole, gets a chance to warn the public if a movie, video game, or GPU is not worth their money. It works both ways: the media also gets the chance to warn that a product is so good you might want to line up in advance. That unspoken rule is what Nvidia is trampling here.
On Wednesday, May 14th, I asked Nvidia in a group press briefing: “Are there not going to be reviews of the RTX 5060 before our readers are able to buy it?” 
Nvidia didn’t deny it. “Units will be available from May 19th,” was Nvidia GeForce PR boss Ben Berraondo’s response, seemingly implying that a lack of early supplies of the GPU, not an underhanded campaign to influence early reviews, would be to blame for the gap.
Earlier in the same briefing, Hardwareluxx’s Andreas Schilling wrote a similar question and got a similar answer: “Could you share your thought on why Nvidia is going to release the driver for RTX 5060 with availability and not giving us the chance to do our reviews prior to this?” 
Berraondo answered, “We are focused on delivering a great day-one experience for GeForce RTX 5060 gamers with our Game Ready Driver that will be available to everyone on May 19.”
But as GamersNexus and other publications soon revealed, not “everyone” had to wait until the 19th to start testing. Nvidia didn’t respond to repeated requests for comment about the GamersNexus allegations. 
It wasn’t Nvidia’s only misleading statement about the card. During that same Wednesday briefing, rather than sharing Nvidia’s benchmark charts, GeForce product management director Justin Walker claimed the new GPU would “let you play your games maxed out at over 100 frames per second,” including demanding titles as Black Myth Wukong at 130fps, Cyberpunk 2077 at 148fps, and Half-Life 2 RTX at 130fps. 
I laughed when I read the fine print and saw what Nvidia meant by “maxed out.” It meant a paltry 720p render resolution, DLSS-upscaled to 1080p, with up to three of every four frames imagined by AI — and even then, only when you paired Nvidia’s budget $299 GPU with a decidedly not budget $599 AMD CPU, one of the best money can buy. 
One of Nvidia’s other pieces of news from that same briefing was that DLSS 4 with Multi-Frame Generation is available in over 125 games and apps. “DLSS 4 is the fastest adopted gaming technology in our history,” Walker proclaimed. 
Does that mean GPU reviewers can no longer ding these graphics cards for marketing features only a handful of game developers bother to use? I thought to myself.
But no: as of today, Nvidia’s website lists just 29 games with full native support for DLSS Multi Frame-Generation. The only way Nvidia can get to 125 is by counting games where players have to force it through Nvidia’s drivers, which doesn’t give any indication of adoption by game developers. 
So now, I’m wondering: where else might Nvidia be trying to pull the wool over our eyes?
I can’t quite understand why Nvidia would risk fracturing trust the way it did this week. I mean, yes, Nvidia now has fuck-you money from AI, and gaming can feel like an afterthought. 
Nvidia’s networking business is now bigger than gaming, which now represents less than 10 percent of Nvidia’s total revenue. The company makes more pure profit from AI in a single quarter than total gaming sales in a year. It’s no wonder the GPUs are in short supply at MSRP when their makers are richly rewarded for putting silicon capacity toward AI chips instead.
But that feels like a good argument for Nvidia to stop caring whether its gaming GPUs sell, not why it might feel the need to meddle with reviews. If the desktop RTX 5060 doesn’t hit sales goals, the company will be more than OK. Nvidia would be less OK if everyone started questioning its integrity.
What might help explain this push, though, is Nvidia’s seeming need to make its founder’s new vision for gaming into a reality. At CES 2025, Nvidia CEO Jensen Huang kicked off a huge debate about “fake frames” among PC gamers when he suggested they were the future of graphics — effectively, that the idea your game should draw each and every scene 60, 120, or more times per second will seem antiquated. That AI not only can, but should fill in the gaps. 
It’s not so far-fetched an idea: as my colleague Tom Warren noted in January, “so much of modern gaming is already ‘fake,’ and it has been for years.” That might be why Nvidia has been so pushy about reviewers adding such comparisons to their reviews. (Nvidia has even bugged us to include MFG results in our AMD reviews, a request we’ve largely ignored.) 
But in the end, Huang’s claim that the $549 RTX 5070 would deliver $1,599 RTX 4090 performance didn’t ring true. The thing about Nvidia’s MFG is it needs enough real frames to begin with, or it doesn’t feel smooth, and if it already feels smooth, you may not need the extra frames. It’s not a silver bullet that can make a 1440p card feel like a 4K card and, according to Dave James with PC Gamer, it isn’t enough to make Nvidia’s new 1080p card feel like a 1440p one, either. 
In one of the first real reviews of the RTX 5060, with video examples, James explains:
You’re not going to be able to use MFG to be able to up the resolution on your low-end RTX 5060 to match your 1440p monitor, even with DLSS running. And you’re not going to be able to use MFG to enable you to run at the highest in-game settings, even sometimes at 1080p.
The extra latency and low input frame rates either make it a latency spiking nightmare or the AI generated frames end up creating a ton of unpleasant artifacts as you run around whatever gameworld you’re in.
Meanwhile, HardwareUnboxed published a review that shows the new 5060 may not be that much faster than the old 4060, even at 1080p. They found it 20 percent faster on average across 18 games, and as low as 8 percent faster in Star Wars Outlaws, 9 percent faster in Stalker 2, and 10 percent faster in Black Ops 6. At 1440p, the $250 Intel Arc B580 offered better 1 percent lows and is the superior deal if you can find it at that price.
We may never know how many PC gamers bought an RTX 5060 without seeing any such comparisons, because Nvidia kept proper reviews from arriving on time. But in many cases, it won’t be too late to return those GPUs. Maybe Nvidia’s bad behavior is enough to push us to buy AMD’s new card or wait for Intel’s next card instead, challenging Nvidia’s 90-percent control of the market and, perhaps, bringing some much-needed competition. 
A weekly newsletter by David Pierce designed to tell you everything you need to download, watch, read, listen to, and explore that fits in The Verge’s universe.
© 2025 Vox Media, LLC. All Rights Reserved"
"NVIDIA","RTX 5060","Nvidia RTX 5060 review live: looks like the 8 GB of VRAM isn't the biggest issue the new card has to face","https://www.pcgamer.com/hardware/live/news/nvidia-rtx-5060-review-doing-it-live/","When you purchase through links on our site, we may earn an affiliate commission. Here’s how it works.

Update, May 20, 2025: It's day two of testing and I'll be digging more into MFG testing and overclocking today, as well as seeing how it stacks up from a creator point of view with some rendering tests.
Something a little screwy has been going on with the new Nvidia RTX 5060 graphics card launch. It was revealed a few weeks ago that Nvidia wasn't going to be helping sample cards for review testing of its budget-oriented RTX Blackwell GPU. Now, that's not completely unprecedented for a card low down the stack which doesn't have a Founder Edition to call its own. What is unprecedented, at least in my long experience, is the company refusing to allow review access to pre-release drivers to specifically block some media from having day one critiques of the hardware live at the time the cards go on sale.
I've had an RTX 5060 sat on my desk for the past few weeks, as MSI has been happy to oblige us with the hardware, however, no vendor has been allowed to give us access to drivers in order for those pre-release cards to actually function as intended. I mean, I've just been using mine as a paperweight.
But not all media has been blocked. Over the weekend, multiple sites had strictly controlled previews go live for the new RTX 5060 GPU, and those sites have had drivers ahead of time, and will have reviews going live for the embargo of 2pm today. Some of those sites are from the Future family of which we are part, yet it seems PC Gamer, and potentially Tom's Hardware, have been singled out as publicatio non grata for this launch.
Now, it has to be said that Nvidia owes us nothing and has zero obligation to support PC Gamer around new graphics card launches. We're talking about a massive, multi-trillion dollar company now, and a GPU which will likely rise to the top of the Steam Hardware Survey in a handful of months no matter what we say; Nvidia doesn't need us. And yet, in all my time as a PC hardware journalist, at many different sites and publications, Nvidia always has supported pre-release reviews.
And it has also consistently been a good sport about both favourable and unfavourable reviews, so long as they were fair, and honestly demonstrated why any conclusions had been drawn. I feel that's how we've been all along with with the RTX 50-series reviews and yet we find ourselves black-listed by the company for this one.
Still, I'm a professional, I believe in giving our honest, independent opinions on PC gaming hardware and so, I'm going to be testing this card out live. Follow along with the live updates below as I go through benchmarking the card once public drivers are finally released.
Because we've not been allowed access to review drivers we also don't have access to any reviewer's guide material which would confirm the specifications of the GPU at the heart of the RTX 5060. But enough has been published and leaked so far that we can be pretty confident that we're looking at another spin of the GB206 GPU used inside the RTX 5060 Ti.
This time around there are six fewer streaming multiprocessors making up the RTX Blackwell chip, so just 3,840 CUDA cores and the requisite number of RT Cores and Tensor Cores coming out of those 30 SMs. That's a 25% increase in core count, and Nvidia has claimed a corresponding 25% increase in gen-on-gen raster performance, too.
Alongside that is the much maligned 8 GB of GDDR7 VRAM, which ought to have the same level of memory bandwidth as the RTX 5060 Ti's 8 GB version because it's running the same 128-bit aggregated memory bus.
To be honest, I don't have that much of a problem with a low-end GPU in a range coming out with 8 GB of video memory. There will be some instances where it can have an impact, as shown when the ill-optimised The Last of Us port initially launched and struggled on 8 GB GPUs. But The Last of Us Part 2 showed that if the devs manage it correctly, even 8 GB can be enough for max settings.
AMD is also releasing new 8 GB graphics cards in this generation, with the RX 9060 XT, expected to launch at Computex this week, coming in both 8 GB and 16 GB configurations. Although we still don't know what the price is going to be for that one.
That leaves it open as to whether we're looking at competition for the RTX 5060 Ti or the RTX 5060 from a price perspective. Though we ought to be looking at Intel, too, when it comes to competition as the Intel Arc B580 could provide some interesting price/performance comparisons to Nvidia's new card as well. That was a card I didn't much trust at launch, but it's drivers have definitely improved.
It only remains to be seen what sort of state Nvidia's RTX 5060 launch drivers will be. Once I've got them installed and running through our benchmark suite, I'll be able to let you know below...
This is the one I'm immediately interested in; how the new card performs against the Intel Arc B580. This is a GPU which would regularly give the RTX 4060 some troubles, but its immature drivers at launch were an issue. They've improved now (we can actually benchmark Cyberpunk 2077!) so whether it will prove a match for Nvidia's new card will certainly be interesting.
It has more memory, so you'd think that might sway some things in its favour, but how that shakes out in our benchmarking suite... well, we'll have to see once the drivers are out.
Nvidia has been promising 20 - 25% higher performance than the RTX 4060, for the commensurate increase in CUDA core counts inside the RTX 5060's RTX Blackwell GPU. That's the straight raster performance increase, but the company has also been touting the Multi Frame Generation increase offering a far higher frame rate increase.
Now, personally, I'm a fan of the frame gen dance. They may be 'fake frames' but if they make my gaming smoother, and so long as there's a good input frame rate the latency isn't prohibitive, then I'm on board. Having tested it in a bunch of different games I'm generally happy with how it looks and performs.
It might be a tougher ask on the RTX 5060, however, as there is going to be a lower input frame rate to start with...
While we wait for the drivers to be unveiled, this is our compatriot site Techradar's take on the performance of the RTX 5060.
The drivers aren't expected until later on this afternoon, so until then, why not catch up with Computex 2025? We're stalking the halls of Taiwan's biggest tech show once again to see what Nvidia, AMD, Intel, Asus, Gigabyte, MSI and more have to offer.
The thing with modern GPUs is what the actual on-street pricing will be. With the cards released this year all we've seen are ever-ephemeral MSRP and sky-high actual price tags. Whether that's the manufacturers massively inflating the cost or retailers deciding to add a few hundred dollars on to get their due, or a combination of both, the prices of graphics cards has been painful this generation.
And it's not an Nvidia thing, either. AMD's impressive RX 9070 XT launched with an emphasis on its relatively low price tag, and at MSRP it delivered a card that competed with the RTX 5070 Ti in terms of performance and undercut it. But now we're in a situation where the two cards are equivalently priced, and the extra feature set of the RTX Blackwell card gives it the edge.
So, what's going to happen with the RTX 5060? Honestly, this feels like our best chance to get a card that remains close to MSRP across the globe for the simple reason that the RTX 5060 Ti 8 GB card puts a hard ceiling on pricing. At the moment that card is $430 at Newegg, and I've seen $410 in recent weeks, too.
That at least gives me hope pricing will not go super crazy for the RTX 5060, despite some of the leaked Best Buy listings.
As of April's Steam Hardware Survey, the RTX 4060 was number three in the list of the most-used graphics card from all the systems tested as part of the data collection carried out for Valve's census.
And top is the mobile equivalent, with the RTX 3060 from the Ampere generation in between them. That's kind of as you would expect; the lowest-priced discrete GPU of the past generation being high up in the list of cards being used today. It wasn't hugely well-received, has the same 8 GB framebuffer as the RTX 5060 (lower than the 12 GB of the RTX 3060), and yet is sitting pretty up there.
No matter what people will say about this card when we do finally get drivers and see the performance of the card, it's going to find its way into a ton of prebuilt gaming PCs, and will be picked up by a lot of folk looking for an affordable upgrade from their RTX 20- or 30-series GPUs.
Just for a second I thought we were on. The RTX 5060 was suddenly available as an option in the driver search panel on the Nvidia site, but all it returned was a No Drivers Found message with the May 12 software still the only one available for download.
But, we ought to have drivers available to download within the next hour. So... er... stay tuned, I guess.
As the Intel social media manager said to the GPU-hungry masses.
In case you're interested, this is the PC we're going to be testing out this MSI RTX 5060 Gaming OC card on, the same as for practically all of the GPUs we've reviewed in the past six months:
CPU: AMD Ryzen 7 9800X3D | Motherboard: Gigabyte X870E Aorus Master | RAM: G.Skill 32 GB DDR5-6000 CAS 30 | Cooler: Corsair H170i Elite Capellix | SSD: 2 TB Crucial T700 | PSU: Seasonic Prime TX 1600W | Case: DimasTech Mini V2
So, what's the first benchmark I'm going to jam through the RTX 5060 when the drivers launch in five mins or so? That's easy... the same one I do for all new graphics cards: Metro Exodus: Enhanced Edition.
It's not that it's a particularly impressive benchmark, it's just that we use three looped runs of the game at 4K to measure our power draw, performance per watt, and temperature benchmarks. As well as take the average GPU frequency throughout the run, too.
It's the benchmark that gives us the most data for a single set of runs, and gives us an idea of where the chip sits from an efficiency point of view, as well as a more standard gaming benchmark stance.
If you're interested in how the sausage is made, so to speak, this is the piece of equipment we use to measure power usage of a particular graphics card. This is the Nvidia PCAT, which is a super handy tool we picked up years ago which measures both the power being drawn from the PCIe slot as well as what the GPU is pulling through whatever cables are attached to the card.
This is the second-gen version which has twin 16-pin power connectors on to measure the latest high-end cards. It also has a single 8-pin connector (which means we have to go back to the first-gen board if we want to test cards with more than one old-school plug) but that's fine for the RTX 5060 which only has a single plug for power.
There's a riser board from the PCIe slot the graphics card plugs into, which is what measures power from the motherboard itself.
Well, the 'RTX 5060 out now' page is live, though the drivers apparently still aren't...
And here's the actual driver release page nominally for the RTX 5060 and F1 25.
[F1 23/F1 24] Game crashes at the end of a race [5240429]
[Diablo II Resurrected] Game displays black screen corruption when using DLSS [5264112]
[SCUM] Game may crash after updating to R575 drivers [5257319]
Shader disk cache will not be created with certain games if OS username contains unicode characters [5274587]
[Lumion 12] Missing certain UI components [5213228]
[Varjo XR3] Varjo XR3 HMD is not working on RTX 50 series GPUs [5173753]
[Notebook] GeForce RTX 50 series TGP limit may be clipped earlier [5170771]
Flickering/corruption around light sources in Ghost of Tsushima Directors Cut [5138067]
Cyberpunk 2077 will crash when using Photo Mode to take a screenshot with path tracing enabled [5076545]
EA Sports FC 25 may crash during gameplay [5251937]
[Forza Horizon 5] Game may crash after extended gameplay [5131160]
[RTX 50 series] Dragons Dogma 2 displays shadow flicker [5252205]
[RTX 50 series] Video playback in a web browser may show brief red/green flash corruption [5241341]
Wuthering Waves may randomly crash during gameplay after updating to R575 drivers [5259963]
[RTX 50 series] Enshrouded crashes after launching game [5279848]
[NVIDIA App] Adding an unsupported app to NVIDIA App and enabling Smooth Motion forces it globally to other apps [5243686]
[RTX 50 series][Battlefeld 2042] Random square artifacts may appear around lights during gameplay [5284105]
Changing a setting in the ""NVIDIA Control Panel"" -> ""Manage 3D Settings"" may trigger shader disk cache rebuild [5282396]
[Gray Zone Warfare] Game may crash on startup [5284518]
Good ol' Nvidia App, it's finally recognised the RTX 5060 as one of its own. Who needs the internet or actual driver pages?
Though, to be fair, Metro Exodus did seem to take an actual age before it started running, making me think my 'joke' was going to be prescient.
Anyways, here we go... Metro Exodus: Enhanced Ed. @ 4K Ultra settings.
It's running a little thirstier than an RTX 4060 in terms of average and peak power draw, but even this 'overclocked' version is only ~64 MHz faster in respect to the clock speed. It is certainly cooler-running, however.
You can really see the efficiency differential between the RTX Blackwell architecture and the RX 7600 XT and Arc B580 from AMD and Intel respectively.
And here are the final efficiency numbers for the card running at 4K and its more likely target resolution of 1080p. Of all the competing cards at this sort of level, it is definitely the most efficient from a performance per watt PoV.
But does that matter when you're just chasing budget frame rates? Now, that's the question.
The question of power vs performance might be a key one at the very high-end of graphics cards, where they really can suck down a ton of power, but at the budget level—where there isn't a huge amount in terms of wattage between them—I'd say the average and 1% Low fps figures are where it really counts.
And our first actual gaming performance numbers will certainly please... Intel.
You can see the RTX 5060 and Arc B580 are delivering almost identical scores in Metro Exodus: Enhanced Edition. If that becomes a continuing trend across my benchmarking, then that might be a worry for Nvidia. Or, it would have been were Intel actually able to ship cards at its $250 MSRP.
A familiar refrain, right? Right now, the most affordable B580 seems to be a Gunnir card that's some $379. Which is, frankly, ludicrous money for that card. At $250 it would now be a great budget buy, but like so many modern GPUs, it's priced out of the running for now.
So, now we have our 3DMark scores, for both Time Spy Extreme and Port Royal—synthetic raster and ray tracing benchmarks respectively. Again, it's the B580 proving problematic for Nvidia, though I would say that it's worth taking these numbers with a pinch of salt when it comes to Intel's 3DMark performance; it's GPUs have always been suspiciously well tuned for UL's benchmark.
Still, the ray tracing figures being not far off the RTX 5060 is something to note.
Things are looking interesting between the RTX 5060 and the Arc B580 right now, but let's look at where things stand vs the RTX 4060. Actually, pretty well. In the Metro benchmarks it's a pretty solid ~25% increase at 1080p and 1440p, with a 33% increase at 4K, where it ought to be equally memory starved.
When it comes to 3DMark, we're looking at a 30% improvement in raster and 43% increase in ray tracing power from the RTX Blackwell chip.
So far a decent upgrade over the RTX 4060...
Maybe more interesting, at least at 1080p and 1440p so far, is that the RTX 5060 is delivering performance around 12 - 17% off the 16 GB RTX 5060 Ti card. Given that is a GPU with a nominal 43% higher price tag the value tag is most definitely more suited to the lower spec card.
Given they're both running on the same GB206 GPU, and with surprisingly few SMs lopped off to make the RTX 5060 chip, you can see why performance might be so close. It's only at 4K where you are going to see real differences in performance, though that is not really a battleground for either of these cards.
Right, I've just finished the Black Myth Wukong benchmarks, and while it's still close against the RTX 5060 Ti outside of 4K, there is clear air between the RTX 5060 and the Arc B580 here. We're talking 49% at 1080p and 42% at 1440p.
At 4K though there is a 24% lead for the Intel chip, but that is looking at 13 fps vs. 17 fps, so neither is a win.
And with that I break for tea and putting the kids to bed... back soon.
Aaaand I'm back, and starting with what has become the Nvidia poster child, Cyberpunk 2077. This is one of the game engines which most heavily supports the suite of modern GeForce technologies, with every one baked into the game, and a game that is used by Nvidia to highlight them to the best effect.
As such, it's generally pretty well tuned for GeForce GPUs. So, I'm expecting good things against the competition from last gen and from AMD and Intel.
Erm. Well, that hasn't gone as well as I expected. The RTX 5060's 1080p performance is pretty excellent, to be fair to the ickle GPU, but as soon as you step away from that lowly resolution things get choppy real quick.
It's some 20% quicker than the Arc B580 at 1080p, and then a full 48% faster than the RTX 4060 at the same res and settings. Win. Win.
But at 1440p it's a couple frames slower than the Intel card and just four fps faster than the RTX 4060.
And at 4K? Well, I'm still waiting for that 2 fps run to finish...
In fairness the B580 only nets 3 fps, but I got higher frame rates out of the RTX 4060 with January drivers. Not good.
I mean, that's kinda the essential tagline for almost the entire Nvidia RTX 50-series lineup, but with the benchmark results here that looks to be true once more. But benchmarks can be deceiving.
The in-game benchmark in Cyberpunk 2077 shows a significant rise in frame rate when you enable Frame Gen, and a huge uptick with 3x and 4x Multi Frame Gen enabled. At 1440p natively you're getting just over 20 fps on average, but with 2x you're getting 81 fps with DLSS set to Quality, and then 108 fps and 138 fps when you hit 3x and 4x.
A quick look at the PC latency should be enough for you to look at that result and be happy you're getting a good gaming experience. The 60-odd ms of latency is no issue in a single player shooter.
But just look at the video below (ignore the occasional chop, that was a video hiccup not something in-game) and you'll see some horrible artifacts due to MFG and a general fuzziness to proceedings. And it feels overall rather floaty, too, when you're playing.
In short MFG is not the panacea here, at least not for 1440p. The input frame rate is not enough to make even a nominal 100+ fps frame rate feel actually good.
There is some frankly unsettling choppiness to the F1 benchmark runs so far. Not like a little frame drop here and there, but complete freezes to the visuals. And it's happening whether at 1080p, 1440p, or 4K.
Now, I don't want to be the one to scream '8GB framebuffer!!!!!!' but it definitely gets worse when you shift up the resolutions. Though, in all honesty, this feels more like a driver issue than a real lack of video memory.
I mean, you should see how gross it looks now I've engaged Frame Gen at 1440p with DLSS Quality. It's nasty.
The RTX 4060 never did this, I'm sure I would've noticed...
Unlike with the Cyberpunk 2077 video, this time the choppiness is not down to the video encoding, that's what it looks like live. And even when it's running 'normally' it doesn't look great. I mean, you don't even need to fullscreen the vid to see how bad it gets.
As I say, I feel this is a driver thing, because I've not seen this in other 8 GB cards.
Onto Total War: Warhammer III and things are a bit more straightforward again. No need to mess around with Frame Gen or upsampling, this is good old fashioned raster gaming. Simple.
And we're back to a consistently healthy lead over the RTX 4060 and a bit of a bump over the Arc B580 at all resolutions.
But this is certainly not a consistent situation across our benchmarking suite, and I would expect not across many other games out there, either.
Homeworld 3 is the same, to be honest. Really solid benchmarks, and it looks and feels good. I'm seeing the RTX 5060 hit 51 fps (28 fps 1% Low) at 4K Epic settings. Across the board that's far ahead of the Arc B580 which struggles with HW3.
BUT, it's not a million miles ahead of the RTX 4060 at 1080p (at least not at 1080p and 1440p DLSS settings), and not a million miles off the RTX 5060 Ti, either. That last is more of a worry for Nvidia's mid-range GPU than this lower-end offering.
And that's the full PC Gamer test suite now run across the RTX 5060, at 1080p, 1440p, and 4K, as well as with DLSS and Frame Generation where it matters. Prepare for benchmark graphs when I come back from making a cup of tea.
Without further ado, here are the final gaming performance benchmark numbers from the full test suite. Just click through for 1440p and 4K figures.
As you would expect, the RTX 5060 is at its most consistent at 1080p. That's its happy place, and the place where it will consistently hold its own alongside the RTX 5060 Ti. And yet, even here, there are times where either the Arc B580 gets near it, or even beats it, or it strays too close to the RTX 5060 Ti itself.
Neither are situations Nvidia will be happy about.
We also have our optimistically titled 'real-world' gaming benchmarks.
Now, these are designed to be a little closer to how the cards will perform in every day situations, where users will tend to use extra features, such as frame generation and upscaling. But things actually start to break down quite badly here for the RTX 5060.
Where Multi Frame Generation is the big new feature for the RTX 50-series it's not magic. It's quite clear where the limitations are for this technology and it's when there isn't enough raw graphical power to comfortably support the funky frame generating features.
Cyberpunk 2077 is the obvious one here as, while the frame rate numbers look great, you only have to look at the 1440p video further down in this live blog to see how bad that actually looks and plays in the game proper. The numbers alone really do not tell the story.
And then F1 24 looks its absolute worst when you turn on upscaling and Frame Generation. The stuttering is utterly game breaking and there will surely need to be some driver work done to fix that.
I've had this card functional in the test rig for maybe five hours now, and that is absolutely not enough time to fully test a new GPU and be confident that you've come to the right conclusion. There are still things I want to test, such as what is really happening with Multi Frame Generation (honestly, I'm a little scared of booting up Alan Wake 2 on the RTX 5060) and I want to go back to the RTX 4060 with these drivers and see if the problems with Cyberpunk 2077 and F1 24 are present with that card now.
But, I dunno, man, things don't look great these few hours in.
My overall takeaway from spending this evening benchmarking this card is that it is mighty inconsistent and doesn't feel like a completely reliable unit, and that's nothing to do with MSI as the card's manufacturers. I've had a few game crashes in Cyberpunk 2077, but that happens, and F1 24 still has the RTX Blackwell bug of going pinkscale if you mess with the video settings too much, but it's the game-to-game performance that feels all over the place.
At best, the RTX 5060 is a GPU that delivers a decent generational performance uplift over its RTX 4060 forebear, though sometimes the Arc B580 upstart gets its nose in where Nvidia doesn't want it. But at worst, it's a bit of a flaky GPU that isn't anywhere near the consistency of previous generations of GeForce graphics card.
I don't feel like the drivers are yet fully there for RTX Blackwell, and I fear (as maybe Nvidia does) that the main draw of the RTX 50-series, namely Multi Frame Generation, just cannot deliver down at this level of native performance.
I'm not feeling the MFG magic with the RT 5060 right now.
But, let's see what the morning brings. Until then, adieu.
And I'm back. Tentatively booting up Alan Wake 2 for some more detailed Multi Frame Gen testing. It's been a great show of the power of MFG in the other cards, but I think we're going to have to dial things back to get a good experience out of Remedy's might game.
As you would expect from an ultra demanding games, such as Alan Wake 2 on Ultra settings, it's all just too much for the RTX 5060. That's the same whether at 1080p or 1440p with full 4x MFG and DLSS Quality settings. You're going to be maxing settings out here no matter what AI magic you throw its way.
The low input frame rate results in a lot of artifacts and some horrible latency issues, which makes it not at all pleasant to play.
But you wouldn't expect it to cope with path tracing and all the pretties Remedy has dropped into its gloriously grim gameworld. If you just drop things down to High settings, even with path tracing enabled, you will get over 60 fps at 1080p with 2x Frame Gen enabled and it feels good, too.
With 4x MFG at this level there are precious few artifacts, and I was seeing 119 fps on average. It was getting a little floaty due to the PC latency going above 100 ms, so 3x ends up being a good compromise.
Here are some vids of the RTX 5060 running Alan Wake 2. The top video is of what I feel is a totally playable setting. The latency feels decent, and there aren't a huge number of visual MFG-related artifacts that I could see as I was going.
The Ultra setting video wasn't anywhere near as comfortable to play, and the high, triple-figure latency isn't something you can necessarily see in the playback, but the experience was nowhere as good.
Still, MFG here is enabling me to play Alan Wake 2 at some impressively high levels. Though don't even think about pushing that resolution any higher because the latency skyrockets at 1440p.
Doing some MFG testing with Cyberpunk 2077 again now to see if it can hang with the RT Overdrive mode... around 3 fps on average at 1440p native so far. I'm going to guess that's going to be a big fat no if the input frame rate is this low.
Okay, I might have spoken too soon. Sure, at 1440p native it's a slideshow, but drop the res down with DLSS Quality settings and some MFG lovin' and it's actually delivering a playable frame rate... in the benchmark.
Let me just see how that 60+ frame rate actually looks and feels in the harsh environment of Dogtown...
Yup, Dogtown don't like it. I'm lucky to hit 30 fps with 3x MFG enabled. And yeah, around 280 - 300 ms PC latency.
As you can see from the video, I have to drop down to RT Low settings to get a consistently above 60 fps frame rate in the GPU-demanding area of Dogtown.
Dragon Age: The Veilguard has been the game I could rely upon in my Multi Frame Gen testing to pretty much always deliver a great experience. And I was kinda expecting we could still get a good 1440p run out of the RTX 5060.
Again, though, we've hit a performance brick wall. It just never really gets going.
At native 1440p it's in the high teens in terms of frame rate, but even when you add on DLSS and 2x Frame Gen I was only getting just over 30 fps on average.
Throwing MFG into the ring, however, at both x3 and x4 means the frame rate jumps, but so does the latency. It's not the worst feeling, especially at x3, but the artifacts using the DLSS Override feature of the Nvidia App introduced to DA:TV is grim.
Here's Dragon Age: The Veilguard running at 1440p with DLSS set to Quality and x4 Multi Frame Gen enabled via the Nvidia App 👇
Check out the strange, almost shimmering artifacts in the grass as the camera moves and pay attention to the character's legs as they run. It's a mess down there.
And check out where the left arm seems to occlude over the shield they are carrying on their back.
Now I've done some MFG testing with the RTX 5060 it's kinda becoming clear that the way we were using it in the upper echelons of the RTX 50-series of graphics cards just isn't going to work down at the bottom.
You're not going to be able to use MFG to be able to up the resolution on your low-end RTX 5060 to match your 1440p monitor, even with DLSS running. And you're not going to be able to use MFG to enable you to run at the highest in-game settings, even sometimes at 1080p.
The extra latency and low input frame rates either make it a latency spiking nightmare or the AI generated frames end up creating a ton of unpleasant artifacts as you run around whatever gameworld you're in.
Say what you like about the RTX 50-series—and plenty has been said, for good or ill—but they are good overclockers. I promised earlier that I would do some overclocking testing on the RTX 5060 to see what I can get out of it, and that's what I'm going to do.
The previous cards in the new RTX Blackwell generation have left a surprising amount of extra GPU headroom on the table. Where, for the past few generations of Nvidia architecture, the dynamic clock speed of the GPUs meant that they were going almost as fast as the chips could go, with maybe only a bit of voltage curve tuning getting you that extra couple of percent.
But in reality, overclocking cards over the past few years has yielded precious little in terms of tangible performance uplifts. Not so with the RTX 50-series cards—we've gotten up to 10% extra performance from some simple tweaks, without stressing the GPUs much at all, certainly not in terms of much in the way of higher temps or power draw.
What of the RTX 5060; can we make up some of the performance shortfall via overclocking and improve what we're seeing from the new card?
That's not just because I'm a dreadful person, who ill treats hardware on the regular—though thinking about it my office machine is in parts strewn across the PC Gamer desks because I brutally ripped out the Arc B580 it was running for some comparative testing—it's because I know how much you should be able to squeeze out of an RTX Blackwell GPU from past experience.
That's pretty hard to read, but I've just gone straight in with a +300 MHz offset on the core clock, nudged the memory clocks up by +1,000 MHz, and let it have the maximum MSI Afterburner will allow in terms of extra power: an extra 9 %.
And it's fine. Running through Black Myth Wukong's benchmark it's a pretty regular 3 GHz+ core frequency, and we get another six or so frame per second at 1440p with DLSS and a handful extra at native.
That's not groundbreaking, but it's something and seemingly stable, for now.
Onwards to +350 MHz on the GPU and +1,500 MHz on the memory. That's solid in Black Myth Wukong at 1440p native, and another frame added onto the average fps score.
It's worth saying that while it might get to a high clock that's stable under the Wukong benchmark, that isn't a guarantee it's completely stable in everything. So we might get to a point where we have to knock the frequency down a bit when we hit F1 24, for example.
The menu screens alone will push the GPU into a turbo frenzy, and that can really upset a slightly unstable graphics chip.
Worth noting here that I'm just doing a bit of light touch overclocking here—I'm not chasing some high marker on HWBot for this card. And I'm definitely not capable of that, I am not a pro by any means. So, I'm just blithely nudging up the GPU clock speed slider, making sure it doesn't crash, and see if there any weird artifacts cropping up as the chip gets warmed up.
Basic stuff. But that's been enough to deliver some welcome performance bumps for little effort, mind.
I'm up at +450 MHz on the GPU clock now, with the mem still at +1,500 MHz. That's seemingly solid in Black Myth Wukong, so I'm going to give Cyberpunk 2077 a shot. Wish me luck.
Yeah, Cyberpunk didn't like that. Choom got himself blasted back to the desktop.
Now, this is kinda interesting. Maybe it's getting late and my eyes grow weary, but having found a +425 MHz GPU clock to be stable in Cyberpunk, I'm only getting another 5 fps from that overclock, but it certainly feels better. That's at 1440p, with DLSS Quality enabled at the RT Ultra setting and with 2x Frame Gen turned on.
At this clock speed and memory setting, Metro Exodus: Enhanced Edition is running completely stable, and I'm seeing an 11% increase in frame rate at both 1080p and 1440p. Which is a pleasing result from a simple overclock.
Worth noting that F1 24 is running now, without that dreadful stutter. Though it also doesn't see the 10% improvement this overclock adds on. You see that in the 3DMark scores, however, and I think we can comfortably say this is a decent overclocking GPU, and it does fix some of the issues I was having with performance before.

PC Gamer is part of Future plc, an international media group and leading digital publisher. Visit our corporate site.

©
Future Publishing Limited Quay House, The Ambury,
Bath
BA1 1UA. All rights reserved. England and Wales company registration number 2008885."
"NVIDIA","RTX 5060 Ti","ASUS GeForce RTX 5060 Ti Prime review","https://www.guru3d.com/review/asus-geforce-rtx-5060-ti-prime-review/","The requested URL /review/asus-geforce-rtx-5060-ti-prime-review/ was not found on this server."
"NVIDIA","RTX 5060 Ti","NVIDIA GeForce RTX 5060 Ti: All We Know So Far","https://www.xtremegaminerd.com/nvidia-geforce-rtx-5060/","NVIDIA’s Blackwell architecture is powering both the gaming and data center GPUs. While the data center Blackwell GPUs have already hit the market, it will still take time for all the GeForce RTX 50 GPUs to roll out in the market. In the RTX 50-series lineup, you will find 60-class cards similar to the previous generation and today we are taking a detailed look at the upcoming GeForce RTX 5060.
The GeForce RTX 5060 will be available in both Ti and non-Ti variants, and this particular post is about the latter. I have gathered all the early information about this GPU from various leaks/reports and for both the desktop and laptop variants. Here you will find all the crucial details about the RTX 5060, including specs, performance, pricing, and release date.NVIDIA Blackwell Architecture and GeForce RTX 5060The NVIDIA Blackwell is the latest architecture for the company’s mainstream gaming and data center GPUs. It’s powering chips like B200, which are being used in data centers for intensive workloads. The same architecture powers the mainstream desktop graphics cards, which are used for gaming, content creation, and other such operations where a GPU can speed up the process.Related ArticlesNVIDIA GeForce RTX 5060 Laptop Outperforms RTX 4070 Laptop GPU, Leaked Time Spy Score Appears OnlineJanuary 1, 20255 Best Graphics Cards for Intel Core i3 12100FJuly 27, 2023NVIDIA hasn’t revealed much about the architecture and we will have to wait till CES to know more about it. What we actually know is that the GPUs based on the Blackwell architecture will belong to the GeForce RTX 50 or RTX 5000 series. This succeeds the Ada RTX 40 lineup and will be NVIDIA’s best gaming GPUs till now.The Geforce RTX 5060 belongs to the budget or lower mid-range category, replacing the RTX 4060 from the previous lineup. The 60-class cards are generally offered for smooth 1080p and 1440p gaming performance. Since the card will be available on both desktop and laptop platforms, the specs will vary, which we will talk about in detail in the next section.SpecificationsThe GeForce RTX 5060 is reportedly going to utilize the GB206 GPU die and a PG151 board. No solid leak has been reported about the Cuda Core count or the core clocks. As far as other specs go, the RTX 5060 is expected to bring the same VRAM capacity as the RTX 4060 but a faster memory type. This means an 8GB GDDR7 memory configuration, which means the GPU is likely going with a 128-bit memory bus.With that in mind, the memory bandwidth will go up to 448 GB/s with 28 Gbps of memory speed, which is about 65% faster compared to 272 GB/s bandwidth on the RTX 4060. The PCI-E interface will also be upgraded from PCI-E 4.0 to PCI-E 5.0 and whether the GPU will bring an increased TDP or not is still unclear.The GeForce RTX 5060 Laptop GPU is expected to feature the same VRAM configuration but a nerfed GPU die. This means fewer Cuda Cores and also lower power consumption. Since, we don’t have solid leaks about the desktop GPU, the information on the RTX 5060 laptop GPU is pretty scarce.Right now almost every other RTX 50-series GPU is reported to be more power-hungry than their predecessors, which means we could see TDP higher than 115W on the desktop variant. We will update this page once newer reports about its specs emerge.Specs Summary:Architecture: BlackwellGPU: GB206Cuda Cores: TBDVRAM: 8GB GDDR6, 128-bitMemory Bandwidth: 448 GB/sBase Clock: TBDBoost Clock: TBDTDP: TBDPerformanceAs far as the performance goes, we expect the RTX 5060 to provide about 30% uplifts over the RTX 4060. Some leaks did appear recently which suggested that the RTX 5060 laptop GPU is 32% faster than the RTX 4060 laptop GPU and so will be the desktop version but this leak seems to be unreliable. However, going by the generational uplifts NVIDIA offers in every new GPU generation, 30% is what we expect.The RTX 5060 should be targeting 1080p ultra settings but should be able to play most games at 1440p with ultra graphics presets as well. With just 8GB of memory, the GPU would throttle performance in some titles but in case there is a 12GB variant, this could be solved. That said, the ray tracing performance is expected to be enhanced noticeably as well.With DLSS 4.0 support, the performance could be boosted even further but raster performance on native resolution will determine its position in the GPU hierarchy. We expect the RTX 5060 to go on par with the RTX 4070 or RX 7800 XT but will still be somehow inferior in various circumstances, particularly due to 8GB VRAM.Price and Release DateThe GeForce RTX 5060 could be priced between $299 and $349. This is based on the previous pricing trends of the 60-class GPUs. The RTX 4060 was launched at $299 and NVIDIA is known for increasing the price with every generation. As far as the release goes, the RTX 5060 is reportedly coming in the first quarter of 2025 but the exact month and date are unknown.GeForce RTX 5060: What to Expect?The Geforce RTX 5060 should be seen as an ideal GPU for 1080p gaming but could also deliver excellent performance at 1440p on ultra settings. If the GPU ever gets a 12GB variant, the performance will be significantly better than the predecessors. With 8GB of VRAM, it won’t stand a chance in some modern titles but should do well in the majority of the games.The RTX 5060 is expected to deliver a solid raster and RT performance but will still likely fall in the budget GPU category. To reach the mid-range category, the VRAM and memory bus should be increased and uncompromised 1440p gaming will be its ultimate test.
The NVIDIA Blackwell is the latest architecture for the company’s mainstream gaming and data center GPUs. It’s powering chips like B200, which are being used in data centers for intensive workloads. The same architecture powers the mainstream desktop graphics cards, which are used for gaming, content creation, and other such operations where a GPU can speed up the process.
NVIDIA hasn’t revealed much about the architecture and we will have to wait till CES to know more about it. What we actually know is that the GPUs based on the Blackwell architecture will belong to the GeForce RTX 50 or RTX 5000 series. This succeeds the Ada RTX 40 lineup and will be NVIDIA’s best gaming GPUs till now.The Geforce RTX 5060 belongs to the budget or lower mid-range category, replacing the RTX 4060 from the previous lineup. The 60-class cards are generally offered for smooth 1080p and 1440p gaming performance. Since the card will be available on both desktop and laptop platforms, the specs will vary, which we will talk about in detail in the next section.SpecificationsThe GeForce RTX 5060 is reportedly going to utilize the GB206 GPU die and a PG151 board. No solid leak has been reported about the Cuda Core count or the core clocks. As far as other specs go, the RTX 5060 is expected to bring the same VRAM capacity as the RTX 4060 but a faster memory type. This means an 8GB GDDR7 memory configuration, which means the GPU is likely going with a 128-bit memory bus.With that in mind, the memory bandwidth will go up to 448 GB/s with 28 Gbps of memory speed, which is about 65% faster compared to 272 GB/s bandwidth on the RTX 4060. The PCI-E interface will also be upgraded from PCI-E 4.0 to PCI-E 5.0 and whether the GPU will bring an increased TDP or not is still unclear.The GeForce RTX 5060 Laptop GPU is expected to feature the same VRAM configuration but a nerfed GPU die. This means fewer Cuda Cores and also lower power consumption. Since, we don’t have solid leaks about the desktop GPU, the information on the RTX 5060 laptop GPU is pretty scarce.Right now almost every other RTX 50-series GPU is reported to be more power-hungry than their predecessors, which means we could see TDP higher than 115W on the desktop variant. We will update this page once newer reports about its specs emerge.Specs Summary:Architecture: BlackwellGPU: GB206Cuda Cores: TBDVRAM: 8GB GDDR6, 128-bitMemory Bandwidth: 448 GB/sBase Clock: TBDBoost Clock: TBDTDP: TBDPerformanceAs far as the performance goes, we expect the RTX 5060 to provide about 30% uplifts over the RTX 4060. Some leaks did appear recently which suggested that the RTX 5060 laptop GPU is 32% faster than the RTX 4060 laptop GPU and so will be the desktop version but this leak seems to be unreliable. However, going by the generational uplifts NVIDIA offers in every new GPU generation, 30% is what we expect.The RTX 5060 should be targeting 1080p ultra settings but should be able to play most games at 1440p with ultra graphics presets as well. With just 8GB of memory, the GPU would throttle performance in some titles but in case there is a 12GB variant, this could be solved. That said, the ray tracing performance is expected to be enhanced noticeably as well.With DLSS 4.0 support, the performance could be boosted even further but raster performance on native resolution will determine its position in the GPU hierarchy. We expect the RTX 5060 to go on par with the RTX 4070 or RX 7800 XT but will still be somehow inferior in various circumstances, particularly due to 8GB VRAM.Price and Release DateThe GeForce RTX 5060 could be priced between $299 and $349. This is based on the previous pricing trends of the 60-class GPUs. The RTX 4060 was launched at $299 and NVIDIA is known for increasing the price with every generation. As far as the release goes, the RTX 5060 is reportedly coming in the first quarter of 2025 but the exact month and date are unknown.GeForce RTX 5060: What to Expect?The Geforce RTX 5060 should be seen as an ideal GPU for 1080p gaming but could also deliver excellent performance at 1440p on ultra settings. If the GPU ever gets a 12GB variant, the performance will be significantly better than the predecessors. With 8GB of VRAM, it won’t stand a chance in some modern titles but should do well in the majority of the games.The RTX 5060 is expected to deliver a solid raster and RT performance but will still likely fall in the budget GPU category. To reach the mid-range category, the VRAM and memory bus should be increased and uncompromised 1440p gaming will be its ultimate test.
The Geforce RTX 5060 belongs to the budget or lower mid-range category, replacing the RTX 4060 from the previous lineup. The 60-class cards are generally offered for smooth 1080p and 1440p gaming performance. Since the card will be available on both desktop and laptop platforms, the specs will vary, which we will talk about in detail in the next section.
The GeForce RTX 5060 is reportedly going to utilize the GB206 GPU die and a PG151 board. No solid leak has been reported about the Cuda Core count or the core clocks. As far as other specs go, the RTX 5060 is expected to bring the same VRAM capacity as the RTX 4060 but a faster memory type. This means an 8GB GDDR7 memory configuration, which means the GPU is likely going with a 128-bit memory bus.
With that in mind, the memory bandwidth will go up to 448 GB/s with 28 Gbps of memory speed, which is about 65% faster compared to 272 GB/s bandwidth on the RTX 4060. The PCI-E interface will also be upgraded from PCI-E 4.0 to PCI-E 5.0 and whether the GPU will bring an increased TDP or not is still unclear.
The GeForce RTX 5060 Laptop GPU is expected to feature the same VRAM configuration but a nerfed GPU die. This means fewer Cuda Cores and also lower power consumption. Since, we don’t have solid leaks about the desktop GPU, the information on the RTX 5060 laptop GPU is pretty scarce.Right now almost every other RTX 50-series GPU is reported to be more power-hungry than their predecessors, which means we could see TDP higher than 115W on the desktop variant. We will update this page once newer reports about its specs emerge.Specs Summary:Architecture: BlackwellGPU: GB206Cuda Cores: TBDVRAM: 8GB GDDR6, 128-bitMemory Bandwidth: 448 GB/sBase Clock: TBDBoost Clock: TBDTDP: TBDPerformanceAs far as the performance goes, we expect the RTX 5060 to provide about 30% uplifts over the RTX 4060. Some leaks did appear recently which suggested that the RTX 5060 laptop GPU is 32% faster than the RTX 4060 laptop GPU and so will be the desktop version but this leak seems to be unreliable. However, going by the generational uplifts NVIDIA offers in every new GPU generation, 30% is what we expect.The RTX 5060 should be targeting 1080p ultra settings but should be able to play most games at 1440p with ultra graphics presets as well. With just 8GB of memory, the GPU would throttle performance in some titles but in case there is a 12GB variant, this could be solved. That said, the ray tracing performance is expected to be enhanced noticeably as well.With DLSS 4.0 support, the performance could be boosted even further but raster performance on native resolution will determine its position in the GPU hierarchy. We expect the RTX 5060 to go on par with the RTX 4070 or RX 7800 XT but will still be somehow inferior in various circumstances, particularly due to 8GB VRAM.Price and Release DateThe GeForce RTX 5060 could be priced between $299 and $349. This is based on the previous pricing trends of the 60-class GPUs. The RTX 4060 was launched at $299 and NVIDIA is known for increasing the price with every generation. As far as the release goes, the RTX 5060 is reportedly coming in the first quarter of 2025 but the exact month and date are unknown.GeForce RTX 5060: What to Expect?The Geforce RTX 5060 should be seen as an ideal GPU for 1080p gaming but could also deliver excellent performance at 1440p on ultra settings. If the GPU ever gets a 12GB variant, the performance will be significantly better than the predecessors. With 8GB of VRAM, it won’t stand a chance in some modern titles but should do well in the majority of the games.The RTX 5060 is expected to deliver a solid raster and RT performance but will still likely fall in the budget GPU category. To reach the mid-range category, the VRAM and memory bus should be increased and uncompromised 1440p gaming will be its ultimate test.
Right now almost every other RTX 50-series GPU is reported to be more power-hungry than their predecessors, which means we could see TDP higher than 115W on the desktop variant. We will update this page once newer reports about its specs emerge.Specs Summary:Architecture: BlackwellGPU: GB206Cuda Cores: TBDVRAM: 8GB GDDR6, 128-bitMemory Bandwidth: 448 GB/sBase Clock: TBDBoost Clock: TBDTDP: TBDPerformanceAs far as the performance goes, we expect the RTX 5060 to provide about 30% uplifts over the RTX 4060. Some leaks did appear recently which suggested that the RTX 5060 laptop GPU is 32% faster than the RTX 4060 laptop GPU and so will be the desktop version but this leak seems to be unreliable. However, going by the generational uplifts NVIDIA offers in every new GPU generation, 30% is what we expect.The RTX 5060 should be targeting 1080p ultra settings but should be able to play most games at 1440p with ultra graphics presets as well. With just 8GB of memory, the GPU would throttle performance in some titles but in case there is a 12GB variant, this could be solved. That said, the ray tracing performance is expected to be enhanced noticeably as well.With DLSS 4.0 support, the performance could be boosted even further but raster performance on native resolution will determine its position in the GPU hierarchy. We expect the RTX 5060 to go on par with the RTX 4070 or RX 7800 XT but will still be somehow inferior in various circumstances, particularly due to 8GB VRAM.Price and Release DateThe GeForce RTX 5060 could be priced between $299 and $349. This is based on the previous pricing trends of the 60-class GPUs. The RTX 4060 was launched at $299 and NVIDIA is known for increasing the price with every generation. As far as the release goes, the RTX 5060 is reportedly coming in the first quarter of 2025 but the exact month and date are unknown.GeForce RTX 5060: What to Expect?The Geforce RTX 5060 should be seen as an ideal GPU for 1080p gaming but could also deliver excellent performance at 1440p on ultra settings. If the GPU ever gets a 12GB variant, the performance will be significantly better than the predecessors. With 8GB of VRAM, it won’t stand a chance in some modern titles but should do well in the majority of the games.The RTX 5060 is expected to deliver a solid raster and RT performance but will still likely fall in the budget GPU category. To reach the mid-range category, the VRAM and memory bus should be increased and uncompromised 1440p gaming will be its ultimate test.
Specs Summary:Architecture: BlackwellGPU: GB206Cuda Cores: TBDVRAM: 8GB GDDR6, 128-bitMemory Bandwidth: 448 GB/sBase Clock: TBDBoost Clock: TBDTDP: TBDPerformanceAs far as the performance goes, we expect the RTX 5060 to provide about 30% uplifts over the RTX 4060. Some leaks did appear recently which suggested that the RTX 5060 laptop GPU is 32% faster than the RTX 4060 laptop GPU and so will be the desktop version but this leak seems to be unreliable. However, going by the generational uplifts NVIDIA offers in every new GPU generation, 30% is what we expect.The RTX 5060 should be targeting 1080p ultra settings but should be able to play most games at 1440p with ultra graphics presets as well. With just 8GB of memory, the GPU would throttle performance in some titles but in case there is a 12GB variant, this could be solved. That said, the ray tracing performance is expected to be enhanced noticeably as well.With DLSS 4.0 support, the performance could be boosted even further but raster performance on native resolution will determine its position in the GPU hierarchy. We expect the RTX 5060 to go on par with the RTX 4070 or RX 7800 XT but will still be somehow inferior in various circumstances, particularly due to 8GB VRAM.Price and Release DateThe GeForce RTX 5060 could be priced between $299 and $349. This is based on the previous pricing trends of the 60-class GPUs. The RTX 4060 was launched at $299 and NVIDIA is known for increasing the price with every generation. As far as the release goes, the RTX 5060 is reportedly coming in the first quarter of 2025 but the exact month and date are unknown.GeForce RTX 5060: What to Expect?The Geforce RTX 5060 should be seen as an ideal GPU for 1080p gaming but could also deliver excellent performance at 1440p on ultra settings. If the GPU ever gets a 12GB variant, the performance will be significantly better than the predecessors. With 8GB of VRAM, it won’t stand a chance in some modern titles but should do well in the majority of the games.The RTX 5060 is expected to deliver a solid raster and RT performance but will still likely fall in the budget GPU category. To reach the mid-range category, the VRAM and memory bus should be increased and uncompromised 1440p gaming will be its ultimate test.
As far as the performance goes, we expect the RTX 5060 to provide about 30% uplifts over the RTX 4060. Some leaks did appear recently which suggested that the RTX 5060 laptop GPU is 32% faster than the RTX 4060 laptop GPU and so will be the desktop version but this leak seems to be unreliable. However, going by the generational uplifts NVIDIA offers in every new GPU generation, 30% is what we expect.
The RTX 5060 should be targeting 1080p ultra settings but should be able to play most games at 1440p with ultra graphics presets as well. With just 8GB of memory, the GPU would throttle performance in some titles but in case there is a 12GB variant, this could be solved. That said, the ray tracing performance is expected to be enhanced noticeably as well.With DLSS 4.0 support, the performance could be boosted even further but raster performance on native resolution will determine its position in the GPU hierarchy. We expect the RTX 5060 to go on par with the RTX 4070 or RX 7800 XT but will still be somehow inferior in various circumstances, particularly due to 8GB VRAM.Price and Release DateThe GeForce RTX 5060 could be priced between $299 and $349. This is based on the previous pricing trends of the 60-class GPUs. The RTX 4060 was launched at $299 and NVIDIA is known for increasing the price with every generation. As far as the release goes, the RTX 5060 is reportedly coming in the first quarter of 2025 but the exact month and date are unknown.GeForce RTX 5060: What to Expect?The Geforce RTX 5060 should be seen as an ideal GPU for 1080p gaming but could also deliver excellent performance at 1440p on ultra settings. If the GPU ever gets a 12GB variant, the performance will be significantly better than the predecessors. With 8GB of VRAM, it won’t stand a chance in some modern titles but should do well in the majority of the games.The RTX 5060 is expected to deliver a solid raster and RT performance but will still likely fall in the budget GPU category. To reach the mid-range category, the VRAM and memory bus should be increased and uncompromised 1440p gaming will be its ultimate test.
With DLSS 4.0 support, the performance could be boosted even further but raster performance on native resolution will determine its position in the GPU hierarchy. We expect the RTX 5060 to go on par with the RTX 4070 or RX 7800 XT but will still be somehow inferior in various circumstances, particularly due to 8GB VRAM.Price and Release DateThe GeForce RTX 5060 could be priced between $299 and $349. This is based on the previous pricing trends of the 60-class GPUs. The RTX 4060 was launched at $299 and NVIDIA is known for increasing the price with every generation. As far as the release goes, the RTX 5060 is reportedly coming in the first quarter of 2025 but the exact month and date are unknown.GeForce RTX 5060: What to Expect?The Geforce RTX 5060 should be seen as an ideal GPU for 1080p gaming but could also deliver excellent performance at 1440p on ultra settings. If the GPU ever gets a 12GB variant, the performance will be significantly better than the predecessors. With 8GB of VRAM, it won’t stand a chance in some modern titles but should do well in the majority of the games.The RTX 5060 is expected to deliver a solid raster and RT performance but will still likely fall in the budget GPU category. To reach the mid-range category, the VRAM and memory bus should be increased and uncompromised 1440p gaming will be its ultimate test.
The GeForce RTX 5060 could be priced between $299 and $349. This is based on the previous pricing trends of the 60-class GPUs. The RTX 4060 was launched at $299 and NVIDIA is known for increasing the price with every generation. As far as the release goes, the RTX 5060 is reportedly coming in the first quarter of 2025 but the exact month and date are unknown.
The Geforce RTX 5060 should be seen as an ideal GPU for 1080p gaming but could also deliver excellent performance at 1440p on ultra settings. If the GPU ever gets a 12GB variant, the performance will be significantly better than the predecessors. With 8GB of VRAM, it won’t stand a chance in some modern titles but should do well in the majority of the games.The RTX 5060 is expected to deliver a solid raster and RT performance but will still likely fall in the budget GPU category. To reach the mid-range category, the VRAM and memory bus should be increased and uncompromised 1440p gaming will be its ultimate test.
The RTX 5060 is expected to deliver a solid raster and RT performance but will still likely fall in the budget GPU category. To reach the mid-range category, the VRAM and memory bus should be increased and uncompromised 1440p gaming will be its ultimate test.
Your email address will not be published. Required fields are marked *
This site uses Akismet to reduce spam. Learn how your comment data is processed.
Xtremegaminerd.com is a participant in Amazon Associate Program and is supported by the readers. The qualifying purchase you make through our links may get us some commission and doesn't cost you an extra penny.
A publication dedicated to PC hardware, gaming, and general tech."
"NVIDIA","RTX 5060 Ti","GeForce RTX 5060 Ti 8GB performance revealed","https://hardforum.com/threads/geforce-rtx-5060-ti-8gb-performance-revealed-the-review-that-nvidia-doesnt-want-you-to-see.2040920/",""
"NVIDIA","RTX 5060 Ti","8GB and 16GB Nvidia GeForce RTX 5060 Ti GPUs listed","https://overclock3d.net/news/gpu-displays/8gb-and-16gb-nvidia-geforce-rtx-5060-ti-gpus-listed/",""
"NVIDIA","RTX 5060 Ti","GeForce RTX 5060 Ti Review with ZOTAC GAMING","https://smallformfactor.net/articles/geforce-rtx-5060-ti-review-with-zotac-gaming-rtx-5060-ti-twin-edge-oc/","Please note: Due to a short testing period, we will be adding updates to this article please check back regularly.
The embargo is over for the RTX 5060 Ti, and I can finally post the results of our review. It’s been an interesting launch for Nvidia as they have chosen not to sample the 8GB models of the RTX 5060 Ti. This is likely due to the fact that 8GB is simply not a sufficient amount of memory in 2025, and really should not have been launched as a 60 series card. Regardless of the reasoning, today we will be looking at the RTX 5060 Ti 16GB model.
I want to thank Zotac for sending us their Zotac Gaming RTX 5060 Ti Twin Edge OC model. Zotac has long been a supporter of small form factor PCs and SFFN in general by providing us industry contacts, news, and review samples. They have never asked us to edit or change any criticism, nor has Zotac placed any pressure on any SFFN staff for an expected review. This is excellent behavior from a company, and an indicator that Zotac is looking for a fair review.
Let’s be very clear before we begin: we are looking at the GeForce RTX 5060 Ti from a small form factor perspective. Small Form Factor users accept a lot of compromises to get their systems to their desired size, and we’re looking at this from a different angle than a traditional review. We will, of course, have game performance numbers for the card, as well as performance samples. However, our conclusion will be focused more on how the card allows for small form factor builds than raw performance and value.
As I said in past reviews: Small Form Factor users aren’t immune to wanting a good value. However, it’s hard to take a moral high ground on value when we decided to pay three times as much for cases that use one-quarter the metal, to house components that have half the function at twice the price. What do we do we these spacing saving builds? Put them prominently on our desks to take up space, and complain about how loud they are.
My own personal bias includes a general like of the Zotac esthetic, and I purchased a Zotac Magnus EN1070 SFF PC for myself approximately half a decade ago, that I reviewed favorably. I had a single support related issue with the EN1070, and it was resolved with a positive outcome.
The TLDR: The 5060TI is what the 4060TI probably should have been from the outset. It’s 10 to 20% faster than the 4060TI, sometimes a bit more. Pricing is reduced from an MSRP of $499 for the 16GB 4060Ti to $429 for the 16GB 5060Ti. While there is a cost reduced 8GB model, I do not recommend it for anyone considering this card. If you buy an 8GB card now in 2025, it will cause issues in modern games relatively soon. Nvidia does have their memory compression technology, however it’s not an industry standard and as such, is subject to developers implementing it as well as Nvidia continuing to support it.
Unfortunately, I’ve had only 3 work days to test our RTX 5060Ti. As such, this will be a living article with future updates being added. Notes will be made at the top of the article for changes.
First, let’s discuss the new Twin Edge card vs the last Twin Edge that I tested; the Zotac GAMING RTX 4070 Super Twin Edge OC. This model has improved substantially upon the performance of the last Twin Edge. I was skeptical at first due to the general aesthetic of the card, but it appears Zotac has made strong progress to improving both the thermal and sound profiles of their GPUs.
Not only is it cooler and quieter, it’s also smaller. The card has shrunk to 220.5mm in length from 234.1mm for the RTX 4070 Super Twin Edge, and 225.5mm from the RTX 4060Ti Twin Edge. The card has grown slightly from 40.2mm thick to 41.6mm thick so watch your clearances. Unlike the older RTX 4060 Ti Twin Edge, there is not a bulge in the middle that takes up additional space. The card is flat.
I test fit the card into two variations of the Densium 4 case. The first Densium was a generation one unit that fit the card by length, but the side panel warped and touched one of the fans. The second Densium was the revised model, and it had no fan issues. I will be test fitting this card into both a Ghost S1 and a Dan A4-SFX in a follow-up to this review at a later date.
Users should be wary of their side panels to ensure they aren’t bowing inward.
Our last Zotac card, the 4070 Super Twin Edge, had a notable coil whine. For the ZOTAC GAMING RTX 5060 Ti Twin Edge OC our test unit for the RTX 5060Ti has remarkably little. While this can vary from setup to setup and card to card, it’s refreshing to get a sample with so little.
To determine this, we first look at the fan speed 40dBA noise normalized performance, and then add a load to the GPU. The difference is the coil whine.


Below is our Coil Whine scale.
Coil Whine Scale

Inaudible – Cannot be heard. The coil whine is either not present or too low to heard or measured against the noise produced by the fans.


Barely Audible – Can barely be heard and requires close inspection to determine if the whine is coming from the GPU itself. The noise is immediately drowned out by music or games at even low volumes.


Moderately Audible – Can be heard from a typical seating position and is clearly coming from the GPU. However, the sound is drowned out by gaming and music at normal listening volumes, and cannot be heard through closed back headphones when no sound is playing, while in a normal sitting position. Steel Series Arctis Pro Wireless headphones were used during testing. Most GPUs I’ve tested recently are at this level.


Audible – Clearly coming from the GPU and clearly audible at seating position. Can be heard while listening to music or games at normal volumes through speakers in a desktop position. Cannot be heard over music or games at normal listening volume while using closed back headphones except in silent moments. Can be quietly heard through closed back headphones when no sound is being played.


Very Audible – Audible at seating position and disturbing to games or gaming at all volumes while using speakers. Can be heard through closed back headphones while playing music and games at even loud volumes. Would cause most users to return the GPU even if supplies are limited.


Defective – Loud and clearly defective component. Can drown out conversation, and make even loud gaming and music impossible.

Based on the testing conducted on the ZOTAC GAMING GeForce RTX 5060 Ti Twin Edge OC, show BARELY AUDIBLE coil whine on the above scale. This is a better rating that my own personal EVGA GeForce RTX 3080 XC3 achieved, and was far quieter than my AMD Radeon RX 6900XT Reference design. The Zotac RTX 4070 Super Twin Edge OC we previously tested rated as Audible. Other samples may vary.



Acoustic Performance
The acoustic performance of the Zotac GAMING RTX 5060 Ti Twin Edge OC is very good. Under stock conditions, I recorded a barely audible rise from  the ambient of 34.9 dBA of my office to 36.7 dBA at 75mm distance on-axis to the fans. At 90 degrees off-axis, I measured 36 dBA for the same load.

Temperature at fixed RPMS 

The Zotac GAMING RTX 5060Ti Twin Edge OC cooling performance scaled well from 30% fan speed at 79.0C to 100% fan speed at 53C. 75% appears to be the point of diminishing returns in terms of more performance per RPM. In the other direction, below 40% RPM the card begins to heat rapidly, but never exceeded thermal design limits.
Of course this is an open bench. Placing the card inside a restrictive case will diminish the results seen here. In other words: GPU get hotter and fan go faster.

Clock Speed and Temperatures
Given that the 5060Ti draws usually between 160 to 180 watts under load compared to the RTX 4070 Supers 220 to 230 watts, the card being easier to cool was a given. The performance was impressive none the less. At a noise normalized 40 dBA, the card was able to achieve a temperature of 62C at 1,921 RPM. The sound profile itself was smooth, and did not suffer the audible oscillations of the older 4070 Super Twin Edge. Core clocks were a stable 2,775 MHz.
The Zotac Gaming RTX 5060Ti Twin Edge OC performed admirably in our temperature vs core clock test. Unlike the RTX 4070 Super Twin Edge which had a 200 MHz deviation at stock power levels, there was only a 45 MHz difference between 100% fan speed, and 30% fan speed at full load. Left to its own devices, the card settled in at 2,775 MHz.

Overclocking Potential
There appears to be good potential to overclock the Zotac Gaming RTX 5060 Ti Twin Edge OC. The fan speed was set to the noise normalized 40 dBA for this test. Achieving a +100 on the core was as easy as dialing it in. This was stable for 20 loops of 3DMARK Speedway. Temperatures did not rise more than 2C compared to stock with both +10 core power and +100 MHz. While I was time restricted, I found the card to be stable at +150 core and +500 memory.
 
Undervolting 
 
Unfortunately, there was little shown during undervolting attempts, and I believe this to be a driver error with the review drivers. Reducing the power to 83% yielded 149 watts power consumption vs 162 watts at stock, and a 2,715 MHz core clock under load.

Performance 
As we mentioned above, the general consensus is that the 5060TI is about 10 to 20 percent faster than the RTX 4060Ti at 1080P and 1440P resolutions. However, this changes when memory bandwidth becomes the limiting factor at high resolutions. The 5060 Ti has substantially more memory bandwidth than the 4060 Ti, and can use that bandwidth to open up some gaming opportunities at 4K resolutions. These will be at console like frame rates, but it’s better to have the option. This is even more pronounced in games with ray tracing and large texture pools that exceed 8GB, which can effect all resolutions. While the 4060Ti did offer a 16GB model, the 8GB was the far more popular choice by consumers, and the 16GB model didn’t offer any additional bandwidth.
Upgrading solely for performance from the RTX 4060 Ti to the RTX 5060 Ti is not particularly advisable, but those SFF users still using the RTX 3060 or RTX 3060 Ti will see a nice improvement in the same power envelope. Our objective benchmarks will be added as the charts are completed, but we saw substantial uplifts over the 8GB RTX 3060 Ti in many modern scenarios. We will be adding our charts over the next day as we refine them.
That said, lets look at a few game examples of pushing the RTX 5060 Ti 16GB hard.
Keep in mind that we recorded this videos using the internal GeForce recording on the RTX 5060 Ti. This does effect performance by 5 to 10%.
 
Immortals of Aveum 
 
In this example, we have a cinematic cut-scene that pushes all of the effects of the UE5 engine. The resolution was set to 4K, and DLSS mode was set to performance. I found the Image quality to be very good, especially if you’re playing from a living room HTPC distance, and exceeded that of the PS5.
 

 
In this next example, we changed it to 1440P Resolution, and moved to DLSS Quality mode. This provided an excellent experience playing the game with well above 60FPS performance. There are some notable stutters, but I believe they are linked to the test driver at this time.
 

 
Cyberpunk 2077
Cyberpunk is still a bear to run for most people especially for the Path Tracing mode. Let’s look at some gameplay examples. In these examples, I chose an area with lots of lighting, geometry, and picked a fight with multiple factions. This is a tough spot for any GPU.
Our first test is pure rasterization at 4K Ultra Settings. DLSS was set to performance. The game was able to get a solid 50 to 60FPS in this scenario, and again looked great, especially from a couch gaming perspective.
 

 
Next we looked at 1440P Ultra Ray Tracing with DLSS Balanced. Again, we’re getting a solid 50 to 60 FPS. It looked good, and was very playable to my eye.
 

 
What does it take to get Path Tracing running on the RTX 5060 Ti? Well, this might be pushing it for this card. Resolution was set to 1080P with DLSS Balanced. It didn’t look bad, and the YouTube compression makes it look worse than it is, but I would hesitate to play this on a desk monitor. This would be better served at living room couch-viewing distance to hide some of the visual flaws.
 

 
For this next example, we will use Path Tracing and 1080P resolution again. However, we’re going to bump up to DLSS Quality, and activate Frame Generation 2X. This will render one AI frame for every real frame. It does cause some visual distortions, but it was better than expected when I recorded it. Again, best served at livingroom couch-viewing distance.
 

 
The Ascent 
 
The Ascent is a three-quarter perspective twin-stick shooter with some very impressive graphics that include ray tracing. For our first example we have 4K with RT and DLSS Performance on. The game looks excellent from both desk and couch-viewing distance. It played very well between 50 and 60 FPS.
 

 
For our next example of The Ascent we set the resolution to 1440P with RT and DLSS Quality. The game played extremely well, looked great, and was running well above 60 FPS.
 

 
We will have far more performance information over the upcoming days. The article will be updated with new performance charts and video links as they become available.
 
Case Pairing
So what case should you pair the ZOTAC GAMING RTX 5060 Ti Twin Edge OC with? There is a real opportunity to break out some of the foundation cases to the modern SFF community. Cases such as a the Dan A4-SFX, original NCASE M1,and Louqe Ghost S1 come to mind. The card would also work well with the Thorzone Mjolnir, and the thermal performance would work well for the Louqe Raw S1. Even the original Silverstone SGO5 would work with this GPU. Unfortunately, the case my mind first went to, the Jonsbo T6, is 5mm too short, however the Silverstone Sugo 16 would pair nicely with it.
Personally, I lean toward the Sugo 16 as far as powerful air-cooled cases. You could easily put any Ryzen AM5 CPU in it with a larger air cooler, and the Twin Edge could draw fresh air from the top.
Alternatively, I like the idea of using the Densium 4 Plus as sold by Overtek. It’s down to the mm, and you are limited to 39mm CPU coolers with Flex-ATX PSUS, but that would be a very potent 5.49L build; something I may experiment with later.
 
Image Credit: Overtek
 
Overall:
 
Do we recommend the ZOTAC GAMING RTX 5060 Ti Twin Edge OC if you’re looking for a RTX 5060 Ti? Absolutely. Zotac has done a great job with this card, and continues to improve with each generation. I strongly encourage Zotac to consider a single fan SOLO model for the RTX 5060 TI, and to continue to push closer to 200mm length for each upcoming revision. We also appreciate the single 8-pin power cable.
As for the RTX 5060 Ti itself, well that’s another story:
I wish it had come in at $399 or even $349 for the 16GB model. $429, while better than the $499 of the 4060TI 16GB, is still a hard price for the average consumer to swallow. We’ve seen a lot of price creep and inflation. However, a 100% rise in price since Pascal (GTX 1060) is excessive. There are market factors, but generally speaking, prices in technology have always moved downward. Now we’re seeing them rise up at staggering rates.
What the actual street price will be is anyone’s guess now.
It’s a tough sell to 4060TI owners who aren’t seeing VRAM limits in their games. The new media engine is excellent for creators that can take advantage of it, but the mere fact there is an 8GB model, which wasn’t sampled, is ridiculous in 2025.
Additionally, if street prices and tariffs put it deep into the $500 or even $600 range, then the RX 9070 and RX 9070XT become the obvious choice.
However, from an SFF perspective, and with the death of the 4070 series availability,  the RTX 5060Ti is likely going to be the fastest card you can get for the Sub 6L cases. It does only pull about 180 watts max when under full load, while also providing the best performance at that power level we’ve ever seen.
I’m confident that several single fan units will eventually arrive on the market, though I expect them to be hotter and nosier than the Zotac Twin Edge model which are recommending.
 

 



 


Revenant 



View all 482 articles 






Based on the testing conducted on the ZOTAC GAMING GeForce RTX 5060 Ti Twin Edge OC, show BARELY AUDIBLE coil whine on the above scale. This is a better rating that my own personal EVGA GeForce RTX 3080 XC3 achieved, and was far quieter than my AMD Radeon RX 6900XT Reference design. The Zotac RTX 4070 Super Twin Edge OC we previously tested rated as Audible. Other samples may vary.



Acoustic Performance
The acoustic performance of the Zotac GAMING RTX 5060 Ti Twin Edge OC is very good. Under stock conditions, I recorded a barely audible rise from  the ambient of 34.9 dBA of my office to 36.7 dBA at 75mm distance on-axis to the fans. At 90 degrees off-axis, I measured 36 dBA for the same load.

Temperature at fixed RPMS 

The Zotac GAMING RTX 5060Ti Twin Edge OC cooling performance scaled well from 30% fan speed at 79.0C to 100% fan speed at 53C. 75% appears to be the point of diminishing returns in terms of more performance per RPM. In the other direction, below 40% RPM the card begins to heat rapidly, but never exceeded thermal design limits.
Of course this is an open bench. Placing the card inside a restrictive case will diminish the results seen here. In other words: GPU get hotter and fan go faster.

Clock Speed and Temperatures
Given that the 5060Ti draws usually between 160 to 180 watts under load compared to the RTX 4070 Supers 220 to 230 watts, the card being easier to cool was a given. The performance was impressive none the less. At a noise normalized 40 dBA, the card was able to achieve a temperature of 62C at 1,921 RPM. The sound profile itself was smooth, and did not suffer the audible oscillations of the older 4070 Super Twin Edge. Core clocks were a stable 2,775 MHz.
The Zotac Gaming RTX 5060Ti Twin Edge OC performed admirably in our temperature vs core clock test. Unlike the RTX 4070 Super Twin Edge which had a 200 MHz deviation at stock power levels, there was only a 45 MHz difference between 100% fan speed, and 30% fan speed at full load. Left to its own devices, the card settled in at 2,775 MHz.

Overclocking Potential
There appears to be good potential to overclock the Zotac Gaming RTX 5060 Ti Twin Edge OC. The fan speed was set to the noise normalized 40 dBA for this test. Achieving a +100 on the core was as easy as dialing it in. This was stable for 20 loops of 3DMARK Speedway. Temperatures did not rise more than 2C compared to stock with both +10 core power and +100 MHz. While I was time restricted, I found the card to be stable at +150 core and +500 memory.
 
Undervolting 
 
Unfortunately, there was little shown during undervolting attempts, and I believe this to be a driver error with the review drivers. Reducing the power to 83% yielded 149 watts power consumption vs 162 watts at stock, and a 2,715 MHz core clock under load.

Performance 
As we mentioned above, the general consensus is that the 5060TI is about 10 to 20 percent faster than the RTX 4060Ti at 1080P and 1440P resolutions. However, this changes when memory bandwidth becomes the limiting factor at high resolutions. The 5060 Ti has substantially more memory bandwidth than the 4060 Ti, and can use that bandwidth to open up some gaming opportunities at 4K resolutions. These will be at console like frame rates, but it’s better to have the option. This is even more pronounced in games with ray tracing and large texture pools that exceed 8GB, which can effect all resolutions. While the 4060Ti did offer a 16GB model, the 8GB was the far more popular choice by consumers, and the 16GB model didn’t offer any additional bandwidth.
Upgrading solely for performance from the RTX 4060 Ti to the RTX 5060 Ti is not particularly advisable, but those SFF users still using the RTX 3060 or RTX 3060 Ti will see a nice improvement in the same power envelope. Our objective benchmarks will be added as the charts are completed, but we saw substantial uplifts over the 8GB RTX 3060 Ti in many modern scenarios. We will be adding our charts over the next day as we refine them.
That said, lets look at a few game examples of pushing the RTX 5060 Ti 16GB hard.
Keep in mind that we recorded this videos using the internal GeForce recording on the RTX 5060 Ti. This does effect performance by 5 to 10%.
 
Immortals of Aveum 
 
In this example, we have a cinematic cut-scene that pushes all of the effects of the UE5 engine. The resolution was set to 4K, and DLSS mode was set to performance. I found the Image quality to be very good, especially if you’re playing from a living room HTPC distance, and exceeded that of the PS5.
 

 
In this next example, we changed it to 1440P Resolution, and moved to DLSS Quality mode. This provided an excellent experience playing the game with well above 60FPS performance. There are some notable stutters, but I believe they are linked to the test driver at this time.
 

 
Cyberpunk 2077
Cyberpunk is still a bear to run for most people especially for the Path Tracing mode. Let’s look at some gameplay examples. In these examples, I chose an area with lots of lighting, geometry, and picked a fight with multiple factions. This is a tough spot for any GPU.
Our first test is pure rasterization at 4K Ultra Settings. DLSS was set to performance. The game was able to get a solid 50 to 60FPS in this scenario, and again looked great, especially from a couch gaming perspective.
 

 
Next we looked at 1440P Ultra Ray Tracing with DLSS Balanced. Again, we’re getting a solid 50 to 60 FPS. It looked good, and was very playable to my eye.
 

 
What does it take to get Path Tracing running on the RTX 5060 Ti? Well, this might be pushing it for this card. Resolution was set to 1080P with DLSS Balanced. It didn’t look bad, and the YouTube compression makes it look worse than it is, but I would hesitate to play this on a desk monitor. This would be better served at living room couch-viewing distance to hide some of the visual flaws.
 

 
For this next example, we will use Path Tracing and 1080P resolution again. However, we’re going to bump up to DLSS Quality, and activate Frame Generation 2X. This will render one AI frame for every real frame. It does cause some visual distortions, but it was better than expected when I recorded it. Again, best served at livingroom couch-viewing distance.
 

 
The Ascent 
 
The Ascent is a three-quarter perspective twin-stick shooter with some very impressive graphics that include ray tracing. For our first example we have 4K with RT and DLSS Performance on. The game looks excellent from both desk and couch-viewing distance. It played very well between 50 and 60 FPS.
 

 
For our next example of The Ascent we set the resolution to 1440P with RT and DLSS Quality. The game played extremely well, looked great, and was running well above 60 FPS.
 

 
We will have far more performance information over the upcoming days. The article will be updated with new performance charts and video links as they become available.
 
Case Pairing
So what case should you pair the ZOTAC GAMING RTX 5060 Ti Twin Edge OC with? There is a real opportunity to break out some of the foundation cases to the modern SFF community. Cases such as a the Dan A4-SFX, original NCASE M1,and Louqe Ghost S1 come to mind. The card would also work well with the Thorzone Mjolnir, and the thermal performance would work well for the Louqe Raw S1. Even the original Silverstone SGO5 would work with this GPU. Unfortunately, the case my mind first went to, the Jonsbo T6, is 5mm too short, however the Silverstone Sugo 16 would pair nicely with it.
Personally, I lean toward the Sugo 16 as far as powerful air-cooled cases. You could easily put any Ryzen AM5 CPU in it with a larger air cooler, and the Twin Edge could draw fresh air from the top.
Alternatively, I like the idea of using the Densium 4 Plus as sold by Overtek. It’s down to the mm, and you are limited to 39mm CPU coolers with Flex-ATX PSUS, but that would be a very potent 5.49L build; something I may experiment with later.
 
Image Credit: Overtek
 
Overall:
 
Do we recommend the ZOTAC GAMING RTX 5060 Ti Twin Edge OC if you’re looking for a RTX 5060 Ti? Absolutely. Zotac has done a great job with this card, and continues to improve with each generation. I strongly encourage Zotac to consider a single fan SOLO model for the RTX 5060 TI, and to continue to push closer to 200mm length for each upcoming revision. We also appreciate the single 8-pin power cable.
As for the RTX 5060 Ti itself, well that’s another story:
I wish it had come in at $399 or even $349 for the 16GB model. $429, while better than the $499 of the 4060TI 16GB, is still a hard price for the average consumer to swallow. We’ve seen a lot of price creep and inflation. However, a 100% rise in price since Pascal (GTX 1060) is excessive. There are market factors, but generally speaking, prices in technology have always moved downward. Now we’re seeing them rise up at staggering rates.
What the actual street price will be is anyone’s guess now.
It’s a tough sell to 4060TI owners who aren’t seeing VRAM limits in their games. The new media engine is excellent for creators that can take advantage of it, but the mere fact there is an 8GB model, which wasn’t sampled, is ridiculous in 2025.
Additionally, if street prices and tariffs put it deep into the $500 or even $600 range, then the RX 9070 and RX 9070XT become the obvious choice.
However, from an SFF perspective, and with the death of the 4070 series availability,  the RTX 5060Ti is likely going to be the fastest card you can get for the Sub 6L cases. It does only pull about 180 watts max when under full load, while also providing the best performance at that power level we’ve ever seen.
I’m confident that several single fan units will eventually arrive on the market, though I expect them to be hotter and nosier than the Zotac Twin Edge model which are recommending.
 

 



 


Revenant 



View all 482 articles 







Acoustic Performance
The acoustic performance of the Zotac GAMING RTX 5060 Ti Twin Edge OC is very good. Under stock conditions, I recorded a barely audible rise from  the ambient of 34.9 dBA of my office to 36.7 dBA at 75mm distance on-axis to the fans. At 90 degrees off-axis, I measured 36 dBA for the same load.

Temperature at fixed RPMS 

The Zotac GAMING RTX 5060Ti Twin Edge OC cooling performance scaled well from 30% fan speed at 79.0C to 100% fan speed at 53C. 75% appears to be the point of diminishing returns in terms of more performance per RPM. In the other direction, below 40% RPM the card begins to heat rapidly, but never exceeded thermal design limits.
Of course this is an open bench. Placing the card inside a restrictive case will diminish the results seen here. In other words: GPU get hotter and fan go faster.

Clock Speed and Temperatures
Given that the 5060Ti draws usually between 160 to 180 watts under load compared to the RTX 4070 Supers 220 to 230 watts, the card being easier to cool was a given. The performance was impressive none the less. At a noise normalized 40 dBA, the card was able to achieve a temperature of 62C at 1,921 RPM. The sound profile itself was smooth, and did not suffer the audible oscillations of the older 4070 Super Twin Edge. Core clocks were a stable 2,775 MHz.
The Zotac Gaming RTX 5060Ti Twin Edge OC performed admirably in our temperature vs core clock test. Unlike the RTX 4070 Super Twin Edge which had a 200 MHz deviation at stock power levels, there was only a 45 MHz difference between 100% fan speed, and 30% fan speed at full load. Left to its own devices, the card settled in at 2,775 MHz.

Overclocking Potential
There appears to be good potential to overclock the Zotac Gaming RTX 5060 Ti Twin Edge OC. The fan speed was set to the noise normalized 40 dBA for this test. Achieving a +100 on the core was as easy as dialing it in. This was stable for 20 loops of 3DMARK Speedway. Temperatures did not rise more than 2C compared to stock with both +10 core power and +100 MHz. While I was time restricted, I found the card to be stable at +150 core and +500 memory.
 
Undervolting 
 
Unfortunately, there was little shown during undervolting attempts, and I believe this to be a driver error with the review drivers. Reducing the power to 83% yielded 149 watts power consumption vs 162 watts at stock, and a 2,715 MHz core clock under load.

Performance 
As we mentioned above, the general consensus is that the 5060TI is about 10 to 20 percent faster than the RTX 4060Ti at 1080P and 1440P resolutions. However, this changes when memory bandwidth becomes the limiting factor at high resolutions. The 5060 Ti has substantially more memory bandwidth than the 4060 Ti, and can use that bandwidth to open up some gaming opportunities at 4K resolutions. These will be at console like frame rates, but it’s better to have the option. This is even more pronounced in games with ray tracing and large texture pools that exceed 8GB, which can effect all resolutions. While the 4060Ti did offer a 16GB model, the 8GB was the far more popular choice by consumers, and the 16GB model didn’t offer any additional bandwidth.
Upgrading solely for performance from the RTX 4060 Ti to the RTX 5060 Ti is not particularly advisable, but those SFF users still using the RTX 3060 or RTX 3060 Ti will see a nice improvement in the same power envelope. Our objective benchmarks will be added as the charts are completed, but we saw substantial uplifts over the 8GB RTX 3060 Ti in many modern scenarios. We will be adding our charts over the next day as we refine them.
That said, lets look at a few game examples of pushing the RTX 5060 Ti 16GB hard.
Keep in mind that we recorded this videos using the internal GeForce recording on the RTX 5060 Ti. This does effect performance by 5 to 10%.
 
Immortals of Aveum 
 
In this example, we have a cinematic cut-scene that pushes all of the effects of the UE5 engine. The resolution was set to 4K, and DLSS mode was set to performance. I found the Image quality to be very good, especially if you’re playing from a living room HTPC distance, and exceeded that of the PS5.
 

 
In this next example, we changed it to 1440P Resolution, and moved to DLSS Quality mode. This provided an excellent experience playing the game with well above 60FPS performance. There are some notable stutters, but I believe they are linked to the test driver at this time.
 

 
Cyberpunk 2077
Cyberpunk is still a bear to run for most people especially for the Path Tracing mode. Let’s look at some gameplay examples. In these examples, I chose an area with lots of lighting, geometry, and picked a fight with multiple factions. This is a tough spot for any GPU.
Our first test is pure rasterization at 4K Ultra Settings. DLSS was set to performance. The game was able to get a solid 50 to 60FPS in this scenario, and again looked great, especially from a couch gaming perspective.
 

 
Next we looked at 1440P Ultra Ray Tracing with DLSS Balanced. Again, we’re getting a solid 50 to 60 FPS. It looked good, and was very playable to my eye.
 

 
What does it take to get Path Tracing running on the RTX 5060 Ti? Well, this might be pushing it for this card. Resolution was set to 1080P with DLSS Balanced. It didn’t look bad, and the YouTube compression makes it look worse than it is, but I would hesitate to play this on a desk monitor. This would be better served at living room couch-viewing distance to hide some of the visual flaws.
 

 
For this next example, we will use Path Tracing and 1080P resolution again. However, we’re going to bump up to DLSS Quality, and activate Frame Generation 2X. This will render one AI frame for every real frame. It does cause some visual distortions, but it was better than expected when I recorded it. Again, best served at livingroom couch-viewing distance.
 

 
The Ascent 
 
The Ascent is a three-quarter perspective twin-stick shooter with some very impressive graphics that include ray tracing. For our first example we have 4K with RT and DLSS Performance on. The game looks excellent from both desk and couch-viewing distance. It played very well between 50 and 60 FPS.
 

 
For our next example of The Ascent we set the resolution to 1440P with RT and DLSS Quality. The game played extremely well, looked great, and was running well above 60 FPS.
 

 
We will have far more performance information over the upcoming days. The article will be updated with new performance charts and video links as they become available.
 
Case Pairing
So what case should you pair the ZOTAC GAMING RTX 5060 Ti Twin Edge OC with? There is a real opportunity to break out some of the foundation cases to the modern SFF community. Cases such as a the Dan A4-SFX, original NCASE M1,and Louqe Ghost S1 come to mind. The card would also work well with the Thorzone Mjolnir, and the thermal performance would work well for the Louqe Raw S1. Even the original Silverstone SGO5 would work with this GPU. Unfortunately, the case my mind first went to, the Jonsbo T6, is 5mm too short, however the Silverstone Sugo 16 would pair nicely with it.
Personally, I lean toward the Sugo 16 as far as powerful air-cooled cases. You could easily put any Ryzen AM5 CPU in it with a larger air cooler, and the Twin Edge could draw fresh air from the top.
Alternatively, I like the idea of using the Densium 4 Plus as sold by Overtek. It’s down to the mm, and you are limited to 39mm CPU coolers with Flex-ATX PSUS, but that would be a very potent 5.49L build; something I may experiment with later.
 
Image Credit: Overtek
 
Overall:
 
Do we recommend the ZOTAC GAMING RTX 5060 Ti Twin Edge OC if you’re looking for a RTX 5060 Ti? Absolutely. Zotac has done a great job with this card, and continues to improve with each generation. I strongly encourage Zotac to consider a single fan SOLO model for the RTX 5060 TI, and to continue to push closer to 200mm length for each upcoming revision. We also appreciate the single 8-pin power cable.
As for the RTX 5060 Ti itself, well that’s another story:
I wish it had come in at $399 or even $349 for the 16GB model. $429, while better than the $499 of the 4060TI 16GB, is still a hard price for the average consumer to swallow. We’ve seen a lot of price creep and inflation. However, a 100% rise in price since Pascal (GTX 1060) is excessive. There are market factors, but generally speaking, prices in technology have always moved downward. Now we’re seeing them rise up at staggering rates.
What the actual street price will be is anyone’s guess now.
It’s a tough sell to 4060TI owners who aren’t seeing VRAM limits in their games. The new media engine is excellent for creators that can take advantage of it, but the mere fact there is an 8GB model, which wasn’t sampled, is ridiculous in 2025.
Additionally, if street prices and tariffs put it deep into the $500 or even $600 range, then the RX 9070 and RX 9070XT become the obvious choice.
However, from an SFF perspective, and with the death of the 4070 series availability,  the RTX 5060Ti is likely going to be the fastest card you can get for the Sub 6L cases. It does only pull about 180 watts max when under full load, while also providing the best performance at that power level we’ve ever seen.
I’m confident that several single fan units will eventually arrive on the market, though I expect them to be hotter and nosier than the Zotac Twin Edge model which are recommending.
 

 



 


Revenant 



View all 482 articles 






The acoustic performance of the Zotac GAMING RTX 5060 Ti Twin Edge OC is very good. Under stock conditions, I recorded a barely audible rise from  the ambient of 34.9 dBA of my office to 36.7 dBA at 75mm distance on-axis to the fans. At 90 degrees off-axis, I measured 36 dBA for the same load.
The Zotac GAMING RTX 5060Ti Twin Edge OC cooling performance scaled well from 30% fan speed at 79.0C to 100% fan speed at 53C. 75% appears to be the point of diminishing returns in terms of more performance per RPM. In the other direction, below 40% RPM the card begins to heat rapidly, but never exceeded thermal design limits.
Of course this is an open bench. Placing the card inside a restrictive case will diminish the results seen here. In other words: GPU get hotter and fan go faster.
Given that the 5060Ti draws usually between 160 to 180 watts under load compared to the RTX 4070 Supers 220 to 230 watts, the card being easier to cool was a given. The performance was impressive none the less. At a noise normalized 40 dBA, the card was able to achieve a temperature of 62C at 1,921 RPM. The sound profile itself was smooth, and did not suffer the audible oscillations of the older 4070 Super Twin Edge. Core clocks were a stable 2,775 MHz.
The Zotac Gaming RTX 5060Ti Twin Edge OC performed admirably in our temperature vs core clock test. Unlike the RTX 4070 Super Twin Edge which had a 200 MHz deviation at stock power levels, there was only a 45 MHz difference between 100% fan speed, and 30% fan speed at full load. Left to its own devices, the card settled in at 2,775 MHz.
There appears to be good potential to overclock the Zotac Gaming RTX 5060 Ti Twin Edge OC. The fan speed was set to the noise normalized 40 dBA for this test. Achieving a +100 on the core was as easy as dialing it in. This was stable for 20 loops of 3DMARK Speedway. Temperatures did not rise more than 2C compared to stock with both +10 core power and +100 MHz. While I was time restricted, I found the card to be stable at +150 core and +500 memory.
Unfortunately, there was little shown during undervolting attempts, and I believe this to be a driver error with the review drivers. Reducing the power to 83% yielded 149 watts power consumption vs 162 watts at stock, and a 2,715 MHz core clock under load.
As we mentioned above, the general consensus is that the 5060TI is about 10 to 20 percent faster than the RTX 4060Ti at 1080P and 1440P resolutions. However, this changes when memory bandwidth becomes the limiting factor at high resolutions. The 5060 Ti has substantially more memory bandwidth than the 4060 Ti, and can use that bandwidth to open up some gaming opportunities at 4K resolutions. These will be at console like frame rates, but it’s better to have the option. This is even more pronounced in games with ray tracing and large texture pools that exceed 8GB, which can effect all resolutions. While the 4060Ti did offer a 16GB model, the 8GB was the far more popular choice by consumers, and the 16GB model didn’t offer any additional bandwidth.
Upgrading solely for performance from the RTX 4060 Ti to the RTX 5060 Ti is not particularly advisable, but those SFF users still using the RTX 3060 or RTX 3060 Ti will see a nice improvement in the same power envelope. Our objective benchmarks will be added as the charts are completed, but we saw substantial uplifts over the 8GB RTX 3060 Ti in many modern scenarios. We will be adding our charts over the next day as we refine them.
That said, lets look at a few game examples of pushing the RTX 5060 Ti 16GB hard.
Keep in mind that we recorded this videos using the internal GeForce recording on the RTX 5060 Ti. This does effect performance by 5 to 10%.
In this example, we have a cinematic cut-scene that pushes all of the effects of the UE5 engine. The resolution was set to 4K, and DLSS mode was set to performance. I found the Image quality to be very good, especially if you’re playing from a living room HTPC distance, and exceeded that of the PS5.
In this next example, we changed it to 1440P Resolution, and moved to DLSS Quality mode. This provided an excellent experience playing the game with well above 60FPS performance. There are some notable stutters, but I believe they are linked to the test driver at this time.
Cyberpunk is still a bear to run for most people especially for the Path Tracing mode. Let’s look at some gameplay examples. In these examples, I chose an area with lots of lighting, geometry, and picked a fight with multiple factions. This is a tough spot for any GPU.
Our first test is pure rasterization at 4K Ultra Settings. DLSS was set to performance. The game was able to get a solid 50 to 60FPS in this scenario, and again looked great, especially from a couch gaming perspective.
Next we looked at 1440P Ultra Ray Tracing with DLSS Balanced. Again, we’re getting a solid 50 to 60 FPS. It looked good, and was very playable to my eye.
What does it take to get Path Tracing running on the RTX 5060 Ti? Well, this might be pushing it for this card. Resolution was set to 1080P with DLSS Balanced. It didn’t look bad, and the YouTube compression makes it look worse than it is, but I would hesitate to play this on a desk monitor. This would be better served at living room couch-viewing distance to hide some of the visual flaws.
For this next example, we will use Path Tracing and 1080P resolution again. However, we’re going to bump up to DLSS Quality, and activate Frame Generation 2X. This will render one AI frame for every real frame. It does cause some visual distortions, but it was better than expected when I recorded it. Again, best served at livingroom couch-viewing distance.
The Ascent is a three-quarter perspective twin-stick shooter with some very impressive graphics that include ray tracing. For our first example we have 4K with RT and DLSS Performance on. The game looks excellent from both desk and couch-viewing distance. It played very well between 50 and 60 FPS.
For our next example of The Ascent we set the resolution to 1440P with RT and DLSS Quality. The game played extremely well, looked great, and was running well above 60 FPS.
We will have far more performance information over the upcoming days. The article will be updated with new performance charts and video links as they become available.
So what case should you pair the ZOTAC GAMING RTX 5060 Ti Twin Edge OC with? There is a real opportunity to break out some of the foundation cases to the modern SFF community. Cases such as a the Dan A4-SFX, original NCASE M1,and Louqe Ghost S1 come to mind. The card would also work well with the Thorzone Mjolnir, and the thermal performance would work well for the Louqe Raw S1. Even the original Silverstone SGO5 would work with this GPU. Unfortunately, the case my mind first went to, the Jonsbo T6, is 5mm too short, however the Silverstone Sugo 16 would pair nicely with it.
Personally, I lean toward the Sugo 16 as far as powerful air-cooled cases. You could easily put any Ryzen AM5 CPU in it with a larger air cooler, and the Twin Edge could draw fresh air from the top.
Alternatively, I like the idea of using the Densium 4 Plus as sold by Overtek. It’s down to the mm, and you are limited to 39mm CPU coolers with Flex-ATX PSUS, but that would be a very potent 5.49L build; something I may experiment with later.
Do we recommend the ZOTAC GAMING RTX 5060 Ti Twin Edge OC if you’re looking for a RTX 5060 Ti? Absolutely. Zotac has done a great job with this card, and continues to improve with each generation. I strongly encourage Zotac to consider a single fan SOLO model for the RTX 5060 TI, and to continue to push closer to 200mm length for each upcoming revision. We also appreciate the single 8-pin power cable.
As for the RTX 5060 Ti itself, well that’s another story:
I wish it had come in at $399 or even $349 for the 16GB model. $429, while better than the $499 of the 4060TI 16GB, is still a hard price for the average consumer to swallow. We’ve seen a lot of price creep and inflation. However, a 100% rise in price since Pascal (GTX 1060) is excessive. There are market factors, but generally speaking, prices in technology have always moved downward. Now we’re seeing them rise up at staggering rates.
What the actual street price will be is anyone’s guess now.
It’s a tough sell to 4060TI owners who aren’t seeing VRAM limits in their games. The new media engine is excellent for creators that can take advantage of it, but the mere fact there is an 8GB model, which wasn’t sampled, is ridiculous in 2025.
Additionally, if street prices and tariffs put it deep into the $500 or even $600 range, then the RX 9070 and RX 9070XT become the obvious choice.
However, from an SFF perspective, and with the death of the 4070 series availability,  the RTX 5060Ti is likely going to be the fastest card you can get for the Sub 6L cases. It does only pull about 180 watts max when under full load, while also providing the best performance at that power level we’ve ever seen.
I’m confident that several single fan units will eventually arrive on the market, though I expect them to be hotter and nosier than the Zotac Twin Edge model which are recommending."
"NVIDIA","RTX 5060 Ti","Review: PNY GeForce RTX 5060 Ti OC","https://www.wired.com/review/pny-geforce-rtx-5060-ti-oc","All products featured on WIRED are independently selected by our editors. However, when you buy something through our retail links, we may earn an affiliate commission.
Not everyone wants to spend $1,000 or more on a graphics card for their PC. I personally know way more people gaming on modest, midrange cards than I do people with the latest and greatest. The newly announced RTX 5060 Ti, with a sub-$500 MSRP, could be the card you've been waiting for to replace an aging RTX 20 or 30 Series card.
Unfortunately, that could also mean these cards are hard to come by. The higher-end 50 Series GPUs still require active effort to get a hold of, although they do seem to be trickling out to retailers. A lower price, plus weeks of low stock, might create a feeding frenzy.
If you're updating an older system that's tied to a 1080p screen at 120 Hz or less, you’ll be very pleased with the performance and ease of use. If you're building a new system in 2025, you should at least be targeting 1440p, and I think there are other cards that will have a longer life at that resolution.
While the more expensive 50-Series GPUs have a new 12V-2x6 connector, the PNY card I have just takes a classic 8-pin connector, which should help with compatibility in older systems. One minor nitpick is that the power connector itself is way over by the bracket at the back of the case. It’s technically closer to the power supply, but it may cause some awkwardness in modern gaming cases, or force you to re-run the cable.
Physically, it's extremely compact, which you'd expect from a card that's trying to squeeze into random leftover cases. The two fans might get the job done, but it certainly sounds louder than three, although these fans are quieter than my case fans during regular use. It has a silent mode as well, and I found the fans didn’t run all the time, especially at 1080p, so I don’t imagine this will be a bother.
I turn the settings all the way up for these games, which is what most folks will do when they get a new graphics card.
All products featured on WIRED are independently selected by our editors. However, we may receive compensation from retailers and/or from purchases of products through these links.
I love talking about the Steam Hardware Survey, which lets users contribute their system data to help guide developers in optimizing their games. A key detail here is that over half of the people surveyed have a 1080p main screen, with less than 20 percent playing at 1440p. With this card, we’re comfortably above 60 frames per second at 1080p in every game in our suite, which is great news for that majority of gamers who can’t or don’t want to upgrade their screen.
It's a little bit tougher at 1440p, but far from unplayable, at least for now. As games get more demanding, and with frame generation proliferating, you may find yourself outpaced within a couple years.
All products featured on WIRED are independently selected by our editors. However, we may receive compensation from retailers and/or from purchases of products through these links.
The 5060 Ti stacks up against the other cards basically where you'd expect performance-wise, but it's important to remember this is both the 16-GB variant and an overclocked model. The performance is just good enough for consistent 1080p at 60 fps that I expect you  ’ll have to compromise on settings with the 8-GB models. I’ll wait until I have them in hand to make a firm conclusion, but I imagine that is the lowest-performance card in the 50 Series that will be acceptable for modern games.
I’ve discussed Nvidia’s new multiframe generation tech in the other GPU reviews, which uses machine learning to create extra frames between the “real” rendered frames. On the high-end cards, multiframe generation was the only way to achieve high-refresh 4K gaming, but on this card it’s more useful for getting consistently smooth gameplay at high-refresh 1080p, or getting over 60 fps at 1440p.
In Cyberpunk 2077, I was able to turn all the settings up except Path Tracing, and set Ray Tracing to “Psycho” and use Frame Generation to get up over 60 FPS. I imagine most people will turn it on as they tinker, see a huge boost to performance without a meaningful hit to the image quality, and use it forever.
I don’t blame you for following their lead, and I think that with cinematic, single-player games in particular, you’ll have a great experience. If you play a lot of fast-paced shooters or twitchy games, you’ll probably want to drop your settings instead.
Anecdotally, the sentiment in my gaming nerd circles is that people aren't even remotely interested in paying over $1,000 for a graphics card. Even if GPUs were closer to their suggested retail price, they don’t want to spend that much to get a system up and running. Frame generation could be the thing to squeeze some more life out of these cards while still saving some cash, even if it’s technically a compromise. If you already own a decent 1080p card, there likely isn't a reason to upgrade, but if your older RTX 20 or 30 series card is on the fritz, and you can snag of of these for retail, it's a worthwhile gaming companion. Otherwise, check out our list of the Best GPUs for even more options.
All products featured on WIRED are independently selected by our editors. However, we may receive compensation from retailers and/or from purchases of products through these links.
© 2025 Condé Nast. All rights reserved. WIRED may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Condé Nast. Ad Choices"
"NVIDIA","RTX 5060 Ti","ZOTAC GAMING GeForce RTX 5060 Ti Twin Edge OC Review","https://www.techpowerup.com/review/zotac-gaming-geforce-rtx-5060-ti-twin-edge-oc/",""
"NVIDIA","RTX 5060 Ti","More Marketing BS: NVIDIA GeForce RTX 5060 Ti Review","https://gamersnexus.net/gpus/more-marketing-bs-nvidia-geforce-rtx-5060-ti-review-benchmarks-vs-gtx-1060-4060-ti-more","More Marketing BS: NVIDIA GeForce RTX 5060 Ti Review & Benchmarks vs GTX 1060, 4060 Ti, & More
The shortest possible conclusion upfront is that the 5060 Ti is about 13%-27% better than the 4060 Ti at 1440p, typically in the range of 20-25%. At 1080p, the new card is 11-24% better, typically about 18-20%. Against the 3060 Ti from 5 years ago, the 5060 Ti at 1440p is 16-39% improved, depending on the game. 1080p posted 21-40% gains, with a huge exception in Black Myth with ray tracing enabled, where there was a 56% uplift. The 3060 Ti was also the card that the 4060 Ti sometimes lost against.
For older devices or possible used candidates, the closest alternatives (by performance) to pay attention to in our charts will be the 3080 (watch our review) and 3070 Ti (watch our review), which often flank the 5060 Ti, and the 7700 XT (read our review) or 7800 XT (read our review) on AMD's side.
Editor's note: This was originally published on April 16, 2025 as a video. This content has been adapted to written format for this article and is unchanged from the original publication.
But pricing is the big challenge today. NVIDIA says that this card has an MSRP of $430, with the 8GB variant at $380 and RTX 5060 non-Ti at $300, launching in May. The 5060 Ti cards launch today with the reviews. We only have the 16GB model right now. We might look at the 8GB version later.
Full transparency up-front. We’re keeping this review as simple and focused as possible, mostly because we’re currently traveling with a big story we’re working on. We still have dozens of gaming charts, but we wanted to be clear on that. 
We’re also going to keep our concluding thoughts simple because we need to see how the actual pricing shakes-out before making firmer judgments, which means there’ll be more discussion in the coming weeks -- likely in HW News or potentially another dedicated story.
With that out of the way, here’s a quick version of the specs:
The short version is that NVIDIA’s RTX 5060 Ti ships in either 8GB or 16GB variants. The review samples we’re aware of are the 16GB model. There shouldn’t be any difference between these beyond the memory, from what we’re told. These cards have 4608 CUDA cores, 144 TMUs, and a gacha box of ROPs. Memory bandwidth is rated at 448GB/s with a memory bus of 128-bit, which is why we have the multiples of 8GB for memory.
The RTX 5070 technically has a lower memory capacity at 12GB. Theoretically, they could do a 24GB model, but these options stem from the bus width and controller choices.
The RTX 5060 non-Ti will ship in May and have 3840 CUDA cores with an 8GB framebuffer, also on a 128-bit bus.
AMD’s competition will include the, in theory, RX 9060 series, for which we don’t have full details yet. We’ll hear about that more likely next month.
We’re keeping it simple today, so let’s just get into the benchmarks.
This one was bad for the RTX 4060 Ti, with the card landing at an abysmal 41 FPS AVG as compared to the 3060 Ti FTW3’s 48 FPS AVG. RIP EVGA. We explained this regression in our 4060 Ti review previously, which we titled “Do Not Buy.” Spoiler alert: The conclusion was to not buy it.
The 5060 Ti isn’t really competing with the 4060 Ti here: It’s competing with the 3060 Ti, and against that, we see an uplift of 12% to 54 FPS AVG. The improvement over the 4060 Ti looks more impressive, but that’s because the 4060 Ti sucks. The uplift over its 41 FPS AVG was 31%. The RTX 5070’s 78 FPS AVG has it about 43% ahead of the 5060 Ti.
Used RTX 3070 (watch our review) and 3070 Ti cards might be worth exploring: We saw completed and sold listings on eBay ranging from $270 to $340, which would put them below the 5060 Ti if it even hits its marketed MSRP, which it probably won’t. The 3070 is about equal to the 5060 Ti, with the 3070 Ti slightly ahead.
AMD’s RX 7800 XT is its closest performer to the 5060 Ti, landing at 58 FPS AVG and leading the 5060 Ti by almost 7%. 
AMD’s RX 7600 (watch our review) falls way down the chart and runs at 32 FPS AVG. But then again, AMD does overall poorly in this particular game, with its 9070 XT (read our review) down below the RTX 5070. We talked about that in our 9070 series reviews.
Finally, NVIDIA’s claimed 50x performance increase over the GTX 1060 (watch our revisit) doesn’t come to fruition when not arbitrarily enabling and disabling favorable settings. The 1060 ran at 16 FPS AVG. 16 x 50 is 800 FPS, which would be 4x the performance of an RTX 5090 (read our review). We’ve gone beyond an RTX 5070 = 4090 and to a RTX 5060 Ti = RTX 9090. In reality, the 5060 Ti is 236% ahead. That’s still a big jump, but no need to stretch the truth about it.
At 1440p, the RTX 5060 Ti landed at 104 FPS AVG, which has it functionally tied with the 3070 Ti’s 108 FPS AVG and only slightly ahead of the 7700 XT’s 98 FPS AVG or 3070’s 97 FPS AVG. The lead over the 3060 Ti is now 16%, or about 25% ahead of the 4060 Ti. These gains are down from 4K. Lows are where you’d expect them for each card, with no meaningful differences.
The 5070 is about 47% ahead of the 5060 Ti, slightly up from the lead at 4K.
AMD’s 9070 ran at 126 FPS AVG here, producing a 22% advantage. The MSRP is higher, but then so is everything, including the companies that set these prices.
The GTX 1060 ran at 33 FPS AVG here, the 1650 (watch our review) at 23 FPS, 6500 XT (watch our review) at 31 FPS, and 3050 (watch our review) at 45 FPS AVG. Predictably, the 5060 Ti is a big improvement over all of these, but again, not 50x.
At 1080p, the 5060 Ti ran at 160 FPS AVG. That’s about the same as the 7700 XT and slightly ahead of the 3070 Ti. It’s finally moving up the relative ranking compared to Ampere. We think you’d still be better off with a used card right now. 
The 4060 Ti ran at 133 FPS AVG here, so the 5060 Ti improves by 20%. Against the 3060 Ti, the 5060 Ti is about 25% better. The 4060 Ti is finally better than the 3060 Ti when at 1080p, so those two have flipped as well.
The lower-end round-up includes the GTX 1060 at 51 FPS AVG, 3050 at 66, 1070 (watch our review) at 70, and 2060 (watch our review) at 83. The 6600 (watch our review) ran at 86 FPS AVG and AMD’s 7600 ran at 107 FPS AVG.
Black Myth: Wukong is up now, first at 4K. The 5060 Ti ran at 31 FPS AVG. That’s obviously unplayable and is because of the resolution and settings, but it’s still useful for relative scaling.
The result has it about equal to a 7800 XT. The 3080 leads by 18%, with the 5070 leading by 29%. The 3070 Ti ran a lower framerate than the 5060 Ti in this one.
At 1440p, the 5060 Ti ran at 57 FPS AVG and landed right between the 7800 XT and RTX 4070 (watch our review). These are all effectively tied. The 5070 held a 72 FPS AVG, or 27% ahead. That’s slightly down from 4K. The 5060 Ti is ahead of the 3070 Ti by 13%, the 4060 Ti by 27%, and the 3060 Ti by 34%. This is one of the games where the 4060 Ti and 3060 Ti are right next to each other. 
AMD’s 7600 ran at 31.6 FPS AVG and isn’t really in the same class of card as what we’re reviewing today. Its 7800 XT and 7900 GRE (read our review) are the most comparable, but in theory, the inbound 9060 XT should be fighting in this territory.
At 1080p, the 5060 Ti pushed 81 FPS average with lows where you’d expect given the average. There is no particularly special frametime consistency benefit.
The 5060 Ti ends up at about the same level as the RTX 3080 and RTX 4070 (watch our review). The 5070, which still doesn’t equal a 4090 (no matter what NVIDIA says), and its 98 FPS AVG puts it 21% ahead of the 5060 Ti.
As for the last generations, NVIDIA’s new 5060 Ti leads the 4060 Ti by 24%, or the 3060 Ti by 40%. 
AMD’s 7900 GRE is its closest card here, slightly ahead of the 5060 Ti, with the 7800 XT just behind. 
Intel’s B580 cards are at around 46 FPS AVG, which has them similar to the RX 7600.
In Starfield at 4K, the 5060 Ti ran at 41 FPS AVG with lows at 34 and 29, proportional to the cards around it. This puts the 3080 ahead of the 5060 Ti and the 5060 Ti ahead of the 3070 Ti. 
AMD’s 7800 XT outdoes the 5060 Ti by 18%, with the more expensive 9070 non-XT up at 63 FPS AVG.
The RTX 5070 ran at 54 FPS AVG here, 32% ahead of the 5060 Ti. We’ll move to 1440p for prior generations.
At 1440p, the 5060 Ti landed at 65 FPS AVG. Unfortunately for NVIDIA, this is a terrible result: The 4060 Ti was at 58 FPS AVG, narrowing the uplift to only 13.4%. That isn’t a big improvement. The lead over the 3060 Ti’s 50 FPS AVG is 30%, also not that impressive for two generations.
AMD’s 7800 XT leads the 5060 Ti by 15.5%, with the 9070 (read our review) obviously way ahead given its higher theoretical price and positioning.
For those considering used options, the 5060 Ti only outdoes the 3070 Ti by about 8%, making it a reasonable alternative that might save some money.
At 1080p, the 5060 Ti held an 82 FPS AVG with lows positioned about the same as everything around it. The 3070 Ti’s 76 FPS AVG encroaches on the 5060 Ti’s result and, from an actual human perspective, would look about the same. The 4070 outperforms the 5060 Ti by 16%, with the 5070 ahead by 26%. 
The 4060 Ti’s 74 FPS AVG means the 5060 Ti is about 11% better, overall a boring generational jaunt. The uplift over the 3060 Ti is 28%.
The 5060 Ti ran at 40 FPS AVG, so it’s nearly exactly tied with the 3070 Ti. The 0.2 FPS AVG advantage is well within run-to-run variance. Lows are also tied. AMD’s 7800 XT is 17% ahead of the 5060 Ti’s average FPS, with the 5070 ahead by 41%.
We’ll move to lower resolutions to look at the prior generation 60 and 60 Ti-class cards.
At 1440p, the 5060 Ti ran at 70 FPS AVG, planting it right in the middle of the 3080 and 7700 XT. The 3070 Ti is right behind with a 65 FPS AVG, with the 4060 Ti at about the level of the 3070. The new 5060 Ti leads the 4060 Ti by 22% and the 3060 Ti with its 53 FPS AVG by 32%.
AMD’s 7600 is far down this chart, so it’ll need something newer in the 9060 class to compete here. Intel’s B580 is also down near the RTX 4060 and RX 6600 XT (watch our review).
As for what’s better than the 5060 Ti: Other than the 3080, the 4070 is about 12% better and 5070 is about 37% higher average FPS.
At 1080p, the 5060 Ti ran at 93 FPS AVG, landing between the 3080 and 7700 XT again. The improvement over the 4060 Ti is just 19%, followed by the 3060 Ti’s 68 FPS AVG for an uplift of 36%. The 3070 Ti gives the 5060 Ti just a 10% lead, doing better than the 4060 Ti.
As for the GTX 1060, considering 50x its performance would put it over 1,100 FPS, we’d say NVIDIA missed the mark on this by orders of magnitude.
Cyberpunk 2077: Phantom Liberty is next. This is newer data, so we haven’t re-run the 4060 Ti, 4060, and 3060 series cards through here yet. We’ll show it anyway for the other comparisons.
At 4K/Ultra without RT first, the RTX 5060 Ti ran at 31 FPS AVG. This has it meaningfully ahead of the 3070 Ti by percentage, improved by 19%. The 7800 XT leads the 5060 Ti by about 10% here, with the 5070 about 30% ahead of the 5060 Ti.
At 1440p, the 5060 Ti ran at 68 FPS AVG, putting the 5060 Ti between the 3080 and 3070 Ti. The 5070 ends up about 30% ahead with its 88 FPS AVG, meaning that, if we just pretend that the MSRP numbers stick, you’re paying about 1% more money for every 1% more performance between the 16GB 5060 Ti and 5070. 
At 1080p, the 5070 is down to a 27% lead over the 5060 Ti. The 5060 Ti now leads the 3070 Ti by 20% and the 3070 non-Ti by 27%. The 7800 XT is about 8% ahead of the 5060 Ti here.
Let’s move to something where we have last-gen numbers.
Dying Light 2 at 1440p is one of the situations that was bad for the 4060 Ti versus the 3060 Ti: The two cards are indistinguishable, with performance identical between them. The RTX 5060 Ti ran at 74 FPS AVG, so it outperforms the 4060 Ti (and therefore 3060 Ti) by about 23%. It took them two generations, but they’ve finally beaten the 3060 Ti in this game. The 4070 is about 6% better than the 5060 Ti here, with the 7800 XT about 14% ahead. The 5070 leads the 5060 Ti by 44% here, so if anything, the 5060 Ti stands to make the 5070 look better. Against the Intel B580’s 63 FPS AVG, NVIDIA’s 5060 Ti is about 17% better in one of the better B580 showings. 
1080p has the 3060 Ti and 4060 Ti again roughly adjacent to one another, with the 5060 Ti leading the 4060 Ti by 19%. The 5060 Ti’s lead has diminished from the 1440p result. The 5060 Ti is similar to the 7700 XT’s performance here, including in lows, with the 4070 leading the 5060 Ti by almost 9%.
Against older cards, the 5060 Ti improves on the GTX 1060 by not 50x, to nobody’s surprise, and instead by about 3.8x. Even with MFG, you would not get 50x. Maybe with DLSS at Ultra Sh*t quality and upscaling from 144p, we’re not certain you could squeeze 50x out of this lemon, though. You’d have to go out of your way to hurt the 1060.
For those still on an RTX 2060, you can expect about a doubling of performance to the 5060 Ti in a scenario like this. 
Resident Evil 4 is up now, first at 4K and without ray tracing. The RTX 5060 Ti ends up performing about the same as the 7700 XT. The RTX 4070 leads the 5060 Ti by about 10-11% here, at 63 FPS AVG to 57, with the 5070 leading by 38%. That’s similar to what we’ve seen elsewhere so far. The 3060 Ti also launched for $400. Adjusted for inflation, that amounts to $491. The new card is $430. So things haven’t changed that much. The price is similar/slightly lower and performance has hardly improved. The improvement over the 2060 is about 135%. 
At 1440p, the 5060 Ti leads the 3070 Ti by 13% and the 4060 Ti by 23%, followed by the 3060 Ti at 36.5%. The reduced resolution has benefitted the 4060 Ti marginally, allowing it to distance itself from the 3060 Ti. The B580 (read our review) is actually around the 3060 Ti’s performance, excepting 1% lows.
As for the RTX 5070, which remains not a 4090 (watch our review), NVIDIA’s biggest lie leads the 5060 Ti by 36%. 
AMD still doesn’t have a new and direct competitor here, but probably will in May in the 9060 series. For now, the 7700 XT is the closest and outperforms the 5060 Ti slightly.
Ray tracing is up next. These games are generally a heavier load, so we have a mix of upscaled benchmarks and of native resolution benchmarks, but all of them are with RT on now.
In Black Myth: Wukong at 4K upscaled, the RTX 5060 Ti landed at 29 FPS AVG. That has it ahead of the RX 9070, which is more of a problem for AMD than it is a positive for NVIDIA. We already knew this about this game and AMD, though. The 9070 XT ends up about tied with the 5060 Ti here, and actually, the 3090 non-Ti (watch our review) at 29.8 FPS AVG is only about 3-4% ahead of the 5060 Ti. This game remains heavily favored for NVIDIA, especially with ray tracing. 
The RTX 5070 has a large lead at 40 FPS AVG, although its memory capacity can prove problematic in some of these heavier scenarios. The 5070 leads by 39% here. The 5060 Ti also shows more meaningful gains over the 3070 Ti in this test than in some of the raster tests, at 31% improved.
At 1080p upscaled, the 5060 Ti's 74 FPS AVG has it ahead of the 9070 XT, 3090, and 3080. It also leads the 4060 Ti by 19% and the 3060 Ti's 48 FPS AVG by 57%.
The generational RT uplift is helping the 5060 Ti distinguish itself more here than it did when rasterized.
Against the first-gen ray tracing 60-class card, the RTX 2060, we're seeing a 175% improvement.
In Dragon's Dogma 2 at 4K and with ray tracing, the RTX 5060 Ti ran at 35 FPS AVG and roughly tied (but technically led) the 3070 Ti. That has it better than the 2080 and 2080 Ti of years past, although the 3080 still manages to best the 5060 Ti. AMD's 7900 GRE outperforms the 5060 Ti slightly, with its newer 9070 cards performing up around 3090 Ti levels -- but at a higher theoretical base MSRP than the 5060 Ti.
At 1440p with RT, the 5060 Ti ran at 62 FPS AVG and kept the lows consistent with the average. There's nothing particularly impressive or bad for the frametime consistency and lows. It’s just kind of where we’d expect it. 
The 4070 and 7800 XT are about 9% ahead of the 5060 Ti. And for that matter, the 3080 is around that same area. The 5070's 82.8 FPS AVG is around 34% ahead of the 62 FPS for the 5060 Ti. We've seen higher in other games.
As for the lower-rank cards, the 4060 Ti ran at 49 FPS AVG and yields a 26% uplift to the 5060 Ti. The 3060 Ti isn't far behind the 4060 Ti in this one, but at least they're in the order you'd expect them to be. 
At 1080p, the 5060 Ti's 82 FPS AVG landed it between the 3080 and 7700 XT again. This has been consistent. This result gives it a 20% improvement over the 4060 Ti and 37% improvement over the 3060 Ti.
Dying Light 2 is up next. Dying Light 2 at 4K upscaled with ray tracing has the 5060 Ti at 31.8 FPS AVG and exactly tied with the 7900 GRE for average and 1% lows. The 5070 improves to 43.7 FPS AVG, or 37% once again. This seems to be a fairly consistent percentage improvement to the 5070.
At 1440p upscaled with RT, the 5060 Ti ran at 60 FPS AVG. We haven't yet re-run the 4060 Ti or 3060 Ti in this one, leaving us to compare instead with the 7900 GRE -- where they're about equal -- and the 3080, which remains a bit ahead of the 5060 Ti. The 5070 leads at 81 FPS AVG, or 34%.
At 1080p, we re-introduce the 4060 Ti and 3060 Ti. The 5060 Ti's 87 FPS AVG has it about 20% ahead of the 4060 Ti, which itself was only 11% ahead of the 3060 Ti. The B580 is actually somewhat close here, roughly tying the 3060 Ti. As for AMD, the 7900 GRE remains the next closest to the 5060 Ti.
Resident Evil 4 is up with ray tracing now, first with 4K and upscaled. The 5060 Ti's 67 FPS AVG has it tied with the 7700 XT, including in the 1% and 0.1% lows. The 4070 and 3080 lead these results, as they have for the past games. The 5070 leads the 5060 Ti by 36%, close to prior results. As for the 4060 Ti, its 52 FPS AVG gives the 5060 Ti a lead of 30% for one of the larger gaps.
At 1440p, the 5060 Ti again falls between the 7700 XT and RTX 3080. The lead over the 4060 Ti is narrowed to 24% now, with the uplift over the 3060 Ti at 39%.
In Cyberpunk 2077: Phantom Liberty at 1080p RT Medium, the 5060 Ti ran at 63 FPS AVG and sat between the 7900 GRE and 7900 XT. The lead over the 3070 Ti is about 10 FPS AVG here, or 17%. The 5070's 82 FPS AVG has it 31% ahead of the 5060 Ti, down in relative improvement from other benchmarks.
At 1080p and with RT Ultra, the 5060 Ti ran at 50 FPS AVG (which is really not bad when considering how heavy this workload is), or just ahead of the 7900 XT Hellhound. The 3070 Ti hit 40 FPS AVG here, with lows suffering for the 0.1% value. The 5060 Ti's lows are OK in this one, supported by the 16GB capacity.
We’ve provided the benchmark numbers above. At the very least, you have the data you need to figure out if an upgrade makes sense for you. This is going to be one where we withhold full value judgment until it properly launches because we do not trust the MSRP to persist for the majority of purchasers.
It certainly isn’t going to 50x a GTX 1060, though.
The 16GB RTX 5060 Ti’s MSRP is set at $430, whether or not we see it at that. That’s better than the previous 16GB 4060 Ti’s launch MSRP of $500, which was atrocious. The 4060 Ti 8GB model was $400, so 8GB more memory used to be a $100 upsell, but no one bought that, so now it’s a $50 upcharge. 
Accounting for inflation favors the 5060 Ti over the 4060 Ti for the 16GB models with MSRPs. 
Nearest performance neighbors to the 16GB RTX 5060 Ti are typically the RTX 3080 and RX 7800 XT above, and RX 7700 XT and RTX 3070 Ti commonly below.
We’d love to dig into value comparisons between the 5060 Ti and its current competitors, but the availability of GPUs at retail in this price bracket ($380-$480) is terrible. 
Using Newegg as a representation shows only 9 SKUs of any video card sold by Newegg in stock in that price range, and only one of them is actually brand new. It’s an EVGA RTX 3060 XC (not a Ti) for $440, which is an awful price.
So without any new cards to buy in this price bracket, we can look toward used options -- and this is where we think people should be seriously looking.
The RX 7900 GRE was around $627 average for sold listings, followed by the RX 6950 XT and 7800 XT at $556-$575 on average and the RX 7700 XT at $472 average.
There’s a potential edge case where a good deal on an RX 6950 XT could be an interesting higher-performing wild card – but that’s only if you’re entirely focused on raster performance, and you’re willing to hunt for a deal on one around $500. Higher power consumption is also something to consider. NVIDIA’s 5060 Ti is more efficient.
A used RTX 3070 Ti at $358 average would be a good lower-performing budget option below the MSRP of a new RTX 5060 Ti of any capacity. We found some that were in the upper $280-$290 to lower $310 range, which would be worth seriously considering. VRAM may be limiting in some situations. The RTX 3080 is going for around $449 and typically beats or is close to the 5060 Ti. The 3080 isn’t cheap enough on the second-hand market yet to get our strong recommendation in this specific scenario. That’s doubly the case for used RTX 4070s at the time of writing, which are newer and have been selling for $621 on average.
And that brings us to what we’ve said a lot in the past: if your computer is doing what you need it to do and if you don’t feel a need to upgrade, then we’d say hold off. But some people do either “need” to buy new devices to replace aging hardware or just really want the escape of building a PC, which we also appreciate and relate to. It’s just going to come down to your price tolerance.
Right now, we really don’t have the answers. 
We’re trying to figure them out, which is why we’re currently flying all around the US at our own expense to talk to companies about the pricing situation. We hate to not be able to give a value judgment at the end of a review, but until this card is actually available -- which will coincide with the launch of this review -- we just can’t know what it’s going to cost.
Once it is, we’re going to do a recap either in the news or standalone.
Just for context: We just met with a distributor that buys hundreds of thousands of GPUs per year. Last week, we saw their cost to buy RTX 5090s. The cheapest was around $2,400 up to $3,000, and that’s their cost. That means $2,000 is impossible. We’re not sure to what extent that’ll affect the 5060 Ti cards, especially after the launch period where pressure to maintain the price is off.
Now maybe that’ll change with the partial tariff exemptions, but those are up in the air.
 Copyright © 2023 GamersNexus, LLC. All rights reserved.GamersNexus.net is Owned, Operated, & Maintained by GamersNexus, LLC. 
Copyright © 2023 GamersNexus, LLC. All rights reserved.GamersNexus.net is Owned, Operated, & Maintained by GamersNexus, LLC."
"NVIDIA","RTX 5060 Ti","Nvidia GeForce RTX 5060 Ti Review","https://www.techspot.com/review/2992-nvidia-geforce-rtx-5060/","The Nvidia RTX 5060 is based on the same GB206 silicon as the more expensive 5060 Ti series. This means the 181 mm² die contains 21.9 billion transistors – though not all are active in this model, as the core count has been reduced by 17%.
Still, the RTX 5060 features 25% more cores than the previous-gen 4060. Not only that, but thanks to the use of 28 Gbps GDDR7 memory, bandwidth has increased by 65% to 448 GB/s.
On paper, this is a $299 graphics card (MSRP) but we'll have to see what's the retail pricing looks like post-launch. This model is also limited to just 8 GB of VRAM, which is bad, however on the upside  there is only one configuration available, making it far less of a trap for gamers compared to the more expensive 8GB RTX 5060 Ti.
Making the VRAM limitation even more problematic is the use of a PCIe 5.0 x8 bus interface. While not ideal even for PCIe 5.0-enabled systems, it becomes a major issue for those on older hardware, especially when restricted to PCIe 3.0. We can explore this further on a later extended review.
Believe it or not, we set up an AM5 test system in our hotel room while attending Computex 2025 in Taiwan. We're using a Ryzen 7 9800X3D processor we brought along, paired with a G.Skill Trident Z5 DDR5-6000 CL30 memory kit and our usual test SSD. A big thank you to MSI for providing the additional components needed to make this review possible.
It's been a challenging but worthwhile process to put this together – so let's dive into the data…
First up is Clair Obscur: Expedition 33, where the RTX 5060 averaged just 48 fps at 1080p. That's the same level of performance seen with the much older RTX 3060 Ti, making it only 17% faster than the RTX 4060.
Increasing the resolution using the Epic preset isn't really viable. The RTX 5060 simply isn't powerful enough and, with just 8 GB of VRAM, quickly runs out of memory. For example, the 5060 Ti 16 GB model is usable here, while the 8 GB version struggles significantly.
In Oblivion Remastered, the RTX 5060 delivered an average of 45 fps at 1080p, which is roughly on par with the RTX 4060 Ti and RTX 3070 – about 25% faster than the RTX 4060.
At 1440p, VRAM limitations show up again. While 34 fps is technically playable, 1% lows drop to just 18 fps, resulting in a very choppy experience.
Performance in Delta Force looks much better, with the RTX 5060 hitting 138 fps at 1080p. However, this is still in line with the RTX 3060 Ti and actually a bit slower than the RTX 4060 Ti.
At 1440p, the RTX 5060 again lands between the 3060 Ti and 3070, though this time its performance is closer to the 3070.
In Stalker 2, the RTX 5060 averaged 47 fps at 1080p – once again similar to the RTX 3060 Ti, and just a 9% improvement over the RTX 4060.
At 1440p, the 8 GB VRAM buffer becomes a major bottleneck, dropping average frame rates to just 6 fps. In this state, the game is essentially unplayable.
The RTX 5060 excels as an esports GPU, delivering strong performance in Counter-Strike 2 at 1080p using the medium preset. Impressively, it offers a 27% increase over the RTX 4060 and matches the performance of the older RTX 3070.
At 1440p, performance remained excellent, averaging 370 fps – slightly ahead of the Radeon RX 7700 XT.
Space Marine 2 ran very well at 1080p, with the RTX 5060 averaging 100 fps, providing a smooth experience and a 23% improvement over the RTX 4060.
At 1440p, performance improved by a massive 33% over the 4060, coming very close to RX 7700 XT levels.
Star Wars Jedi: Survivor also ran smoothly, with 95 fps on average at 1080p. This level of performance puts the RTX 5060 in the same range as the RTX 4060 Ti and RTX 3070.
At 1440p, the trend continues: 57 fps on average matches 4060 Ti and 3070 levels and represents a 30% uplift over the RTX 4060 – an impressive result.
Call of Duty: Black Ops 6 saw the RTX 5060 render 75 fps at 1080p, which was only a 10% uplift over the 4060 and 7% slower than the RTX 3070 – not a great result overall.
It's a similar story at 1440p, where performance is essentially on par with the RTX 3060 Ti. That's a disappointing outcome given how much time has passed since that GPU's release.
In A Plague Tale: Requiem at 1080p, the RTX 5060 delivered just a 5% improvement over the RTX 3070, making it slightly slower than the 7700 XT. However, it was a significant 36% faster than the RTX 4060.
At 1440p, it continued to outperform the 4060 with a 33% advantage, although it only managed to match the performance of the RTX 3070 and 4060 Ti.
Starfield performance was even weaker. At 1080p, the RTX 5060 matched the RTX 3060 Ti with just 55 fps on average.
At 1440p, the margin remained narrow, with the RTX 5060 averaging 44 fps – just 5% faster than the 3060 Ti.
Performance in Cyberpunk 2077 at 1080p was solid. The RTX 5060 averaged 100 fps, putting it on par with the 4060 Ti and 28% ahead of the RTX 4060.
At 1440p, it maintained strong performance with 66 fps on average, again delivering results similar to the 4060 Ti and RTX 3070 – 35% faster than the 4060 in this case.
The RTX 5060 delivered surprisingly strong results in God of War Ragnarök at 1080p, averaging 128 fps – an impressive 45% improvement over the RTX 4060.
That margin was reduced at 1440p, but the 5060 still came in 32% faster than the 4060, again delivering performance comparable to the RTX 3070 and 4060 Ti.
In Dying Light 2, the RTX 5060 effectively matched the RTX 3070 and 4060 Ti at 1080p.
At 1440p, the story remained consistent – though here, the Arc B580 also entered the performance mix.
Interestingly, Dragon Age: The Veilguard proved more difficult. At 1080p, the RTX 5060 rendered just 68 fps, making it only 11% faster than the RTX 4060 and notably slower than both the RTX 3070 and 4060 Ti.
At 1440p, the performance gap narrowed, with the 5060 aligning more closely with the RTX 3070 and 4060 Ti, though average frame rates dropped to 49 fps – not exactly impressive.
Spider-Man Remastered posed no challenge for the RTX 5060 at 1080p. It averaged 159 fps, narrowly edging out the 4060 Ti and RTX 3070, and delivering a 25% uplift over the RTX 4060.
At 1440p, performance remained strong with 110 fps on average. Relative performance was typical, closely matching the Arc B580, 4060 Ti, and RTX 3070.
Performance in Hogwarts Legacy at 1080p was also impressive. The RTX 5060 averaged 101 fps – slightly faster than the RTX 3070, significantly ahead of the 4060 Ti, and 44% faster than the RTX 4060. This game is very memory-intensive, so the high-speed GDDR7 memory is well utilized here.
At 1440p, a different bottleneck appears. The RTX 5060 only matched the 4060 Ti, resulting in a 13% performance uplift over the 4060.
In The Last of Us Part I, the RTX 5060 delivered 85 fps at 1080p, putting it on par with the 4060 Ti and 25% faster than the RTX 4060.
However, at 1440p, performance fell apart. With just 8 GB of VRAM, the 5060 couldn't maintain consistent frame times using ultra-quality settings.
Finally, in Star Wars Outlaws, the RTX 5060 struggled. At 1080p, it rendered only 42 fps – a mere 8% improvement over the RTX 4060.
At 1440p, frame rates dropped further to 31 fps, making it 19% faster than the 4060 but still delivering very weak performance overall.
Across the 18 games tested, the RTX 5060 matched the performance of the RTX 4060 Ti and RTX 3070, while coming in 6% slower than the 7700 XT. It was also 22% faster than the RTX 4060, which aligns closely with Nvidia's official claims.
At 1440p, we saw several examples where 8 GB GPUs began to fall apart. In some cases, performance appeared acceptable, but the visual quality suffered due to missing textures that couldn't fit into local video memory.
Overall, the RTX 5060 remained on par with the RTX 3070 and 4060 Ti, though it was just 6% faster than Intel's Arc B580 and 27% faster than the RTX 4060.
As expected, achieving a high-end ray tracing experience is difficult – if not impossible – with the RTX 5060. The GPU simply doesn't have enough power, and its 8 GB of VRAM is insufficient for ray tracing in modern titles.
For example, in Alan Wake II at 1080p with DLSS Quality enabled, the RTX 5060 averaged just 36 fps. That made it 20% faster than the RTX 4060, but 14% slower than the 4060 Ti.
Those hoping to enable ray tracing at 1440p will be disappointed – it's simply not viable on this GPU.
Thanks to DLSS, it's possible to approach 60 fps at 1080p in Cyberpunk 2077 using the Ultra RT preset. However, since this relies on upscaling, it's not true 1080p rendering. Performance is comparable to the Arc B580 – not a particularly strong result.
At 1440p, ray tracing is off the table. We also encountered VRAM limitations during our brief testing.
Marvel's Spider-Man Remastered is a well-optimized title, and even with ray tracing maxed out, the RTX 5060 managed an impressive 112 fps on average at 1080p – similar to the 4060 Ti.
At 1440p, the RTX 5060 performed even better, averaging 109 fps and pulling well ahead of the 4060 Ti. That's a solid result, nearly matching the RTX 4070. Still, you'd typically expect a product labeled ""5060"" to at least match the previous-generation GPU positioned one tier higher.
At upscaled 1080p with the high ray tracing preset, Dying Light 2 ran at 72 fps on the RTX 5060 – matching the 7700 XT and 4060 Ti. This is usable performance, though not outstanding, despite being almost 30% faster than the RTX 4060.
At 1440p, the RTX 5060 struggled more, delivering just 48 fps on average. This result was still similar to the 7700 XT and 4060 Ti.
We know MSRP isn't always reflective of reality, especially at launch, but it's still useful for establishing a baseline. If all GPUs were sold at their suggested prices, the Arc B580 would offer the best value, followed by the RX 9070, and then the RTX 5060.
At MSRP, the 5060 comes in at a cost per frame of $5.35 – a 21% improvement over the RTX 4060 and a 30% improvement over the RTX 3060. That sounds solid, or at least it would be if the card had more VRAM. We'll come back to that shortly. For now, let's take a look at real retail pricing.
At the time of writing this review, the RTX 5060 was available and in stock on Newegg for $330 – about 10% over MSRP. In the current market, that makes it relatively decent value, assuming you ignore the elephant in the room: the 8 GB VRAM buffer.
Even when factoring in the poor results seen in some VRAM-limited games, particularly in titles where missing textures or inconsistent frame times become an issue, the RTX 5060 still manages to be 8% better value than the 7800 XT.
That's not a strong showing considering the Radeon GPU comes with 16 GB of VRAM. Compared to the outgoing RX 7600, it's 14% better value, and 21% better value than remaining RTX 4060 stock.
So, given today's market conditions, the RTX 5060 offers decent value for buyers looking to purchase a brand-new GPU. But with just 8 GB of VRAM, it's not a product we can recommend at that price.
So there you have it. On the surface, the RTX 5060 appears to stack up fairly well. If you don't look too closely, you might even call it good value. But deeper analysis reveals a troubled product that will almost certainly age incredibly poorly.
As we've clearly established by now, 8 GB of VRAM is simply not enough – and in 2025, it should not exist on any GPU priced above $200. As an esports card, it holds up reasonably well, though if that's your target use case, we'd suggest exploring the second-hand market instead.
It's frustrating how good the RTX 5060 could have been. Even with just 12 GB of VRAM, we might have been able to tentatively recommend it at its current price. With 16 GB, it could have been a genuinely solid product.
As it stands, the RTX 5060 is effectively a discounted RTX 4060 Ti – offering about 25% savings. That might sound appealing, but nearly two years after the 4060 Ti's release, it's hardly exciting. Looking further back, the 5060 essentially offers RTX 3070-like performance at a 40% discount – but nearly five years have passed since Ampere launched.
In our opinion, Nvidia had a clear opportunity to deliver a meaningful upgrade here. Instead, they've recycled the same class of GPU for five years, offering incremental discounts with each release.
The real challenge for Nvidia will be the incoming Radeon RX 9060 XT series. If AMD's numbers prove accurate, the RTX 5060 won't be worth considering – it's dead on arrival, at least for buyers who follow real, independent reviews. On that note, Nvidia has handled this launch very poorly. It's been a PR disaster. Ironically, Nvidia's marketing may be more effective than AMD's at convincing GeForce owners to switch to Radeon. In fact, this might be the only way that shift was ever going to happen.
That's going to wrap up our review of the RTX 5060. We debated calling it a preview rather than a full review – but claiming an ""RTX 5060 preview"" feels like a bit of a self-own at this point. So let's call it a quick review.
We'll cover more details, including ray tracing performance, power consumption, and overclocking, once we're back from Computex. Needless to say, Steve has outdone himself to deliver a comprehensive and honest look at this GPU under the circumstances. If you've found it helpful, we appreciate your support.

About
Ethics Statement
Terms of Use
Privacy Policy
Manage Ad Consent

TECHSPOT : Tech Enthusiasts, Power Users, Gamers
TechSpot is a registered trademark and may not be used by third parties without express written permission.
TechSpot is represented by Future PLC. Learn about advertising opportunities.
© 1998 - 2025 TechSpot, Inc. All Rights Reserved."
"NVIDIA","RTX 5060 Ti","Nvidia GeForce RTX 5060 Ti 16GB: Great at MSRP, but retail prices could be a problem","https://www.tomshardware.com/pc-components/gpus/nvidia-geforce-rtx-5060-ti-16gb-review/10","When you purchase through links on our site, we may earn an affiliate commission. Here’s how it works.


Why you can trust Tom's Hardware




Our expert reviewers spend hours testing and comparing products and services so you can choose the best for you. Find out more about how we test.

Nvidia's RTX 5060 Ti 16GB delivers a solid combination of performance for the suggested $429 base MSRP. However, as we've seen with every other GPU launch of the past five months, retail prices can be much higher. It's impossible to separate performance from pricing when looking at the overall value of a GPU, and the only thing concrete that we can point to are the MSRPs. Except those can run the gamut from being at least moderately accurate to being completely nonsense.

Where will the RTX 5060 Ti 16GB actually land? We suspect it will trend closer to $500 in the near term, but supply will have a lot to say about that. We've seen the RTX 5070 sell for $550–$600 over the past few weeks, and it's substantially faster than the 5060 Ti 16GB — about 30% faster overall. If we put $600 down as the nominal price for the 5070, then the 5060 Ti 16GB needs to stay well below $500 to offer a similar value. But both GPUs could end up at much higher prices, so you'll just need to see what's available when you're ready to buy.

When the 4060 Ti 16GB came out a month after the 8GB variant, it felt severely underwhelming. Neither version was really designed to handle 4K gaming, but that was the only place where we measured a significant difference in performance. Two years later, things haven't changed too much, but the reduced $50 price gap (on paper at least) between the 5060 Ti 8GB and 16GB makes the 16GB a far easier recommendation. In fact, we'll go so far as to question why Nvidia even felt the need to create an 8GB version.

Yes, 8GB will be cheaper, and it will also be more limited due to the lack of VRAM. There are games (Indiana Jones and the Great Circle) where you can't even try to run ultra settings on an 8GB card. That's an Nvidia promoted game that simply crashes to desktop with a video memory error when you try higher settings on the 4060 and 4060 Ti 8GB GPUs, along with a bunch of other previous generation RTX cards.
The real market for the 5060 Ti 8GB likely isn't the DIY or gamer crowds. Instead, it will probably find its way into prebuilt PCs, where the big OEMs want the highest model number possible at the lowest price. Shaving $25 off the bill of materials for a $1,200 desktop by using an 8GB 5060 Ti instead of the 16GB model? We'll probably see a lot of that. We'll be looking to acquire and test an RTX 5060 Ti 8GB as soon as possible to show exactly how that will affect gaming in the near future.

The good news with the 16GB card is that memory bandwidth has improved thanks to GDDR7, so that it's not likely to hit VRAM capacity or bandwidth limitations. 56% more bandwidth than the 4060 Ti is a sizeable improvement. The fact that most games only show about 15% higher performance indicates that GPU compute is the limiting factor more than bandwidth, however.

We also wish Nvidia has simply opted for an in-between solution. It's using 24Gb (3GB) GDDR7 chips on the mobile RTX 5090, as well as the RTX Pro 6000 Blackwell server, workstation, and data center variants. Obviously those chips exist and appear to work just fine. How much would it have cost to use four 3GB chips on one side of the PCB, rather than putting eight 2GB chips on both sides of the PCB in clamshell mode? We can't help but think the 16GB configuration costs more, so perhaps there just isn't enough 3GB chip supply right now to cover all the products that would benefit.

And so we end up with the weird bifurcation where you have either too little VRAM in the case of the 8GB model, or more VRAM that the GPU really needs in the case of the 16GB card. But we'd rather have too much than not enough. If you're interested in buying the RTX 5060 Ti, we would strongly advise potential customers to get the 16GB model this round. There are simply too many 'edge' cases where 8GB isn't enough, and they're becoming increasingly common. $50 extra is money well spent.
But as we've already said numerous times, the price difference could very easily end up being more than $50. And factors like on again/off again tariffs, limited supply, product demand, and more could push the 16GB card to the point where maybe it won't be the better choice. The RTX 5070 still serves as a ceiling on how much more the 5060 Ti 16GB can realistically cost before it's ""too much,"" but with 5070 cards often listed for $700 or more, there's a lot of wiggle room right now.

Price and availability will be the key determiners of how good the 5060 Ti 16GB looks, and that will also vary by market. Europe and Asia might end up with a much different GPU landscape than the U.S. as far as graphics card values go.

What we can say is that the 5060 Ti 16GB isn't a massive generational improvement, but it is an improvement. It's also supposed to be less expensive than its 4060 Ti 16GB predecessor. Those are both good things, and stuff like neural rendering, DLSS 4, and Multi Frame Generation are merely extras that you can use as you see fit. Now we just wait to see what today's launch looks like, how quickly the 5060 Ti models sell out, and how high prices go.

Our score of 4-stars represents a ""best guess"" on what the 5060 Ti 16GB will look like in the current GPU market. Obviously, prices for all graphics cards, new and used, are all over the map. If the RTX 5060 Ti 16GB costs 50% more than the MSRP, and other cards don't show a similar markup, that makes it a worse value and a less desirable card and it would deserve a lower score. We can't predict where things will go, so pay more attention to the performance and real-world pricing than the single score that we've assigned, because uncontrollable factors play into the overall package.

Current page:

Nvidia GeForce RTX 5060 Ti 16GB: Great at MSRP, but retail prices could be much higher


Jarred Walton is a senior editor at Tom's Hardware focusing on everything GPU. He has been working as a tech journalist since 2004, writing for AnandTech, Maximum PC, and PC Gamer. From the first S3 Virge '3D decelerators' to today's GPUs, Jarred keeps up with all the latest graphics trends and is the one to ask about game performance.

Tom's Hardware is part of Future US Inc, an international media group and leading digital publisher. Visit our corporate site.

©
Future US, Inc. Full 7th Floor, 130 West 42nd Street,
New York,
NY 10036."
"NVIDIA","RTX 5070","Nvidia GeForce RTX 5070 review: an RTX 4070 Super with a DLSS 4 badge","https://www.rockpapershotgun.com/nvidia-geforce-rtx-5070-review","The more genned frames, the more things stay the same
I knew the RTX 5070 was tricking me. Parked next to the extravagant silliness of the two-grand RTX 5090, this £539 / $549 graphics card looked like a very agreeable deal, offering all the same DLSS 4 and Multi Frame Generation as its bigger, pricier brothers. Also, an upgrade to the RTX 4070 Super, a GPU that could handle 4K without looking too out of place in a premium 1080p rig. Tragically, though, the RTX 5070 breaks a sacred covenant, a mutual understanding between PC owners and parts makers that’s held strong for decades: if you buy a new version of a thing, it should be faster than the old version of that thing. Look past the MFG illusion, and far too often, it isn’t.
Rifling through the specs doesn’t turn up any clear reasons for this lack of performance gain; the RTX 5070’s CUDA core count comes in at 6144, putting it merely in-between the RTX 4070 (5888) and the RTX 4070 Super (7168), but both base and boost clock speeds have been raised to compensate. VRAM, meanwhile, remains at 12GB, but with a switch from GDDR6X to GDDR7 that confers significantly more bandwidth. Then there’s the power rating, which both the delayed Founders Edition and this here triple-fan PNY GeForce RTX 5070 OC set at 250W – 50W more than an entry-level RTX 4070 Super. It looks, and sounds, like a decent generational update.
While the RTX 5070 does make the cut as a 4K contender, surpassing 60fps in most Ultra-quality games without upscaling, the only time it produced a visibly faster framerate was in Cyberpunk 2077. Elsewhere, it only outperforms the RTX 4070 Super by a scant handful of single-digit frames per second, and repeatedly loses out to the non-Super RTX 4070 Ti in the process.
Obviously, very few potential buyers are going to be upgrading directly from last year’s RTX 4070 Super to the RTX 5070, and the latest model does at least make for a meaty improvement on the RTX 3070. Trouble is, that improvement has already been made in the last generation, and for prices that – even if you’re lucky enough to find an RTX 5070 in stock below £600 – are likely drop much faster, especially where the secondhand market is concerned.
It’s also worth addressing a specific claim, made by Nvidia during their original RTX 50 series announcement, that the RTX 5070 delivers ""RTX 4090 performance"" with MFG. This is, depending on how generous you want to be, either lacking in context or complete tosh. While it’s true that, for instance, 4x MFG can drag the RTX 5070 to 71fps in a fully path-traced Alan Wake 2, that’s only equal or faster than the RTX 4090 if you deny the older GPU its own tools. Sure enough, it only needs DLSS 3’s old-timey, 2x frame generation to average 82fps on otherwise identical settings. In short, the only way the RTX 5070 is truly moving at the same pace as the RTX 4090 is if they’re in the same delivery van.
You could still consider MFG to be the RTX 5070’s party trick, though at 4K specifically, it’s not as much of a difference-maker as it is on on burlier cards like the RTX 5070 Ti and RTX 5080. In both Cyberpunk and Alan Wake 2, the RTX 4070 joins its 40-series rivals in being unable to churn out playable path tracing performance on DLSS Quality; in fact, in the latter, it’s narrowly the slowest of the lot. As a result, when 4x MFG kicks in, it’s producing a higher number in a framerate counter but isn’t actually helping the games to run and smoother or more responsively than on the DLSS 3-limited 40 series models. As far as your PC knows, it’s still only running at 26/21fps, and all the AI-generated frames in the world won’t get rid of the sluggish feel that aiming and camera control will suffer in those conditions.
1440p is a more comfortable environment for the RTX 5070’s frame gen tech, though its problems with overfamiliar performance remain. If anything, it’s more bothersome here, as the higher framerates make it even harder to tell a naked-eye difference between this and its predecessors.
In Assassin’s Creed Mirage, the RTX 4070 Super even finishes ahead, if only be a single frame, while even the games that the RTX 5070 does relatively well in (Cyberpunk 2077, F1 24) only see an uptick of 10% or so. There is a simultaneous gap-narrowing with the RTX 5070 Ti, suggesting you don’t need to stretch to the Ti version for quality Quad HD, and again, anyone upgrading from the RTX 3070 will enjoy a sizeable speed boost. Still, these benefits could apply to the RTX 4070 Super just as much as the RTX 5070.
The same can’t be said, in fairness, for frame gen performance. Although these tests suggest the RTX 5070 is in fact slightly worse at dealing with path tracing/full ray tracing than those pesky RTX 40 GPUs, unlike at 4K, it is able to produce enough pre-generation frames to support a workable application of 4x MFG.
I’m not saying this is an adequate replacement for traditionally rendered frames, but at least at this resolution, you can kind of see where Nvidia is coming from. Spared from the need to compensate for sub-30fps framerates, frame gen is free to simply shine up the visual smoothness of already adequately-running games, as it should be.
Still, it’s also hard not to peer over at those towering RTX 5070 Ti and RTX 5080 bars, wondering why the standard RTX 5070 couldn’t just do a little more in the conventional rendering department.
The drop to an even less demanding screen rez does little to raise the RTX 5070’s value proposition. Yet again it’s haunted by the Ghosts of XX70 GPUs Past, especially in Assassin’s Creed Mirage, where it repeats its embarrassing loss. It does gain a few extra frames over the RTX 4070 Super in Shadow of the Tomb Raider, but then that’s already running so fast that you’d need a 300Hz monitor and downright inhuman observation skills to perceive them in action.
That all means we must return to the world of maxed-out lighting effects and AI frame generation in search of a consolation prize. Like at 1440p, the RTX 5070 starts off inexplicably worse than the RTX 4070 Super when path tracing in involved, though in both games it still forms itself a solid enough base to apply 4x MFG without input lag coating your PC innards in treacle.
You’ll have to forgive me for not sharing these results with the kind of bubbly enthusiasm that multicoloured bar charts deserve. I know it’s not like the RTX 5070 has regressed to the point that it’s a bad GPU in the general sense – get it at RRP, like this PNY model costs, and it’s still a fine multi-discipline graphics card that will comfortably play anything outside of the most brutalising 4K settings. And what I was saying earlier, about the RTX 4070 Super going cheaper faster? Reader, I’m afraid that was conjecture. Right now, these are still on sale in the £600-£700 range, and while plenty of board partner RTX 5070s will surely fill the same space, if these two GPUs cost the same then there’s no compelling reason to stick with the older one. There’s maybe an argument in favour of the RTX 4070 Super’s power efficiency – I measured it peaking at 219W during my tests, versus 251W on the RTX 5070 – but for all its shortcomings, I would rather have MFG than a 30W saving.
Even so, it’s also completely fair and reasonable to expect some kind of meaningful, not wholly AI-reliant improvement from each new generation. The RTX 5070 simply does not deliver, and I’m honestly a little worried that if we simply shrug and accept that, frame generation really will be the only way that future GPUs bother to upgrade ""performance"". Even when, as this very card’s 4K path tracing results show, such an approach is inadequate when the games side of the industry continues to ask more and more of your hardware.
This review is based on a retail unit provided by the manufacturer.

Find out how we conduct our reviews by reading our review policy.    

        Rock Paper Shotgun is better when you sign in
      

        Sign in and join us on our journey to discover strange and compelling PC games.
      
© 2025 Rock Paper Shotgun, an IGN Entertainment, Inc. brand. 18 Mansell Street Level 3, London, E1 8AA, United Kingdom. All rights reserved. No part of this website or its content may be reproduced without the copyright owner's permission. Rock Paper Shotgun is a registered trademark of Gamer Network Limited, an affiliate of IGN Entertainment, Inc."
"NVIDIA","RTX 5070","RTX 5070 stock: Nvidia's Founder's Edition GPU is available in the UK","https://www.gamesradar.com/hardware/desktop-pc/where-to-buy-rtx-5070/","Where to buy the $549 / £539 GeForce GPU and live stock updates.

When you purchase through links on our site, we may earn an affiliate commission. Here’s how it works.

RTX 5070 stock is looking healthier than before, but getting the GPU at MSRP is tricky in the US. Players in the UK can grab the Blackwell graphics card for £539 at various retailers, and even the Founder's Edition is now in stock. However, I'm not seeing the same selection at Newegg, Best Buy, or Amazon for $549. Whether that's down to new US tariffs or just custom pricing remains to be seen, and I'll be on the hunt for affordable models regardless.
The RTX 5070 joined the best graphics card race on March 5, and there is admittedly more stock available now. Amazon has now got a model starting at $794.74, but that's still way more than I'd advise paying for the Nvidia GeForce RTX 5070. If you're in the UK, though, you'll be able to swing by retailers like Scan and pick one up for £539.99.
When RTX 5070 prices in the US normalize, it should mark the beginning of more affordable next-gen GPUs. The RTX 5070 Ti is also meant to be a middle ground option between premium and mid-range, but that doesn't really pan out when it's well above $749 / £739. So, I'll be consistently hunting for both versions at MSRP to help you upgrade your PC for the right price.
April 25, 2025 - RTX 5070 stock is looking healthier than last week, but prices in the US are still higher than MSRP. On average, you're looking at around $650 for the cheapest custom models, but the UK has plenty of options for £530. I've updated this page to reflect availability in both regions and have adjusted the advice to reflect the situation with inflated price tags and the potential effect of tariffs. 
Newegg has some RTX 5070 stock now available, including an Asus Prime model for $699. That's still $150 more expensive than it should be, but those of you who are willing to pay a little more should check it out.
RTX 5070: From $659.99RTX 5070 Ti: Out of stock
Just like everywhere else, RTX 5070 stock is non-existent at Best Buy right now. I'm continually checking in at the big box store in case models show up, but I'm also wary that many of its listings are vastly more than $600.
RTX 5070: From $659.99RTX 5070 Ti: From $939.99
Be extremely wary when browsing Amazon for RTX 5070 stock. It's probably the first place that will get resellers trying to flog their GPUs at monstrous prices, so unless a model pops up at MSRP, you'll want to hang fire.
RTX 5070: From $659.99RTX 5070 Ti: Check stock
I've just spotted a new Gigabyte GeForce Eagle RTX 5070 at B&H for $669.99, which happens to be the cheapest model I've come across since launch. There's no sign of MSRP stock, but prices are seemingly coming down as retailers replenish.
RTX 5070: From $604.99RTX 5070 Ti: From $959.99
Nvidia currently has custom models listed via Micro Centre, but there's no available RTX 5070 stock at the moment. The Founders Edition also isn't scheduled to show up until later this month, so you'll want to focus on getting another version at MSRP.
RTX 5070: Out of stockRTX 5070 Ti: Out of stock
The UK Nvidia store actually has the Founder's Edition in stock right now, but there are also other custom flavours available through the green team's site and associated retailers.
RTX 5070: From £549.98RTX 5070 Ti: From $869.99
There are plenty of RTX 5070 Ti models listed and ready to go at Overclockers UK, but they do some in above MSRP. The cheapest version is the $799 PNY GeForce RTX 5070 Ti OC, but other models like the Gigabyte Gaming OC will set you back nearly a grand. There are also some pre-built gaming PCs with 5070 Ti cards inside them, which could be the move.
RTX 5070: From £529.99RTX 5070 Ti: From £728.99
The absolute cheapest RTX 5070 cards are currently out of stock, but you're looking at just under $550 for an Asus Prime model. Not too shabby since the MSRP is $529.99, so well worth considering if you need one right now.
RTX 5070: From £529.99RTX 5070 Ti: From £729.98
Currys certainly isn't the first retailer I would check when buying an RTX 5070 Ti, but it's still an option if things are looking rough. There are a couple of 5070 Ti cards listed here, although you'll end up paying a little bit more than MSRP.
RTX 5070: Out of stockRTX 5070 Ti: Out of stock
There are a select few RTX 5070 cards at Ebuyer right now starting at around £540. so not a bad option if you miss the boat with some of the cheapest listings at other retailers.
RTX 5070: From £559.99RTX 5070 Ti: From £889.99
The RTX 5070 actually represents a decrease in price generation-to-generation. The card will be available for $549 / £549 - that's less than the RTX 4070, which launched at $599 / £589 in 2023. Meanwhile, the RTX 5070 Ti price is set at $749 / £749.
The RTX 5070 will officially arrive on March 5 at 9am ET, so you'll want to head straight to retailers or the Nvidia Store to grab a GPU. If previous launches are anything to go by, the graphics card could sell out pretty quickly, and there will also be muliple custom models that cost above MSRP.
On paper, the RTX 5070 looks set to completely take over from the 4070, with double the performance when both DLSS and Full RT are engaged. That's according to Nvidia, we'll have to wait and see how the card performs in our own testing. However, with its AI improvements in DLSS 4 and Nvidia Reflex 2, things are certainly looking snappier. 
My Nvidia GeForce RTX 5070 review is now live, and while you'll have to rely on AI upscaling to pull off that RTX 4090 performance promise, it's still a solid mid-range GPU. At 1440p, you'll be running games like Cyberpunk 2077 at well over 100fps without DLSS 4. However, if you do fancy playing at 4K with ray tracing enabled, Multi-Frame Generation can boost things beyond what the previous flagship could achieve.
If you've already for a 40-series GPU like the RTX 4070 Super, upgrading to the RTX 5070 probably isn't worthwhile. Extra Multi-Frame Generation aside, the two models aren't drastically different, and this card feels more like an RTX 3090 replacement for under $600. Keep in mind that GPUs from a few generations ago completely lack any sort of Frame Generation support, and the tool is pretty much the key to running games at 4K with ray tracing on without spending over $1,000.
Before you completely settle on an RTX 5070, it is worth noting that AMD RX 9070 series stock is also set to arrive on March 6. Both the XT and vanilla card have Nvidia's mid-range cards in their sight, and they'll be properly challenging the green team's duo with respective $549 and $599 price tags. I'll be waiting till I have full benchmarks for all cards before calling a victor, but if it's a next-gen mid-range GPU you're after, the RTX 5070 isn't your only option.
The RTX 5070 Ti is already available to buy, and I'd say it feels more like an RTX 5080 alternative than a souped up 70-class GPU. During testing, I was able to hit frame rates that weren't too far off Nvidia's $999 model, and its certainly the 4K card I think a majority of players will end up grabbing.
Weekly digests, tales from the communities you love, and more
If you want comfortable 4K performance without much compromise or an excessive need for AI upscaling, the RTX 5070 Ti might be the perfect GPU for you. There's no Founder's Edition, so you will be at the mercy of custom AIB pricing. Plus, the card is about the same size as many RTX 4090 versions since brands are using the same colling setups this time around. However, the UHD experience at hand is still pretty great, and features like 16GB GDDR7 VRAM and its beefy GPU may keep your rig hitting system requirements further into the future.
The Nvidia GeForce RTX 5070 should start at $549 / £539, but that MSRP only really applies to a select few graphics cards. Since you won't be able to grab a Founders Edition GPU at launch, you'll instead want to eye up alternatives like the Asus Prime OC since they'll stick with recommended pricing.
Actually picking out RTX 5070 MSRP stock at retailers is a little tricky, so I've plucked out the cards I'd go straight for if you want to spend well under $600. All my picks happen to be at Newegg, and the retailer seems like a safe bet since all its early listings come in at MSRP:
Nvidia GeForce RTX 5070 Ti prices start at $749 / £729, but there's no guarantee that you'll find one at that MSRP. While there are models that will stick with the base price, you'll end up paying more for some variants due to additional RGB lighting, different cooling solutions, and factory overclocks.
Some of you may want to pick up a graphics card with a specific aesthetic, but if you're specifically hunting for RTX 5070 Ti stock at MSRP, you'll want to check out there specific models:
Looking to build an entire system? Swing by the best CPU for gaming and best RAM for vital rig parts. Alternatively, check out the best Alienware gaming PC builds if you're not into DIY.
Managing Editor of Hardware at GamesRadar+, I originally landed in hardware at our sister site TechRadar before moving over to GamesRadar. In between, I've written for Tom’s Guide, Wireframe, The Indie Game Website and That Video Game Blog, covering everything from the PS5 launch to the Apple Pencil. Now, i'm focused on Nintendo Switch, gaming laptops (and the keyboards, headsets and mice that come with them), PS5, and trying to find the perfect projector. 
Please logout and then login again, you will then be prompted to enter your display name.

GamesRadar+ is part of Future plc, an international media group and leading digital publisher. Visit our corporate site.

©
Future Publishing Limited Quay House, The Ambury,
Bath
BA1 1UA. All rights reserved. England and Wales company registration number 2008885."
"NVIDIA","RTX 5070","Nvidia GeForce RTX 5070 Review: A New Contender","https://siriuspowerpc.com/nvidia-geforce-rtx-5070-review/","The Nvidia GeForce RTX 5070 has been one of the most anticipated releases in the GPU market, especially with its launch on March 5, 2025, just ahead of AMD’s RX 9070 series. This new addition to Nvidia’s RTX 50-series aims to strike a balance between performance and affordability, positioning itself as a mid-range option that offers significant improvements over its predecessors without reaching the heights of the flagship RTX 4090.
The RTX 5070 is built on the 5 nm manufacturing process, featuring the GB205-300-A1 variant with approximately 31,000 million transistors and a die size of around 263 square millimeters. This compact design ensures a balance between power and efficiency, making it suitable for both gaming and professional tasks. 
The card comes with 12 GB of GDDR7 memory connected via a 192-bit bus interface, providing a bandwidth of 672.0 GB per second thanks to its effective memory speed of 28 Gbps. These specifications suggest that the RTX 5070 is well-equipped to handle high-resolution textures and complex calculations required by modern software.
In terms of raw performance, the RTX 5070 offers an 18% increase in speed over the RTX 4070 at 1440p and a 22% increase at 4K resolutions. However, compared to the RTX 4070 Super, the improvements are more modest, with only a 3% increase at 1440p and a 4% increase at 4K. 
While these gains might not be dramatic, they are significant for users upgrading from older models like the RTX 3070 Ti, where the RTX 5070 provides a substantial 55% performance boost.
The RTX 5070 is primarily designed for 1440p gaming, but it can achieve playable frame rates at 4K with the help of DLSS (Deep Learning Super Sampling) and other optimizations. For those seeking enhanced 4K performance, the RTX 5070 Ti offers approximately 29% higher frame rates, though at a significantly higher cost.
Nvidia recommends a power supply of 650 watts for the RTX 5070, matching the requirements for the RTX 4070 and RTX 4070 Super. The graphics power has increased to 250 watts, up from 200 watts for the RTX 4070 and 220 watts for the RTX 4070 Super1. In practical testing, the RTX 5070 averaged around 210 watts across nine games, slightly above the RTX 4070 Super’s average of 206 watts.
The peak power draw reached 250 watts during the Metro Extreme benchmark, with a maximum temperature of 77 degrees Celsius recorded, which is higher than the RTX 4070 and RTX 4070 Super.
Despite the increased power consumption, Nvidia has managed to maintain efficiency, which is a notable achievement considering the performance gains. However, the RTX 5070 operates hotter than its predecessors, which may require more robust cooling solutions.
Beyond gaming, the RTX 5070 shows improved performance in video editing and AI tasks. In Proon’s XL benchmark, it outperformed the RTX 4070 by nearly 28% and surpassed the RTX 4070 Super by almost 4%. 
For video editing, using DaVinci Resolve, the RTX 5070 was about 11% faster than the RTX 4070 and approximately 10% faster than the RTX 4070 Super. These improvements make the RTX 5070 a viable option for content creators seeking a balance between cost and performance.
The RTX 5070 is priced at $549, which is $50 cheaper than the RTX 4070. This pricing strategy positions the RTX 5070 as a more affordable option for those seeking improved performance without breaking the bank. However, the market is competitive, with AMD’s RX 9070 series looming on the horizon, which could potentially challenge Nvidia’s offerings in terms of price and performance.
The Nvidia GeForce RTX 5070 represents a solid mid-range offering in the GPU market, providing a balance of performance and efficiency. While it may not match the performance of higher-end models like the RTX 4090, it offers significant improvements over its predecessors and is well-suited for 1440p gaming and creative applications. 
The RTX 5070’s pricing makes it an attractive option for those looking to upgrade without the hefty cost associated with flagship GPUs. As the GPU landscape continues to evolve, the RTX 5070 will likely find its niche among gamers and content creators seeking reliable performance at a reasonable price.
The RTX 5070 Ti offers better performance, particularly at 4K resolutions, but at a higher cost. The choice between these models will depend on specific needs and budget constraints. 
Nvidia’s strategy of offering a range of options within the RTX 50 series caters to different segments of the market, ensuring that there is a suitable GPU for various user requirements.
Overall, the RTX 5070 is a capable GPU that delivers on its promise of improved performance and efficiency, making it a worthwhile consideration for those in the market for a mid-range graphics card.
Getting a gaming PC under $1500 that delivers real value takes smart shopping these days. High-end gaming systems cost upwards
AMD Radeon RX 9060 XT is here with 2048 cores,		
The next generation of graphics cards is officially on the horizon, and NVIDIA has once again raised the bar. The
Elevate your gaming experience with Sirius Power PC - your gateway to superior desktop pcs and premium PC components."
"NVIDIA","RTX 5070","Nvidia GeForce RTX 5070 review and benchmarks","https://cirrkus.com/news/trends/nvidia-geforce-rtx-5070-review-and-benchmarks/21332255/","The RTX 50-series launch hasn’t gone smoothly so far. Severely limited stock, high prices, and manufacturing issues have left PC gamers frustrated. Now, Nvidia is trying to get things heading in the right direction with the RTX 5070, a GPU that it promised would deliver $1,599 RTX 4090-like performance for $549.
It doesn’t, and it was never going to. It’s about 20 percent faster than the RTX 4070 and a bare 4 percent faster than last year’s RTX 4070 Super. It’s a solid card for 1440p gaming, just like the RTX 4070, but as with the rest of the 50-series cards, Nvidia’s performance claims rely on Frame Generation rather than meaningful improvements to rendering.
At $549, the RTX 5070 will also come up against AMD’s new $549 Radeon RX 9070 and $599 Radeon RX 9070 XT when they launch on March 6th. If AMD manages to beat Nvidia’s RTX 5070 or come close to the $749 RTX 5070 Ti, that’s going to put a lot of much-needed pressure on Nvidia’s pricing.
You might want to wait a couple of days to purchase anything until we can talk about those AMD cards.
The RTX 5070 Founders Edition card I’ve been testing looks like a miniature RTX 5090 with a slightly darker paint job and no LEDs. It’s still a two-slot card, and it retains the two fans at the bottom that direct cooler air over the card and exhaust it out of the top, as well as the slightly angled 12V-2×6 power connector that makes it easier to fit into more cases. 
I really like the size. It feels like the ideal companion for a small form factor PC, and I don’t think I’d be too worried about the heat this little GPU would generate or the power draw (more on that later).
I’ve been testing the RTX 5070 card with AMD’s Ryzen 9 9800X3D processor and Asus’ 32-inch 4K OLED PG32UCDP. I’ve put it up against the previous RTX 4070 and RTX 4070 Super cards, as well as the more expensive RTX 5070 Ti and AMD’s RX 7900 XT, which you can see in the benchmark charts below.
I’ve tested a variety of games at both 1440p and 4K, including more demanding titles like Black Myth: Wukong and benchmarking favorites like Cyberpunk 2077 and Shadow of the Tomb Raider. All games have been tested at very high or Ultra settings, so you’ll get a good idea of the RTX 5070’s true capabilities, though you’ll probably want to drop the settings for better frame rates.
As with every other 50-series card, the 5070 isn’t much of an upgrade in raw performance over the last generation. Without DLSS or RT enabled, the RTX 5070 is 18 percent faster than an RTX 4070 at 1440p and 22 percent faster at 4K; compared to the 4070 Super, it’s only 3 percent faster at 1440p and 4 percent at 4K. You’ll see a solid boost if you’re coming from a 30-series card or older, though; it’s nearly 55 percent faster than the RTX 3070 Ti. 
The RTX 5070 is still largely a 1440p card, which has been the sweet spot for Nvidia’s 70 lineup for a few generations. While it delivers playable frame rates for most games in my test suite at 4K resolution, you’ll need to enable DLSS in some titles if you want to be over 60fps consistently with all the settings maxed out. If you’re willing to drop some quality settings and enable DLSS, the RTX 5070 becomes a lot more capable.
If you’re looking for better 4K performance, stepping up to an RTX 5070 Ti gets you about 29 percent higher frame rates, but you’ll pay at least 35 percent more, assuming either card stays anywhere near MSRP.
My main concern about the RTX 5070 for 4K gaming is its 12GB of VRAM. I’ve already run into performance issues testing entry-level 4K gaming with the 8GB RTX 3070 Ti in certain games, and I don’t think 12GB of VRAM is going to fare well in the future, either. This is particularly relevant when AMD is about to ship its RX 9070 and RX 9070 XT with 16GB of VRAM. I wish Nvidia had done the same.
It was obvious from the start that Nvidia’s claims that the RTX 5070 would match the RTX 4090 relied entirely on DLSS 4’s Multi Frame Generation. The 50-series cards have up to 4x frame generation with DLSS 4, so they can generate up to three additional frames per frame rendered traditionally. The RTX 4090 can use DLSS 4, including Nvidia’s updated transformer models, but only has 2x frame generation.
It’s only the fact that the 5070 can generate three times as many interstitial frames as the RTX 4090 that lets it appear anywhere near as fast. In Cyberpunk 2077 at 4K with Ultra settings, ray tracing enabled, and 4x Frame Generation, the RTX 5070 averaged 122fps. The older RTX 4090, with 2x Frame Generation, managed 128fps, and it’s rendering far more of them the old-fashioned way.
If you take Nvidia at face value and assume performance equals frame rate, the RTX 4090 is still 5 to 10 percent faster than the RTX 5070 when frame generation is cranked up all the way on both cards. In traditional rendering, the RTX 4090 is 75 percent faster on average. Without ray tracing or frame generation, the RTX 4090 got 76fps in Cyberpunk 2077, and the RTX 4070 got 48fps. 
As I’ve discussed in previous reviews, and as even Nvidia’s materials show, multi-frame generation makes gameplay look smoother, but the game will still feel sluggish if the base frame rate is low. You can really feel it on the RTX 5070, especially in 4K. With DLSS Super Resolution and x4 Multi Frame Generation, Cyberpunk 2077 hit 77fps at Ultra settings with full path tracing, but it still feels like the 24fps it is before Multi Frame Gen is applied.
Multi Frame Gen makes more sense at 1440p because the base frame rates are a lot higher. I wouldn’t buy the RTX 5070 because of Multi Frame Generation alone, but the DLSS 4 improvements and the new transformer model have certainly improved image quality for me in a variety of games, and you can even force DLSS 4 in unsupported games through Nvidia’s new app. But you don’t need an RTX 5070 for DLSS 4, so if you don’t care about Multi Frame Generation and you can somehow find an RTX 4070 Super at a reasonable price, it’s still a good choice.
Beyond gaming, the RTX 5070 can also deliver better performance in video editing or for AI workloads. In Procyon’s AI XL (FP16) test, the RTX 5070 is nearly 28 percent faster than the RTX 4070, or nearly 4 percent faster than the RTX 4070 Super. For video editing, I tested the RTX 5070 with PugetBench’s DaVinci Resolve test and found it’s 11 percent faster than the RTX 4070 and nearly 10 percent faster than the RTX 4070 Super.
Nvidia recommends a 650-watt power supply for the RTX 5070, which is exactly the same as both the RTX 4070 and RTX 4070 Super. The total graphics power has increased to 250 watts in total, up by 50 watts over the RTX 4070 and just 30 watts over the RTX 4070 Super.
At 4K resolution, the RTX 5070 averaged 210 watts across the nine games tested — 25 watts more than the RTX 4070 averaged (185 watts) and just four watts more than the RTX 4070 Super (206 watts). The RTX 5070 hit its max power draw of 250 watts during the Metro Exodus Extreme benchmark, and it hit a maximum temperature of 77 degrees Celsius in that same test on my open bench, compared to 67C on both the RTX 4070 and 4070 Super.
I’m once again impressed that Nvidia hasn’t massively increased the power requirements here, but it looks like the RTX 5070 will run hotter than previous cards. While the RTX 5090 is certainly power-hungry, both the RTX 5070 and RTX 5070 Ti are a lot more efficient, while delivering gains over the previous-generation cards.
I feel like a broken record saying that the RTX 5070 continues the trend of modest generational improvements with the RTX 50 series, but unfortunately, that’s the case here again. This is really an ideal 1440p card thanks to the high frame rates at that resolution, or an entry-level 4K card if you’re willing to drop settings down and enable DLSS.
But it’s hard to recommend the RTX 5070 until the question hanging over it has been answered: AMD’s RX 9070-series cards. AMD surprised everyone with a $549 price tag for its RX 9070 and $599 for its RX 9070 XT last week. Both cards are challenging Nvidia’s pricing for its RTX 5070 and RTX 5070 Ti, and if they’re good enough, they might put pressure on Nvidia to adjust its pricing.
Until we know how all four cards compare, I would hold off on buying an RTX 5070 on day one. AMD might just surprise us with its performance as well as its pricing.
Mission: Impossible – The Final Reckoning Worldwide Box Office: Already Crosses A Major Milestone Before Closing Its Big Domestic Weekend →
Memorial Day Sales on Mattresses and Bedding →
Travis Kelce Offered Hairy Back Solutions, Waxes and Razors! →
Hits The $250 Million Mark Ahead Of Memorial Day! →
Rihanna and A$AP Rocky Party in DJ Booth at Cannes Party, New Video →
When Sofia Coppola Protected Teenage Kirsten Dunst During A Tough The Virgin Suicides Scene With This Clever Move →
Mission: Impossible and Fortnite just both made huge comebacks →
Continues Its Advantageous Trend, Outpaces Deadpool & Wolverine →
METALLICA’s KIRK HAMMETT Joined By ROBERT TRUJILLO For Jam At Philadelphia Stop Of ‘The Collection: Live’ Book Tour →
Billy Ray Cyrus, Liz Hurley Take Their Romance on the Road to Rome →
How Tall Is Henry Cavill Compared To Other DC Actors? Find Out Who Towers Over Superman! →
When Matt Damon Went Off-Script In Saving Private Ryan, What Steven Spielberg Did Next Surprised Everyone! →
Bianca Censori Makes Imaginations Run Wild In Pantless Outfit →
Chiara Ferragni vs. Olivia O’Brien Who’d You Rather?! (Cuties In Crochet Edition) →
I remember mum screaming through the mailbox, says Barry Keoghan as he opens up on traumatic childhood →
Justin Bieber Awkwardly Kisses SZA While His Wife Watches →
The 10 best movies we saw at the 2025 Cannes Film Festival →
Trombone Shorty Teams Up With Acura To Enrich NOLA Youth →"
"NVIDIA","RTX 5070","NVIDIA GeForce RTX 5070 Content Creation Review","https://www.pugetsystems.com/labs/articles/nvidia-geforce-rtx-5070-content-creation-review/","At CES, NVIDIA announced its next generation of consumer desktop graphic cards: the GeForce RTX™ 50 series, based on the Blackwell architecture, including the recently launched NVIDIA GeForce RTX™ 5070. As we covered a couple of weeks ago in our RTX 5070 Ti review, these GPUs promise improved performance in gaming and content creation and feature more and improved CUDA cores, fourth-generation Ray Tracing and fifth-generation Tensor cores, and the latest NVIDIA NVENC/NVDEC media engines. These hardware updates are packaged alongside a multitude of new technologies like DLSS 4, RTX Mega Geometry, and new NVIDIA Broadcast features.
In this article, we will be reviewing the new midrange NVIDIA GeForce RTX 5070. Replacing the outgoing NVIDIA GeForce RTX 4070 SUPER, the 5070 is likely to be the lower end of NVIDIA’s mid-range graphics card stack. 70-class cards usually are the everyman’s card, offering a good balance between performance and price, though whether the -70 or -70 Ti is a better value seems to change with each generation.
NVIDIA has introduced a number of new features and capabilities with this GPU generation, such as Neural rendering, Mega geometry, and DLSS 4 plus MFG. While we won’t be covering those in this article, we hope to do so in the future. Nonetheless, many of them are exciting for both gamers and professionals, and we continue to be impressed by NVIDIA’s software and feature support. If you are interested in the new features for content creators, we have a blog post available that summarizes everything NVIDIA has announced to date: NVIDIA GeForce RTX 50-Series | Features for Content Creators.
Below, we have listed the most relevant GPU specifications from AMD, Intel, and NVIDIA. For more information, visit Intel Ark, NVIDIA’s 40-series GeForce page, NVIDIA’s 50-series GeForce page, or AMD’s Radeon RX Page.
Unlike the prior 50-series cards, the RTX 5070 seems, in some ways, more like an upgraded 3070 than an upgraded 4070 SUPER. Indeed, from the 4070 SUPER, the RTX 5070 features 15% fewer CUDA cores alongside a marginal 37 MHz higher boost clock. The base clock is 345 MHz higher, so we may see higher average clocks, but in most cases we expect the boost clock to be a better predictive specification. On the positive side, the 5070 features an improved VRAM bus, with a throughput of 672 GB/s, compared to the 4070 SUPER’s 504 GB/s. However, this comes along with 30 W more power draw and the same amount of VRAM. In this day and age, 12 GB of VRAM on a $550 card is too little for too much, and we would have hoped for 16 GB. Overall, despite a nominal $50 price reduction, we are unimpressed with the 5070 on paper—we don’t expect much from the card over the 4070 SUPER in either performance or value.
For our GPU testing, we have shifted to an AMD Ryzen 9 9950X-based platform from our traditional Threadripper platform. The 9950X has fantastic all-around performance in most of our workflows and should let the video cards be the primary limiting factor where there is the possibility of a GPU bottleneck. This means the results are more comparable to our recent Intel Arc B580 review but less so to our past GPU reviews. However, at this point in our 50-series testing, we have benchmarked most of the GPUs from the last few generations, so there should be plenty of comparative data for most needs. For testing, we used the latest available GPU drivers, though as we are re-using data from our previous 50-series testing in this review, we do have a few different drivers in the mix; we have not seen large performance differences between them. We tested everything on the “balanced” Windows power profile, while Resizeable BAR and “Above 4G Decoding” were enabled for every GPU as well.
In this article, our primary focus will be the new 5070, so we tested it against the most comparable cards from the last few generations: the RTX 4070 SUPER and RTX 3070. This is in addition to all the cards we tested in our previous 50-series reviews, so we still have most of the RTX 30- and 40-series cards, in addition to the 2080 Ti and 7900 XTX. We think that the 4070 SUPER is the best comparison from the last generation, and from the 30-series cards, we’re interested in seeing how it compares to both the 3070 Ti and 3070.
Unlike the 5070 Ti, the 5070 has a first-party FE (Founders Edition) model available from NVIDIA. We will be testing with that card as it should be representative of the baseline performance you get from any 5070, without additional factory overclocking or possibly-questionable coolers. We are still using the ASUS GPUs for the 5070 Ti, 4070 Ti, and 3070 Ti we mentioned in our previous review, as we have been generally impressed with the quality and reliability of ASUS’ cards and use them extensively in our own systems and in past testing.
In terms of applications, the new NVIDIA Blackwell cards have some lingering compatibility issues at present as we await developers’ integration of the new CUDA 12.8 and TensorRT 10.8 toolkits. As a result, the RTX 50-series of graphics cards is not supported in Redshift (Cinebench) or Octanebench—though the latest version of the core Octane renderer does support them—and has performance issues in V-Ray CUDA rendering. Due to this, we have slightly fewer results than is typical for our GPU reviews: PugetBench for Premiere Pro, After Effects, DaVinci Resolve, and Lightroom Classic, as well as Unreal Engine, V-Ray, and Blender.
We build computers tailor-made for your workflow. 
Get in touch with our technical consultants today.
We choose our benchmarks to cover many workflows and tasks to provide a balanced look at the application and its hardware interactions. However, many users have more specialized workflows. Recognizing this, we like to provide individual results for benchmarks as well. If a specific area in an application comprises most of your work, examining those results will give a more accurate understanding of the performance disparities between components. Otherwise, we recommend skipping over this section and focusing on our more in-depth analysis in the following sections.
One of the more frequent requests we get for software testing is Adobe’s Lightroom Classic. Although we have a benchmark for the application, at present, it tests relatively few GPU-accelerated features and so is typically a lower priority for us during GPU reviews. Nonetheless, we understand that even seeing things like GPU-accelerated image export times can be useful in purchase decisions for heavy LRC users, so we do our best to include Lightroom testing where possible. Additionally, we hope to have a revamped Lightroom Classic benchmark finished later this year, which should include many more tests and be more in line with modern Lightroom workflows.
In the overall score (Chart#1), the 5070 is on-par with the RTX 4090, although the majority of the tests in Lightroom Classic are unaccelerated, so the overall score really isn’t all that meaningful since almost every GPU performs within the margin of error.
When looking at a GPU-accelerated portion of the benchmark, image exporting (Chart #2), there is a larger difference between cards. Here, the 5070 is well ahead of the 30-series cards, leads the 4070 SUPER by 3%, and falls behind the 4070 Ti SUPER by 5%. The latter two are within the margin of error, especially since Lightroom tends to have a larger amount of variance in its tests.
Premiere Pro is a much more GPU-accelerated application than Lightroom Classic. In it, the RTX 5070 is essentially equivalent to a 4070 Ti SUPER and leads the base 4070 SUPER and 3070 by 6% and 18%, respectively. This is even given that Adobe has yet to fully add support for the new media engines of the 50-series cards, meaning we expect this gap to grow in the future.
Our LongGOP tests (Chart #2) are where we would expect the media engines to play a large role in elevating the performance of the 5070. Although NVIDIA’s next-generation NVENC/NVDEC are faster than last gen, making the 5070 6% faster than the 4070 SUPER and 17% faster than the 3070, when support for the new H.264/HEVC 4:2:2 10-bit acceleration is implemented by Adobe, we expect this lead to grow.
Intraframe codecs (Chart #3) are not GPU-accelerated, and predictably, we see no real differentiation between GPUs. However, we do see some in our RAW tests (Chart #4). RAW codecs use a mix of CPU and GPU, with the exact ratio changing based on the individual codec. On average, for RAW codec processing, the 5070 outperforms the 4070 SUPER by 2%; well within our margin of error. This does put it 27% ahead of the 3070, though, making it a good upgrade if you are currently a few generations behind.
The final set of tests in our Premiere Pro benchmark is for GPU Effects (Chart #5). Here, the 5070 nearly matches the 4070 Ti SUPER, with a score 15% ahead of the RTX 4070 SUPER and nearly three times the performance of the 3070. Overall, the 5070 offers a small upgrade over the outgoing 4070 SUPER, for, nominally, $50 less. This makes it a fine value at MSRP, and especially compelling as an upgrade from a midrange 30-series GPU.
Though traditionally a CPU-based application, After Effects has added 3D capabilities that rely heavily on GPU acceleration. We recently updated our Pugetbench for After Effects benchmark to test these capabilities, and we are excited to be able to include After Effects in our GPU reviews.
In terms of overall performance (which includes both CPU and GPU-focused tasks), the 5070 scores 7% higher than the 4070 SUPER and 18% higher than the 3070. Two of our subscores, 2D (Chart #2) and Tracking (Chart #4), are principally CPU-bound, so we see no real difference in them. However, in 3D workflows (Chart #3), the 5070 extends the leads to 14% and 52% over the 4070 SUPER and 3070, respectively. These are both great generational and skip-generation uplifts.
For traditional After Effects usage, the graphics card is still largely irrelevant, and a cheaper card is typically preferred so that more of your budget can be allocated to CPU, RAM, or storage. However, if you are looking to explore the new 3D workflows in After Effects, the new 5070 can offer a compelling upgrade over older cards and is especially strong compared to AMD’s Radeon cards.
As an application, DaVinci Resolve tends to be more sensitive to GPU performance than Premiere Pro or After Effects. Additionally, NVIDIA has provided a pre-release version of DaVinci Resolve to reviewers which is compatible with 50-series cards and properly uses the updated NVENC/NVDEC engines to accelerate additional “flavors” of LongGOP codecs, such as HEVC 4:2:2 10-bit.
Overall, this means that the 5070 scores 14% higher than the 4070 SUPER and 34% higher than the 3070. Much like Premiere Pro, though, there are some subtests that do not effectively make much use of the GPU, such as Intraframe (Chart #3), RAW (Chart #4), and Fusion (Chart #5). In the first of these, GPU has no effect, while in the latter two, the 5070 is about 11-15% faster than the 4070 SUPER and 30% faster than the 3070.
Our LongGOP tests (Chart #2) are perhaps the most interesting results in DaVinci Resolve, as this includes codec variants that were previously only accelerated with Intel Quick Sync. With the new support for HEVC 4:2:2 10-bit, the 5070 is able to have a 38% performance advantage over the 4070 SUPER, extending to 66% over the 3070. We will note that if you regularly work with H.264/HEVC codecs, it may be worth investing in the higher tier 50-series cards, as they offer additional NVENC (encoding) and/or NVDEC (decoding) chips. 
GPU Effects (Chart #5) and AI (Chart #7) are our final categories of tests. In GPU Effects, the new 50-series card once again flexes its increased compute capabilities to establish a 15% lead over the 4070 SUPER and 51% over the 3070. However, these workloads benefit greatly from a more powerful GPU, so if you make use of a lot of GPU effects, the higher-end cards may be a better value depending on how much time they could save you. We continue to see odd behavior in the AI features of DaVinci Resolve, with the 5070  merely performing on par with the 4070 SUPER. This still allows it to outperform the 3070 by 28%, but we definitely hope future app or driver updates improve the neural engine performance in Resolve.
Topaz Video AI is an application that uses AI models to upscale lower-resolution video to a higher-resolution output or low framerate to a higher framerate. We use the built-in benchmark to test performance across all of the included models at both 1080P and 4K and then combine those together into a singular overall score.
Unfortunately, Topaz Video AI has yet to be updated to fully enable 50-series support, so performance is disappointing compared to last-gen hardware. In fact, the new 5070 is slower than the 4070 SUPER by 22% but still manages to beat the older RTX 3070 by 22%. We would not recommend a 50-series card for Topaz or if you use it in your day-to-day work until the software is updated.
Our Unreal Engine benchmark combines several scenes at varying resolutions and enabled features (e.g., Ray tracing) to see how various common factors affect GPU performance. We combine those FPS results together to get composite scores. Here, we decided to pull out Ray-tracing and Rasterized scenes as individual sub-scores to see how the new RT cores perform.
The Overall FPS (Chart #1) has the 5070 performing on par with a 4070 SUPER, which is a much worse showing than the other 50-series cards. It can still be a good upgrade over older 30-gen cards like the 3070, however, where it is 60% faster. Much like the rest of the 50-series cards, we see little relative change between ray-traced and rasterized performance. Once again, the 5070  is a great upgrade from the 30-series, but much less necessary if you already have a 4070 SUPER.
For offline, GPU-based rendering, we could only test with two of our usual four benchmarks. At present, the 50-series cards have only experimental support in Redshift 2025.13 (but no compatibility with Cinebench 2024, which uses the Redshift renderer) and support in the OctaneRenderer 2025.1-beta 2 (but not in OctaneBench); as such, we were not able to test with either. Additionally, there is currently a known issue with CUDA rendering in V-Ray, resulting in low performance. We expect that this will be fixed as the implementation of the CUDA 12.8 toolkit is rolled out to applications, but it is definitely a note of caution for early adopters of the 50-series.
In V-Ray RTX rendering (Chart #1), the RTX 5070 scores 18% higher than the 4070 SUPER and nearly 2x higher than the 3070. This places it on par with the 4070 Ti SUPER. The 5070 is less impressive in Blender, only matching the RTX 4070 SUPER, although it still leads the 3070 Ti by 90%.
Overall, the RTX 5070 seems to provide moderately better performance than the 4070 SUPER while offering better features at a nominally reduced price. However, MSRP is quickly becoming meaningless, which can quickly erode any advantages the card has over last-gen options and make price-to-performance comparisons something that can change daily.
In video editing and motion graphics, the RTX 5070 is only marginally faster than the RTX 4070 SUPER in most cases, but in GPU-specific workflows, it can be around 15% faster than the previous generation. It is much more attractive if you have an older card like the RTX 3070, which can be 12-34% faster overall and up to 50-60% faster in GPU-heavy situations.
In rendering applications, the 5070 is a mixed bag. It is often only on par with the previous generation RTX 4070 SUPER, although it is 60% to 2x faster than an older RTX 3070. However, there is still the lingering issue of compatibility and performance quirks, so we would recommend buying with caution or holding off for a bit before committing to a 5070 for a rendering system. We are currently maintaining a list of known issues in content creation applications that you can check in on to see when these are resolved.
In total, NVIDIA’s new GeForce RTX 5070 is an OK mid-tier GPU that promises decent performance and features for the price. If you are currently on a 30-series card or older and aren’t looking to break the bank, this could be an excellent option for professional use. The 12GB of VRAM is going to be a limitation if you want to get into heavier GPU-centric workflows like 6K+ video editing, or using Unreal Engine, but if what you are doing is more CPU-bound, or at low enough resolution to not need a ton of VRAM, this can be a great entry- to mid-level option.
If you need a powerful workstation to tackle the applications we’ve tested, the Puget Systems workstations on our solutions page are tailored to excel in various software packages. If you prefer to take a more hands-on approach, our custom configuration page helps you to configure a workstation that matches your exact needs. Otherwise, if you would like more guidance in configuring a workstation that aligns with your unique workflow, our knowledgeable technology consultants are here to lend their expertise.
We build computers tailor-made for your workflow. 
Get in touch with one of our technical consultants today.
Puget Systems builds custom workstations, servers and storage solutions tailored for your work.
Extensive performance testingmaking you more productive and giving better value for your money
Reliable computerswith fewer crashes means more time working & less time waiting
Support that understandsyour complex workflows and can get you back up & running ASAP
A proven track recordas shown by our case studies and customer testimonials
© Copyright 2025 - Puget Systems, All Rights Reserved."
"NVIDIA","RTX 5070","NVIDIA GeForce RTX 5070 Founders Edition Review","https://www.storagereview.com/review/nvidia-geforce-rtx-5070-founders-edition-ai-on-a-budget","NVIDIA GeForce RTX 5070 Founders Edition Review: We test its Blackwell architecture, DLSS 4, AI performance, and gaming benchmarks.
The NVIDIA GeForce RTX 5070 Founders Edition joins the new 50-series lineup, arriving after the high-end 5080 and showcasing NVIDIA’s Blackwell architecture in a more mid-range package. While much has been speculated about its power efficiency, AI horsepower, and ray-tracing improvements, we will find out for ourselves in real-world testing. After running it through various benchmarks—from AI text generation to advanced 3D rendering—we can tell you exactly where it lands in NVIDIA’s next-gen roster.
Gamers and hardware enthusiasts are keen to see if the RTX 5070 can finally offer the strong price-to-performance ratio that midrange cards have always promised. Over the past few years, multiple RTX generations have been plagued by supply constraints, inflated prices, and underwhelming generational leaps, leaving consumers eager for more meaningful improvements.
On paper, the RTX 5070 promises a nice leap from the RTX 4070, touting DLSS 4’s Multi Frame Generation, higher memory bandwidth, and extra raw compute power—all in service of smoother gaming and snappier productivity. So, let’s take a look at its features, specifications, and details, then dive into how these advancements truly stack up against the previous generation, separating hype from real-world performance gains.
Leveraging the PCIe Gen5 interface, the RTX 5070 features 6,144 CUDA cores, running at a 2.16 GHz base clock and boosting up to 2.51 GHz. This results in 31 TFLOPS of FP32 compute performance, a clear improvement from the 4070’s 29 TFLOPS. 4th-gen ray tracing cores deliver 94 TFLOPS of RT power, offering better real-time reflections, global illumination, and path tracing support. 5th-gen Tensor cores push 988 AI TOPS, significantly improving AI-enhanced upscaling, frame generation, and computational workloads.
Compared to the RTX 4070 FE, NVIDIA indicates that the 5070 FE boasts up to a 1.9x improvement in 3D rendering when using DLSS 4 with Multi Frame Generation and 20% more rasterization performance. The difference is even more dramatic for those still using an RTX 3070: over 3x performance with DLSS 4 and 65% better traditional raster performance.
One of the most considerable hardware improvements is the jump to GDDR7 memory. The RTX 5070 retains 12GB of VRAM but upgrades from GDDR6X to GDDR7 on a 192-bit bus, increasing memory bandwidth to 675 GB/s—a 33% boost over the 4070’s 504 GB/s. This means faster texture loading, better 4K performance, and improved handling of large datasets in creative applications.
DLSS 4 support is one of the biggest draws of the RTX 50 series. It introduces Multi Frame Generation, a new AI-powered technique that creates multiple frames per rendered frame, significantly boosting FPS in supported games. DLSS 4 enhances Ray Reconstruction and Super Resolution, improving image clarity and stability.
At launch, over 75 games already support DLSS 4, including newer games like Avowed and Kingdom Come Deliverance II and older releases like the massively popular Red Dead Redemption II and Microsoft Flight Simulator. Thanks to the new RT cores and AI-assisted denoising techniques, games that leverage ray tracing and path tracing will likely see a noticeable boost.
For competitive gamers, NVIDIA Reflex 2 brings Frame Warp, a new method for reducing system latency by dynamically adjusting frame timing based on real-time mouse input. This could benefit fast-paced shooters like Overwatch, Call of Duty multiplayer, Marvel’s Rivals, and other esports titles.
Beyond gaming, the RTX 5070 is great for creators. One major upgrade is dedicated hardware-accelerated 4:2:2 video encoding, which makes exporting professional-grade video up to 6x faster than the RTX 4070. NVIDIA Studio drivers provide optimized performance in video editing, 3D rendering, and live streaming, with AI-powered tools integrated into over 130 creative applications.
For AI-based workflows, the 5070 delivers three times the GenAI performance of the RTX 4070, thanks to FP4 acceleration and the latest Blackwell tensor cores. While certainly not the same power as the upper models of the 50-Series, this still makes it a powerful choice for machine learning, AI-driven image processing, and computational photography—helping users train models faster, enhance images with AI, and push the limits of creative automation.
Despite its increased 250W TGP (up from the 4070’s 200W), the RTX 5070 remains surprisingly efficient given the significant performance boost. NVIDIA has balanced power draw and computational output well, ensuring users get a significant performance boost without a disproportionate jump in energy consumption. While the higher TGP might raise eyebrows, the efficiency and cooling improvements make it a smart trade-off for those needing sustained power without excessive energy waste.
Moreover, the NVIDIA RTX 5070 Founders Edition features an optimized dual-fan setup that efficiently manages airflow and heat dissipation. With larger heatsinks and an improved vapor chamber, it’s clear that NVIDIA prioritized thermal stability. This means users can push the 5070 through extended AI training runs, high-resolution rendering, or other intensive workloads without worrying about performance dips due to thermal throttling.
For those who need reliable performance without overheating concerns, the 5070’s design feels like a well-engineered upgrade rather than a simple brute-force power bump.
As expected, the RTX 5070 is built for high-refresh-rate gaming and professional display setups. It supports 4K at 480Hz and 8K at 120Hz through its three DisplayPort 2.1b ports and one HDMI 2.1b port. This makes it a great choice for esports monitors with extreme refresh rates, high-end 4K gaming displays, and professional 8K monitors in video production, 3D rendering, and HDR content creation.
However, it’s worth noting that the HDMI Forum announced HDMI 2.2 at CES 2025, which dramatically boosts bandwidth to 96Gbps, allowing for 10K resolution and refresh rates up to 240Hz. If you’re looking to future-proof for that level of performance, you’ll likely need to wait for the next generation of RTX cards to take full advantage.
Now, let’s examine the performance results and see what the RTX 5070 offers. For RTX 4070 owners—or even those still holding onto a 30—or 20-Series GPU—the big question is whether its improvements in AI-driven graphics, ray tracing, and memory speed make upgrading worthwhile.
Here is the test platform we will be using for our RTX 5070 FE testing:
The AMD Ryzen Threadripper 7980X sits at the core of our test system, with 64 cores and extensive multi-threading capabilities. This ensures CPU limitations don’t interfere with GPU-focused benchmarks, especially in AI processing, ray tracing, and high-resolution rendering, where workloads are heavily offloaded to the GPU.
We paired the 7980X with the ASUS Pro WS TRX50-SAGE WIFI motherboard to deliver ample PCIe bandwidth, ensuring the GPU can achieve its full potential without bottlenecks. Our system also includes 128GB of DDR5 memory at 4800MT/s, providing enough headroom for smooth operation when handling large datasets. Although the Samsung 980 Pro is a slightly older Gen4 SSD, it still delivers fast read/write speeds, keeping load times minimal and preventing data-heavy tasks—like AI model inference or texture streaming—from being hindered by storage performance.
This setup should allow us to isolate and accurately measure the GeForce RTX 5070’s real-world performance, so let’s get right into it.
We will compare the RTX 5070 FE against the following GPUs:
First up in our gauntlet of tests is the Procyon AI Text Generation Benchmark. This benchmark simplifies AI LLM performance testing by offering a compact and consistent evaluation method, allowing for repeated testing across multiple LLM models while minimizing the complexity of large model sizes and variable factors. Developed with AI hardware leaders, this benchmark optimizes the use of local AI accelerators for more reliable and efficient performance assessments. The results measured below were tested using TensorRT.
Here, the RTX 5070 Founders Edition improves on the previous generation’s 4070 across every test. Whether it’s the Overall Scores, Output Time to First Token, or Tokens Per Second, the 5070 FE shows a jump in performance—generally in the neighborhood of 10–15%. That bump translates into faster AI outputs, shorter wait times, and smoother model inference. For instance, in the Phi benchmark, the 4070’s score of 3,191 increases to 3,453 on the 5070 FE, with Tokens Per Second climbing from 141.575 to 150.435. While not earth-shattering, that difference is enough to see a noticeable improvement in synthetic workloads and real-world usage.
Compared to the more powerful 5070 Ti, the 5070 FE expectedly trails by roughly 15–25%, depending on the specific test. For example, in the same Phi benchmark, the Ti version hits 4,179 on the Overall Score—a marked jump from the FE’s 3,453. This performance gap remains consistent across the Mistral and Llama series tests, showing that the Ti provides a more substantial margin for users who need heavier AI throughput. Meanwhile, the flagship 5080 sits at the top of the leaderboard, outpacing the 5070 Ti by an additional 5–10%. Its advantage is most evident in token generation speed and time to first token, where shaving off a fraction of a second can add up in high-volume workloads.
While the 5070 FE is no slouch for AI inference tasks, power users who want even faster generation times—or those handling vast datasets—might find that the 5070 Ti or 5080 better meets their needs in this performance area.
The Procyon AI Image Generation Benchmark consistently and accurately measures AI inference performance across various hardware, from low-power NPUs to high-end GPUs. It includes three tests: Stable Diffusion XL (FP16) for high-end GPUs, Stable Diffusion 1.5 (FP16) for moderately powerful GPUs, and Stable Diffusion 1.5 (INT8) for low-power devices. The benchmark uses the optimal inference engine for each system, ensuring fair and comparable results.
The RTX 5070 FE improves over the 4070 in every image-generation test—most notably in Stable Diffusion 1.5 (FP16), where it jumps from a 2,400 overall score on the 4070 to 2,937 on the 5070 FE. This gap translates into faster image production and shorter inference times across FP16 and INT8 benchmarks, making the 5070 FE a strong contender for users focused on AI-driven graphics workloads. Compared to the 5070 Ti and 5080, the 5070 FE lags roughly 20–25% in most tests, which may justify the Ti’s steeper price for users with incredibly demanding AI image-generation needs. Meanwhile, the 5080 pushes things further, topping the Ti by another 15–20%.
From a real-world standpoint, the RTX 5070 FE delivers a clear edge over the 4070, translating into noticeably faster inference times and smoother content creation workflows. However, if you’re regularly handling complex or large-scale image-generation tasks, the extra 20–25% performance boost offered by the 5070 Ti—or the additional 15–20% leap with the 5080—might be worth the investment.
Luxmark is a GPU benchmark that uses LuxRender, an open-source ray tracing renderer, to evaluate a system’s performance in handling highly detailed 3D scenes. This benchmark is pertinent for assessing the graphical rendering prowess of servers and workstations, especially for visual effects and architectural visualization applications, where accurate light simulation is critical.
In Luxmark, which gauges advanced ray-tracing performance, the RTX 5070 FE delivers around a 20% boost over the 4070 in the Food scene test (7,535 vs. 9,061) and roughly 10% in the Hall scene (20,003 vs. 22,062). Compared to the 5070 Ti, it lags by about 33% in Food (9,061 vs. 12,073) and 30% in Hall (22,062 vs. 28,635)—a notable gap that may justify the Ti’s higher price tag for users tackling large-scale 3D renders or VFX projects. The 5080 extends those margins even further, outperforming the 5070 FE by more than 50% in Food (9,061 vs.13,637) and around 40% in Hall (22,062 vs. 30,815).
In related workflows—such as architectural visualization or visual effects production—the 5070 FE still represents a solid overall jump from the 4070.
Geekbench 6 is a cross-platform benchmark that measures overall system performance. The Geekbench Browser allows you to compare any system to it.
In the results table below, the RTX 5070 FE puts up 188,892 points, a decent improvement from the 4070’s 174,725. An 8% jump in Geekbench 6’s GPU OpenCL score—from 174,725 to 188,892—may not sound huge at first glance. However, it can still offer a modest boost for those tackling resource-intensive tasks like high-resolution video editing, complex 3D rendering, or machine-learning inference. While it won’t dramatically impact simpler workflows, the extra headroom can be more noticeable if you frequently push your GPU to its limits.
The gap expectedly widens when compared to the higher-end models of the 50-Series. The 5070 Ti racks up 246,875 points, about 30% more power. At the top of the leaderboard, the 5080 pushes even further to 265,397, appealing to workstation pros and serious enthusiasts. Still, the 5070 FE is a sweet spot for users who want a noticeable upgrade in heavier GPU-based tasks without having to venture into more expensive territory.
The V-Ray Benchmark measures rendering performance for CPUs, NVIDIA GPUs, or both using advanced V-Ray 6 engines. It uses quick tests and a simple scoring system to let users evaluate and compare their systems’ rendering capabilities. It’s an essential tool for professionals seeking efficient performance insights.
Here, the RTX 5070 FE posts a score of 6,553, which is a significant 47% jump over the 4,469 scored by the RTX 4070. This increase will likely translate into a much smoother experience in rendering-intensive workloads, such as ray-traced scenes, architectural visualizations, or high-end CGI. For professionals or enthusiasts upgrading to the 5070 from a 4070, render times could see a noticeable drop, depending on the rest of their system setup.
Comparing the RTX 5070 to the higher-end models, the 5070 Ti shows a vpath score of 8,018, outperforming the 5070 FE by about 22%. The 5080, again, pushes even further to 9,311, offering 42% more performance than the 5070 FE. While the Ti and 5080 are better suited for heavy-duty rendering workloads, the 5070 FE still represents a substantial step up from the previous generation, making it a strong mid-range option.
3DMark Port Royal, Speed Way, and Steel Nomad are GPU benchmarks testing performance in different scenarios. Port Royal focuses on ray tracing, Speed Way evaluates performance in racing simulations, and Steel Nomad challenges GPUs with high-intensity, realistic graphics. They assess GPU capabilities in rendering, lighting, and dynamic scenes.
The NVIDIA RTX 5070 delivers up to 31% higher performance than the RTX 4070 across 3DMark benchmarks, offering a substantial value upgrade for its price point. The ASUS PRIME RTX 5070 Ti pushes performance even further, averaging 41% higher than the RTX 4070.
We’ve been using the Procyon AI Image test as a benchmark to measure the power consumption of all the new 50-series NVIDIA GPUs. Running the Stable Diffusion XL FP16 test, we look at the interval 2nd to last image being generated. We measure the time from the start to finish of that interval, peak power consumed, average high draw, and idle power after the test completes. Our testing showed an average consumption of 409W under sustained workloads, with peaks hitting 429W and idle draw at 184W. With the NVIDIA GeForce RTX 5070 having a listed power rating of 250W, the consumption was around 225W, with a peak of 245W. The card completed our 19.2-second test cycle with 2.46Wh of total energy use. This is a noticeable uptick over the 5070Ti and more than double what we measured from the GeForce RTX 5090.
The NVIDIA RTX 5070 FE hits a vital price point at $549, offering a blend of performance and next-gen features without veering into high-end pricing. For those coming from a 30- or 20-series GPU, this card represents a clear upgrade—higher frame rates, improved memory bandwidth, and AI-powered features like DLSS 4’s Multi Frame Generation contribute to a smoother, more responsive experience. If you’re on a 40-series GPU, the 5070 still delivers meaningful improvements, particularly in rendering and AI workloads.
In our testing, the RTX 5070 consistently outperformed the 4070 by 10–15% in AI-driven applications, with even bigger jumps in rendering and compute-heavy tasks. V-Ray, for example, saw a performance boost of over 40%, significantly reducing render times for complex ray-traced scenes. These gains highlight NVIDIA’s Blackwell advancements, including enhanced tensor and RT cores and the switch to GDDR7 memory, which allows for better handling of high-resolution textures and demanding creative workloads. While 12GB of VRAM leaves a bit to be desired, it’s more than enough for many gaming and AI tasks, especially smaller local models.
Ultimately, the RTX 5070 FE feels like a balanced option that makes next-gen features accessible at a reasonable price. NVIDIA has left room for those who need more power with the 5070 Ti and 5080, but for most users, the 5070 strikes an outstanding balance between cost and performance. The real test, however, will be availability. If NVIDIA keeps stock levels stable, this could finally be the midrange card that delivers both performance and accessibility without the supply headaches of previous generations.
Newsletter | YouTube | Podcast iTunes/Spotify | Instagram | Twitter | TikTok | RSS Feed
Lyle is a long-time staff writer for StorageReview, covering a broad set of end user and enterprise IT topics.
Products and solutions from our affiliate partners:
Subscribe to the StorageReview newsletter to stay up to date on the latest news and reviews. We promise no spam!
Copyright © 1998-2024 Flying Pig Ventures, LLC Cincinnati, Ohio. All rights reserved.
To provide the best experiences, we and our partners use technologies like cookies to store and/or access device information. Consenting to these technologies will allow us and our partners to process personal data such as browsing behavior or unique IDs on this site and show (non-) personalized ads. Not consenting or withdrawing consent, may adversely affect certain features and functions.
Click below to consent to the above or make granular choices. Your choices will be applied to this site only. You can change your settings at any time, including withdrawing your consent, by using the toggles on the Cookie Policy, or by clicking on the manage consent button at the bottom of the screen."
"NVIDIA","RTX 5070","NVIDIA is Selling Lies | RTX 5070 Founders Edition Review","https://gamersnexus.net/gpus/nvidia-selling-lies-rtx-5070-founders-edition-review-benchmarks","NVIDIA is Selling Lies | RTX 5070 Founders Edition Review & Benchmarks
NVIDIA is selling lies and reviewers shouldn’t be afraid to mince words. That is precisely what the RTX 5070 is. It was marketed on the back of lies considering NVIDIA stated that the card, which has 12GB of VRAM, would offer 4090, which has 24GB of VRAM, performance for $549 with the help of AI. Here’s an NVIDIA-compliant comparison of the RTX 5070 with MFG against the RTX 4090 without any frame generation at all.
Editor's note: This was originally published on March 4, 2025 as a video. This content has been adapted to written format for this article and is unchanged from the original publication.
These are really abusive settings, stressing both cards heavily. But NVIDIA said the 5070 would be equal to the 4090 (watch our review), and we’re not even close. The 5070’s spiky behavior is exceeding 200 ms in this like-for-like comparison except that it has MFG. The 4090 isn’t perfect, but WITHOUT any frame generation at all, it’s hitting at worst about 50-60 ms, with an average closer to 26. 
Here’s what that actually looks like. Using NVIDIA’s own FrameView tool, we also get a glimpse into another problem: Latency. The PC Latency on the 4090 was around 51 ms during a like-for-like comparison. 51 is a lot better than what we saw in the 5070 with MFG 4X after 5 minutes of being in the game. That was 500-720 ms. Now, to be fair, if you only played Cyberpunk with these settings for about 15-20 seconds at a time, things look a lot better… 
The 5070 is getting absolutely clobbered for VRAM, which is just 12GB to the 4090’s 24GB. To call these the same is an absolute, flat-out lie. These cards are not the same. In situations where the card runs out of VRAM in particular, it can never dream to be a 4090. It is simply impossible. 
Here’s a quick chart showing passes 1 through 3 of the 5070 as it gets progressively worse with each benchmark pass as the framebuffer fills. The 4090, meanwhile, maintains its performance the entire time.
NVIDIA has got to stop lying. This isn’t just marketing, this is an actual, verifiable lie. But let’s just pretend MFG 4X is somehow a fair comparison against the 4090 in a scenario that’s maybe not VRAM constrained. It’s an OK technology, but it is not a like-for-like comparison because the images themselves are not the same. When you turn it on, it’s like Clive here has 6 feet. 
NVIDIA could have launched its RTX 5070 to mediocre or lukewarm reviews. Instead, it decided to openly lie on stage and manipulate its performance numbers to mislead consumers into thinking an otherwise lukewarm card is equal to the flagship of last generation. NVIDIA, this was avoidable. You are making all of the unforced errors right now. This was a completely unnecessary fumble. We could have just listed all the numbers and called it a day, but now have to start by writing about NVIDIA’s lies and marketing.
The RTX 5070 is supposed to have an MSRP of $550. It’s hard to say what it’ll actually be for street pricing. But basing only off of MSRP, that makes this card $50 higher than the MSRP of the RTX 3070 at its launch in 2020 and $50 cheaper than the RTX 4070’s launch MSRP of $600. 
This looks like the patterned NVIDIA stutter-stepping of price slowly upwards, where they overshoot, then bring it back down. We saw this from 10 to 20 to 30 series.
The RTX 5070 GPU has an advertised 6144 CUDA cores, a 192-bit bus, and 12GB of GDDR7 memory. It might even have all of its ROPs. The 5070 Ti has an MSRP at $750 and carries 16 GB of GDDR7 memory on a 256-bit bus, so there’s more bandwidth, a little more capacity, and an increase to 8960 CUDA cores, and it may even have all of the ROPs that you’re paying for.
Clocks on the 5070 are advertised at 2.51 GHz boost.
A quick glance at pricing breaks it down like this: 
There are 5 MSRP models of the 5070 Ti (read our review) on Newegg, which is just enough to technically claim the MSRP exists. The rest of them are higher, often $900 and over. Board partners have now directly repriced their cards, with VideoCardz noticing that MSI had eliminated all of its $750 RTX 5070 Ti cards just before the 5070 launch. 
As we write this review, following the VideoCardz news post, MSI has now reintroduced two $750 models, dropping from $820 and $840 to $750. NVIDIA has historically applied pressure to partners to make MSRP units available, so it wouldn’t surprise us if the VideoCardz story triggered a reaction to keep two models available. They can also achieve this with rebates.
As for things available right now, we found the RX 7800 XT Pulse for $530 and... that’s it. When we sorted Newegg to $450 to $650 for “in stock” and “sold by Newegg,” meaning no third-party sellers and no unavailable models, all we got were a bunch of refurbished units and the 7800 XT Pulse.
We’re going to get into the benchmarks next. This review contains most of the games that we benchmark since we’re preparing for the 9070 and 9070 XT launch. We currently have around 50-60 GPUs tested in our lineup, so to fit the charts, we need to remove a few from the chart.
Final Fantasy 14: Dawntrail is up first for a traditional raster title at 4K, without any bulls*** to artificially inflate numbers.
This game is actually playable on most modern hardware at this resolution. The RTX 5070 ran at 78 FPS AVG, landing it right between the 4070 Ti (watch our review) and the 4070 Ti Super. Maybe we should call it the 4070 Ti V3 instead. The 5070 Ti outperforms the 5070 by 25% here, the 4070 Ti Super is ahead by 11%, and the 7900 XT by 6%.
Going by name only, the generational lead over the 4070 (watch our review) is 30%, or 48% over the 3070 (watch our review) and 118% over the 2070 (watch our review), followed by 244% over the 1070. 
Unfortunately, the 5070 is not better than the 4090 -- but if we were to be artificially intelligent and actually intellectually dishonest, it might be!
At 1440p, the RTX 5070 Ti’s lead over the RTX 5070 is reduced to 22% from 25% at 4K. The 5070 is also reduced in its advantage over the 4070 Ti, now just 1% from around 6% at 4K. From 150 FPS to 172 FPS, we now have 3 GPUs from NVIDIA. If you want to make NVIDIA look as ridiculous as possible, you could maximize the size of the clown car by drawing a box from the 4080 Super at 202 FPS to the 4070 FE at 117 FPS. The result is 7 modern NVIDIA GPUs spanning an 85 FPS range. Divided evenly, that would be one GPU for every 12 FPS. In this situation though, 4 of them land within a 36 FPS range: The 5070 Ti, 4070 Ti Super (read our review), 5070, and 4070 Ti mean you can dial it into the individual decimals if you were so picky about your performance.
The 7900 XT (read our revisit) lands in the middle of all of these and has a 12% lead over the RTX 5070, assuming all ROPs are present on the 5070.
The 5070 is, again, not better than the 4090. 
Remarkably, we still have OK scaling at 1080p for most of these GPUs. The fact that the 9800X3D allows the 5090 (read our review) and 4090 to still have a slight gap at 1080p really speaks to how exciting AMD’s 9800X3D is.
Anyway, the 5070 ran this workload at 225 FPS AVG and fell below the 4070 Ti. No wonder NVIDIA wants everyone using MFG. The 4090, predictably, outperforms the 5070 massively. It’s a 67% gap here.
The 7900 XT now has a reduced lead of 9%.
Black Myth: Wukong at 4K and rasterized had the 5070 at 40 FPS AVG, giving the 5070 Ti a 27% lead. The 5070 really struggled in this one and ended up tied with the 4070 Ti and, to its benefit, the 7900 XT. We’ll be curious to see how the 9070 does in this one… Surely we don’t already know as we’re writing this and definitely haven’t looked at the results and in no way would we ever encourage you to just wait and see what the results are…
If we were to randomly multiply the frames, it’d be better than the 4090 -- until you randomly multiplied the 4090 with lossless scaling. Jensen’s claim is like a schoolyard argument: The 5070 is infinity times better.
At 1440p, we re-introduce other 70-class cards of the past. The 5070 FE ran at 72 FPS AVG, with frametime pacing unremarkable. The 5070 is tied with the 4070 Ti and 7900 XT. Generationally by name only, the 5070 improves on the 4070 by 22%, the 3070 by 58%, and the 2070 non-Super by 137%.
1080p has the RTX 5070 down at 98 FPS AVG, allowing the 4070 Ti to gain a slight 2% lead. The 5070 Ti is ahead by 17.6% here, assuming all ROPs are present. 
Against the prior cards containing 70 naming, the 5070 is ahead of the 4070 by 18%, the 3070’s 63 FPS AVG by 56%, and the 2070’s 43 FPS by 126%.
Starfield at 4K is up now. The RTX 5070 ran at 54 FPS AVG here, giving the 4070 Ti’s 59 FPS a 9% lead. It’d be interesting if AMD’s 9070 performed similarly to the 7900 XT here since the Hellhound is 20% ahead of the 5070. We’ll find out tomorrow.
The 5070 Ti has a larger lead over the 5070 in this test than the last 1080p benchmark, running a 68 FPS AVG for a 27% advantage over the 5070 FE. Performance of the 5070 over the 2070’s 22 FPS is about 145% ahead.
1440p significantly reduces the 5070 Ti’s uplift over the 5070, bringing it down to 22% from the prior 27%. The 7900 XT, which is an important comparison in this test for... reasons, runs at 98 FPS AVG with comparable lows to its neighbors. That has it 18% ahead of the RTX 5070’s 83 FPS AVG. 
As for the RTX 4070, the 5070 leads by a paltry 10.6%, then 48% over the 3070’s 56 FPS.
At 1080p, the 5070’s 104 FPS AVG planted it between the 4070 and 4070 Ti again, with an unimpressive 9% improvement on the 4070. The gap shrinks as the resolution decreases in this game. The 5070 Ti is now ahead by only 19%, down from 27% at 4K. As for AMD, its RX 7900 XT is 15% ahead here, down slightly from the 1440p advantage.
Dragon’s Dogma 2 at 4K is next. This game is relatively heavy on the GPU in our test area, but is unique for its ability to also produce a heavy CPU load in cities.
The RTX 5070 ran at 56 FPS AVG here and had good frametime pacing represented in the lows, but then again, so did everything around it. It’s proportional.
The 5070 very slightly leads the 4070 Ti. 
As for the 4090: We’re still somehow not matching the RTX 4090, which is up at 98 FPS. We must have mistyped the =5070*4 formula that Jensen prescribed reviewers. 
The 5070 Ti leads the 5070 by a much larger 31% in this benchmark, producing one of the most notable jumps yet. The 3090 Ti ran at 64 FPS for a 14% lead over the 5070, which might seem totally non-sequitur and unrelated, but it isn’t.
Anyway, the 7900 XT held a 61 FPS AVG in this one, with the XTX just past the 5070 Ti. 
At 1440p, the RTX 5070 basically ties the RTX 4070 Ti again. The 7900 XT’s 104 FPS AVG is just a 9% lead here, with the 4070 Ti Super a slight step above that. The 5070 Ti has a 25% lead over the 5070 here, posting one of its better 1440p comparative results.
Somehow, and we don’t know how this happened, the RTX 4090 just seems to be better than the RTX 5070. That’s just so weird. Maybe we should run NVIDIA Multi-Fraction Generation to divide the 4090 down to a 5070.
At 1080p, the 5070’s 126 FPS AVG has it just behind the 4070 Ti. The 7900 XT leads by only 7% here, with the 5070 Ti knocked down to a 21% lead from 25% at 1440p. The 5070 Ti’s 151 FPS AVG is also about the same as the 7900 XTX, which has its own slight advantage.
As for the 4090, well, ours really must be broken since it’s not making the 5070 look good enough.
Cyberpunk is up now. We have a more limited data set with this since we updated the game to v2.21 and have been working on re-running everything.
At 4K, the 5070 ran at 41 FPS AVG. That put it 6% ahead of the 4070 Ti and 144% ahead of the 2070, or 68% ahead of the 24 FPS result of the 3070. As for the 5070 Ti, its 50 FPS AVG gives it a 22% lead here, with the 7900 XTX (watch our review) ahead of that at 57 FPS AVG. The 7900 XT has a noteworthy 13% lead against the 5070 here.
At 1440p, the 5070 ran at 89 FPS AVG and had lows at 75 FPS 1% and 71 FPS 0.1%. The 5070 Ti at 108 FPS is about 21% ahead, not changing much from 4K. The 7900 XT is ahead of the 5070 in AVG, 1%, and 0.1%, holding a 12% lead in average framerate.
Once again, the 4070 Ti basically ties the 5070, with the latter slightly ahead this time.
1080p reintroduces other 70-class cards: The 5070 Ti ran at 167 FPS AVG, with the 5070 at 138. That’s still 21%. The 4070 hasn’t been rerun here yet, but the 3070 is present at 85 FPS, yielding a 62% lead to the 5070, followed by the 2070 at 60 FPS for 130%, then the GTX 1070 at 35 FPS AVG. That’s about a 293% improvement to the 5070.
AMD’s RX 7900 XT seems like it’ll be particularly relevant soon. This one ran at 150 FPS AVG, with lows expected at 116 and 104. That’s an 8.8% improvement over the 5070.
Dying Light 2 is up now. This is another of the heavier games, especially with RT later.
At 4K, the 5070 ran at 56 FPS AVG and struggled in this title, allowing the 5070 Ti an advantage of 25%. The 7900 XT doesn’t look great by comparison here, at 55 FPS AVG itself.
As for the 4070 Ti, it’s again roughly equal. 
1440p has the 5070 at 106 FPS AVG, now trading places with the 7900 XT. The 5070 Ti’s lead is reduced to 22%, with the 4070 Ti now falling slightly behind the 5070.
By generational naming, the 4070 ran at 78 FPS AVG (so the 5070 is 36% better) and the 3070 was at 67 FPS AVG (or 60% better on the 5070).
Resident Evil 4 is up last for raster testing. We’re almost through these.
At 4K, the 5070’s 78 FPS AVG has it just behind the 4070 Ti that we’ve been tracking. It’s also about 6 FPS ahead of the 7800 XT, so measurably different but functionally equal. The 5070 Ti’s 107 FPS AVG positions it 36% ahead in this one, which is a huge gain and among the largest we’ve seen. The 7900 XT seems like a good AMD comparison, landing at 100 FPS AVG and leading the 5070 by 28%.
At 1440p, the 5070 held a 152 FPS AVG and trailed the 4070 Ti by 8 FPS. The 5070 Ti is 30% higher framerate here, down from 36% at 4K. As we’ve seen, the gap tends to close at lower resolutions, although it’s still a huge gap in this game.
The 7900 XT is now up at 186 FPS AVG, with a reduction in the percentage advantage to 22% from 28% at 4K.
Generationally, the 5070 runs 23% faster than the 124 FPS on the 4070, 54% faster than the 91 FPS on the 3070, and 157% ahead of the 59 FPS for the 2070.
Finally for raster, we’re now at 1080p for Resident Evil 4. This one is interesting for the further closing of the gap between the 5070 and 5070 Ti, which now ranges from 282 FPS to 224 FPS for a 26% improvement on the Ti. The 7900 XT is about 18% higher framerate here.
We’re moving on to ray tracing now. NVIDIA has historically held significant advantages in some games for ray tracing.
AMD says it has significantly improved its ray tracing performance. We explained why the company is claiming this in our news piece covering the 9070 announcements, so this will become highly relevant in tomorrow’s reviews.
Black Myth: Wukong is up first. This is one of the two titles in this RT test suite that heavily favors NVIDIA. We’re testing with upscaling here.
At 4K, the RTX 5070 ran at 40 FPS AVG. That has the 5070 Ti at 30% ahead, with the 4070 Ti about tied with the 5070. AMD doesn’t appear until the 7900 XTX, down at 20 FPS AVG. That’s a massive lead of 99% over the 7900 XTX. The 5070 doubles the 7900 XTX’s performance.
That’s not good for AMD’s last generation. We’ll see if that lead can be halved with the new generation. The 5070 leads the RTX 3080 by 44%. Based on math from AMD’s claims in its presentation, that’s about where it should land here. Too bad no one knows if that’s a good reference point yet.
At 1440p, the 5070 Ti ran at 88 FPS AVG, the 5070 at 73 FPS AVG (between the 4070 Ti and 4070 Ti Super), and the original RTX 70-class card ran at 24 FPS AVG. AMD’s best here, as of today and not tomorrow, is the 7900 XTX at 37 FPS AVG.
Now for 1080p. The 5070 held a 97 FPS AVG here, encroaching on the 4090 but still not beating it. The 5070 Ti leads the 5070 by just 15% in this situation, with the 5070 now ahead of the 4070 Ti and Ti Super cards after slow gains in the other resolutions. The 7900 XTX ran at 49 FPS AVG, closer to a 4060 and behind the 3070. AMD named the 3080 in its slideshow announcing the card. If it lands near that mark, it’d be around or ahead of the 66 FPS AVG result for the FTW3.
Speaking of: EVGA really knew what was coming when it left the GPU market.
Dragon’s Dogma 2 is back with RT now. This one is more balanced between the vendors.
At 4K, the 5070 ran at 49 FPS AVG, establishing a 9% lead for the 7900 XT. That’s a similar gap to what we saw without RT. The 5070 Ti leads by 30% again here, with the 7900 XTX leading that. 
At 1440p with RT, Dragon’s Dogma 2 puts the 5070 at 83 FPS AVG, reducing the 5070 Ti’s lead to 24%. The 7900 XT sits between both, with the 7900 XTX ahead of the 5070 Ti.
Based on the charts AMD has released, the math would position the 9070 and 9070 XT as flanking the 5070 Ti, but we’ll find out soon enough.
Generationally, the 5070 leads the 4070 by 23%, the 3070’s 51 FPS by 62%, and the 2070 non-Super by 145%.
Down to 1080p, the 5070 Ti’s lead over the 5070 is now 22%, with the 7900 XTX still leading the Ti and 7900 XT still leading the 5070. The 4070 is relatively close to the 5070 here, now with an 18% advantage to the 5070. 
But maybe the 4090 can breathe some excitement into it. The 4090 ran at 169.2 FPS AVG. If we multiply the 5070 by a billion, that’d put it at 107.4 billion FPS AVG, which is an uplift of 6,347,516.73%. This is clearly the card to get. And as we all know, it’s not possible to multiply the 4090 by arbitrary numbers with lossless scaling because then that would hurt 50-series marketing.
Dying Light 2 with RT is next. This first one is 4K upscaled. The RTX 5070 ran at 44 FPS AVG here, just below the 4070 Ti. The 7900 XTX held 46 FPS AVG in this one, again showing AMD’s prior deficit in RT performance. It’s not as bad as in Black Myth, but considering the original pricing of the 4070 Ti and 7900 XTX, AMD was in a position that was hard to fight from. We’ll see how the 9070 series compares tomorrow.
At 1440p, the 5070 beats the 7900 XT by 13%, falls behind the 4070 Ti, and allows the 5070 Ti, which is basically an RTX 4080 v3 or v4, a lead of 27%.
At 1080p upscaled, the 5070 produced 116 FPS AVG and again sat just below the 4070 Ti, with the 7900 XTX and 7900 XT flanking the 5070. The 5070 Ti ran at 141 FPS AVG, which will be the number for AMD’s 9070 XT to target.
Generationally, the 5070 leads the 4070 by 25% and 3070 by 58%.
Resident Evil at 4K upscaled is next. This one has the 5070 at 91 FPS AVG, roughly tying the 4070 Ti once again. The 7900 XT leads the 5070 by 18%, with the 5070 Ti leading by 29%. The 7900 XTX sits ahead of that at 134 FPS AVG.
At 1440p, the 5070’s 149 FPS AVG put it 23% ahead of the 4070 and 70% ahead of the 3070. The 7900 XT leads the 4070 Ti and 5070.
Finally for RT, our new Cyberpunk results with the updated game version.
First, with 4K and RT Ultra, the 5070 ran at 17 FPS AVG. This is without upscaling and is intentionally the heaviest workload we run.
That positions it right between the 7900 XT and 7900 XTX. That’d be unfortunate positioning if AMD weren’t launching something with better RT in a day, but we’ll have to check in tomorrow for the rank.
The 5070 Ti continues its trend of being NVIDIA’s third or fourth iteration of a 4080 card and leads the 5070 by a huge 56% here. The workload is just too heavy for the 5070 to handle and it has neither the bandwidth or compute capability.
Here’s RT Ultra at 1080p. The 5070 held 64 FPS AVG with these settings, putting the 5070 Ti about 32% ahead. The 7900 XTX trails the 5070 here, unfortunately for AMD’s former flagship.
We also run RT Medium for Cyberpunk. We’ve found that RT Ultra and RT Medium can significantly affect the hierarchical ranking of NVIDIA and AMD, so we run both. NVIDIA runs away at Ultra.
With these settings and at 4K still, the 5070 now runs at 22.5 FPS AVG. This reduces the 5070 Ti’s lead to a more normal 35%. The 5070 just didn’t have the ability to keep up at 4K/RT Ultra.
We’ll keep thermals short and simple. Using our usual benchmark of Port Royal at 4K and looping, we measured the 5070 FE at steady state at around 74 degrees Celsius for GPU core temperature. The memory temperature was about 76 degrees. Both of these numbers are acceptable; memory is well within spec and is completely fine. Core has some room for a hotter computer case. The core temperature isn’t impressive, but is acceptable, and there’s a little bit of buffer there.
The 5070 FE’s fans ran at about 2500 RPM to maintain this temperature. We skipped acoustic testing this time since we have the two 9070 reviews we have to get through. 
For efficiency as usual, we use a power interposer in between the GPU and the power supply. That means we’re intercepting slot power and the PCIe power through the cables and we do that so we can isolate the GPU entirely, measure its power consumption during a workload. Then we take the frame rate numbers to do some simple math and produce an efficiency number. 
In these charts, you’ll get a few things. You’ll get the total power consumption for that workload in watts and you also get its efficiency in FPS per watt not workload-normalized so we allow it to run at the frame rate that it can naturally run at. 
For efficiency with Final Fantasy 14 at 4K, we ended up with this data. This chart isn’t as dense as our 1440p chart that’s up next.
The 5070 FE ran at 0.33 FPS/W here, pulling 233W during the workload. The 5070 Ti pulled 264W, allowing it an improvement in efficiency to 0.37 FPS/W. AMD’s prior RX 7900 XT wasn’t particularly efficient, giving NVIDIA a large advantage in efficiency for the 5070. The FPS was close enough to be mostly observably equal to a player, but the 7900 XT ran at just 0.25 FPS/W from its 324W power draw during the test.
We’ll have to see what the 9070 series does to improve this, as this has been one of AMD’s GPU weaknesses over the years.
At 1440p, the 5070 ran at 0.65 FPS/W. That puts the 5070 Ti as about 12% more efficient than the 5070 when producing a variable workload. The AMD 7900 XT pulled 325W in this test, landing at 0.53 FPS/W. Its framerate is higher, but the power is also disproportionately higher, which hurts its efficiency despite a higher framerate.
At 1080p, the 5070 hits nearly 1 FPS/W. The 4060 Ti has passed it for efficiency, with the 5070 Ti improved by 0.11 FPS/W on top of the 5070’s 1.0 result. The 7900 XT is down at 0.75 FPS/W and is still pulling about the same power as previously.
F1 24 at 4K and with ray tracing is up next. In this one, the 5070 produced 0.17 FPS/W, pulling close to TDP at 246W in order to produce its hardly playable framerate. The 5070 Ti ran at 0.20 FPS/W, with the 4080 ranking at the top for its balance of power and framerate. Note that bar size changes from numbers that look the same are valid -- it’s just from the hidden decimal places.
The 7900 XT ran at 0.12 FPS/W, a significant fall from the 5070’s result. The 9070 series has a lot of work to do here.
At 1080p and still with RT, the 5070 ran at 0.55 FPS/W, again giving the 5070 Ti a 13% efficiency advantage. We’re curious to see where the 9070 and 9070 XT land. The 7900 XT isn’t competitive in efficiency, with a lot of this particular result being because of its relatively low RT performance last generation. That’s what AMD is trying to tackle.
In Black Myth without RT and at 1080p, the 5070 Ti held a 0.53 FPS/W rank, putting it in the second slot. The 5070 is at 0.48, with the 4060 Ti still trading back-and-forth depending on the test. The 7800 XT was at 0.31 FPS/W, with the 7900 XT just below that. The 5070 is not NVIDIA’s most power efficient card due to the performance trade-offs, but is relatively efficient overall.
In Dragon’s Dogma 2 at 1440p and with RT, the RTX 5070 ranked at 0.36 FPS/W, sandwiching it between the 4090 that it’s not better than and the 5090. The 7900 XT is down at 0.28 FPS/W, yielding an efficiency benefit in a non-normalized framerate workload of 29%. That’ll be the mark for the 9070 series to hit.
Finally, in Starfield rasterized at 1440p, the 5070 ran at 0.43 FPS/W and landed just below the 4090, which it remains not better than, again. The 7800 XT (read our review) from AMD ran at 0.32 FPS/W, so AMD has a lot of ground to gain here tomorrow. This will be an area we’ll focus to test for improvements, as theoretically, it should be better than it was.
One thing’s for certain: NVIDIA’s marketing about the 5070 being a 4090 was wrapped entirely in bull**** and built on a foundation of manure, which is only fitting for a company whose CEO is never found without a leather jacket.
The GPU market is insane right now. The fact that we sorted Newegg for $450 to $650 GPUs and got basically one is insane. The current buying experience matches that of the COVID-era boom around late 2020 and the 2017 crypto mining boom. That makes it hard to evaluate value, and broadly speaking, we’d say to just wait for things to cool off if your current machine can last you a bit longer. You’ll likely save money as pricing settles. If that doesn’t matter to you, maybe time will -- and unless you get lucky and snipe an early stock of the cards, you may at least save some time.
As for its value, we’re going to kick the can to tomorrow’s reviews of the 9070 XT and 9070. We don’t think you should buy this card until you learn about AMD’s competition, as the on-paper price is in similar territory. We’ll see how the in-reality price is for both of them.Check back for the 9070 XT and 9070 reviews.
 Copyright © 2023 GamersNexus, LLC. All rights reserved.GamersNexus.net is Owned, Operated, & Maintained by GamersNexus, LLC. 
Copyright © 2023 GamersNexus, LLC. All rights reserved.GamersNexus.net is Owned, Operated, & Maintained by GamersNexus, LLC."
"NVIDIA","RTX 5070","Nvidia GeForce RTX 5070 review: $549 price and performance look familiar","https://www.tomshardware.com/pc-components/gpus/nvidia-geforce-rtx-5070-review-founders-edition","When you purchase through links on our site, we may earn an affiliate commission. Here’s how it works.

The Nvidia GeForce RTX 5070 Founders Edition looks good on paper, with 20% higher performance than its predecessor and a $549 MSRP. It's a good upgrade, but we can't help but worry retail availability will prove to be just as limited as with other Blackwell GPU launches, meaning the price is likely to climb significantly higher for at least a few months (and probably longer).
DLSS 4 Transformers, MFG, and other AI features
Serious concerns with retail pricing and availability
RX 9070 lands tomorrow with 'better' specs

Why you can trust Tom's Hardware




Our expert reviewers spend hours testing and comparing products and services so you can choose the best for you. Find out more about how we test.

The Nvidia GeForce RTX 5070 Founders Edition has a big hole to fill in the graphics card market. As the first true mainstream offering for the Blackwell RTX 50-series GPUs, it takes over from the discounted RTX 4070 Founders Edition with the same nominal $549 base MSRP. It also has the same 12GB of VRAM and nearly the same number of streaming multiprocessors (SMs) — 48 versus 46 — but with the new Blackwell features. On paper, getting a faster GPU for less money with new features should make this one of the best graphics cards, but we have some concerns.

The biggest problem will no doubt be retail availability and pricing, and we've seen every GPU launch of the past few months sell out almost instantly. From Intel's $249 Arc B580 to the $1,999 RTX 5090, with the RTX 5080 and RTX 5070 Ti filling in the middle, MSRPs have been effectively non-existent. We don't expect the 5070 to buck that trend, and it's all starting to feel a lot like 2021 — just with AI-induced GPU shortages rather than cryptocurrency mining shortages. When will it end? That's a difficult question to answer.

Nvidia posted record earning of $130 billion for the 2025 fiscal year that just ended, more than double its 2024 earnings. Nearly all of the gains came from its AI and data center business, which accounted for 88% of gross revenue. Gaming was a very distant second place at just 8.7% of the total revenue. Nvidia has been saying it's no longer primarily a gaming company for a while now, and nowhere is that more apparent than in the financials.

With massive demand coming from the AI sector, and with limited 5nm-class wafers from TSMC, the simple economics show that it's far more profitable to make data center and AI products right now rather than consumer GPUs. It's not that Nvidia won't order any consumer GPUs, but it's unlikely to be anywhere near sufficient to meet the demand. And in fact, right now virtually every graphics card of the past two years is either sold out or severely overpriced relative to the launch MSRP — with the only exceptions being the RTX 4060, AMD's RX 7600 (the RX 7600 XT currently starts at $430, $100 more than its original MSRP), and Intel's Arc B570.

The prospects for reasonably priced GPUs look grim, in other words. It could be many months before anything gets close to MSRP — and that goes for AMD's RX 9070 XT and RX 9070 that are slated for review tomorrow. We expect those to be just as hard to acquire at MSRP as the RTX 5070, which will officially go on sale tomorrow. But maybe our pessimism will prove misplaced! For now, all we can do is look at the performance and features on tap, and hope that supply will catch up to demand sooner rather than later.
We've written a lot of supplemental coverage about Nvidia's new Blackwell RTX 50-series GPUs. If you want a primer, or additional information, check out these articles:

• Blackwell architecture
• DLSS 4, MFG, and full RT testing
• Neural rendering and DLSS 4
• RTX 50-series Founders Edition cards
• RTX AI PCs and generative AI for games
• Blackwell for professionals and creators
• Blackwell benchmarking 101
We've been kept busy during the past two months testing and retesting graphics cards. The fourth Nvidia GPU launch of the year and sixth new graphics card since December hasn't given us time to catch our collective breath, never mind getting all the other prior generation GPUs we'd like to test filed through our new test suite.

Last month we also took a closer look at DLSS 4 and MFG, using the 5080 and 5090, which will have to suffice for now — time constraints didn't allow us to cover the same tests on the RTX 5070 Ti or the 5070, or the 9070 XT and 9070 for that matter. But we'll get around to those hopefully by next week and update the appropriate review pages.

Until then, the TLDR remains the same: MFG is a great way to inflate benchmark scores, and in the right scenarios it can feel better than framegen or non-framegen even if it has slightly higher input latencies. But the benchmark numbers tend to be much higher compared to how games actually feel. It's not bad as such, but subjectively MFG4X might feel more like 30~40 percent faster than the non-MFG performance, rather than the 200% improvement benchmarks can show. It will look smoother even while typically delivering the same or lower levels of responsiveness.

For additional information about Nvidia's Blackwell RTX GPUs, check the links in the boxout. The RTX 5070 Founders Edition represents the reference clocks and design from Nvidia, which will likely be just as fast as most of the non-reference card models from AIB partners. It might also be slightly more affordable, assuming you can find any in stock. But as usual, let's start with the specs table to see how it compares to the prior generation.
The paper specifications don't necessarily tell the full story. For example, the Blackwell architecture doubles the ray/triangle intersections per clock for the RT cores, the tensor cores support new number formats like FP4, and the CUDA cores all support FP32 and INT32 operations (only half of the CUDA cores in the RTX 40- and 30-series GPUs supported INT32 operations). That leads to what might appear at first to be little to no change in performance potential.

RTX 5070 has peak theoretical FP32 compute of 30.9 TeraFLOPS, compared to 29.1 TeraFLOPS on the RTX 4070 — a mere 6.2% increase. TGP (Total Graphics Power) has increased from 200W to 250W, however, along with memory getting a sizeable 33% bump in bandwidth thanks to the move to GDDR7 memory. So in theory, the 5070 should be somewhere between 6% and 33% faster than its direct predecessor for 'normal' workloads (i.e. things that don't leverage the FP4 support or MFG). In practice, the gains are on the higher end of that range for most games.

Die size and transistor counts are interesting as well, mostly because the previous generation AD104 GPU was used in the RTX 4070 Ti and had up to 60 SMs available, even though only 46 were enabled in the 4070. The GB205 die only has up to 50 SMs, however, with 48 enabled in the RTX 5070. That's what makes the new chip smaller and also gives it fewer transistors — both chips are made on the same TSMC 4N node.

AI compute does potentially favor the RTX 5070 a lot, but only if we include FP4 support. It has up to 988 TFLOPS of FP4 compute (which Nvidia classifies as ""TOPS"" even though that's normally only used for integer calculations), more than double the 4070's 466 TFLOPS of FP8. But for FP8 compute, it's the same 6.2% difference as the graphics FP32 compute. Clock speeds on paper are only slightly higher with the 5070 compared to the 4070, but we'll need to look at real-world clocks as Blackwell and Ada GPUs tend to run at much higher clocks than the stated boost clocks.

The RTX 5070 offers a much bigger improvement over the older RTX 3070, naturally, with about 50% more theoretical compute and up to 6X more AI compute (comparing FP4 to FP16, with sparsity in both cases). But AMD's upcoming RX 9070, which we'll review tomorrow, looks set to deliver some serious competition. Check back in 24 hours and we'll have the full review for AMD's 9070 and 9070 XT.
Again, before we even get to the benchmarks, there are a couple of elephants in the corner.

First is retail availability and pricing. We have every reason to expect the RTX 5070 cards will sell out quickly tomorrow when they go on sale, and that many models will end up at significantly higher prices than the ostensible $549 MSRP. After all, the cheapest graphics cards are pretty much stupidly expensive — and that goes for used cards on places like eBay as well, where the RTX 4070 price in the past 30 days has averaged over $650. Will a card that's newer, faster, and has more features cost less than the previous generation? Not a chance.

The other item to remember is the impending AMD Radeon RX 9070 and RX 9070 XT launch, which will be one day after the RTX 5070 — meaning, MSRP-priced reviews go up tomorrow, and the cards go on sale starting March 6. The RX 9070 competes directly with the RTX 5070 on price, or at least MSRP. Traditionally, AMD GPUs also don't command quite as much demand as Nvidia GPUs. But the RX 9070 XT for $50 more looks like it will potentially compete with the RTX 5070 Ti, or alternatively it should easily beat the RTX 5070 for a relatively minor price increase.

But AMD GPU availability right now isn't any better than Nvidia GPUs. Everything from the RX 7600 XT and above is horribly overpriced, and the previous generation RX 7900 GRE that was intended to compete with the RTX 5070 at the $549 price point now sells for over $900, with the average eBay price for used GPUs over the past 30 days sitting at $711. Newer, faster, and better RX 9070-class GPUs will inevitably sell out and end up going for much more than $549 or $599.

Current page:

Introducing the Nvidia GeForce RTX 5070 Founders Edition


Jarred Walton is a senior editor at Tom's Hardware focusing on everything GPU. He has been working as a tech journalist since 2004, writing for AnandTech, Maximum PC, and PC Gamer. From the first S3 Virge '3D decelerators' to today's GPUs, Jarred keeps up with all the latest graphics trends and is the one to ask about game performance.

Tom's Hardware is part of Future US Inc, an international media group and leading digital publisher. Visit our corporate site.

©
Future US, Inc. Full 7th Floor, 130 West 42nd Street,
New York,
NY 10036."
"NVIDIA","RTX 5070","NVIDIA GeForce RTX 5070 Founders Edition Review","https://www.techpowerup.com/review/nvidia-geforce-rtx-5070-founders-edition/",""
"NVIDIA","RTX 5070","Nvidia GeForce RTX 5070 Review","https://www.techspot.com/review/2960-nvidia-geforce-rtx-5070/","The RTX 5070 is the $550 GeForce 50 series GPU that Jensen confidently told gamers during CES that would deliver performance equal to the RTX 4090... the previous generation's $1,600 flagship product.
After the bold announcement, user forums were flooded with gamers celebrating this exciting new generation – 4090 performance for $550? Yeah, that does sound pretty amazing.
Of course, the excitement was quickly squashed after it became clear that Nvidia was using multi-frame generation to make these extremely misleading and, frankly, false marketing claims. We were quick to point this out in our CES coverage, but many still came away expecting great things from the RTX 5070 from the Nvidia announcement.
Now, we finally have it, and it is unbelievably underwhelming.
The RTX 5070 is so boring, disappointing, and ultimately pointless that we are going to shorten our review.
While we conducted extensive testing, we won't go over all the individual game data because it's highly repetitive and none of it is exciting.
But first, let's address Nvidia's blatant lie – the one we just mentioned – where they proudly claimed the RTX 5070 would deliver 4090 performance. Obviously, this is a massive falsehood, and it's a damaging one that undermines features like frame generation by weaponizing them to mislead gamers.
In simple terms, Multi-Frame Generation is just a more advanced version of Single-Frame Generation. Instead of generating one frame, it can generate up to three, increasing smoothness – provided your monitor has a high enough refresh rate to display these frames. What it doesn't do is increase performance.
In simple terms, Multi-Frame Generation is a more advanced version of Single-Frame Generation – it can increase smoothness – what it doesn't do is increase performance.
Unlike rendering normal frames, generating frames doesn't lower latency. In fact, if anything, the overhead of generating frames makes latency worse. No reduction in latency means no performance increase. The game won't feel or play any faster, but it will look smoother – with some artifacts thrown in. That's just frame smoothing, not a genuine performance boost.
So, again – generating frames doesn't boost performance. No matter how many frames are generated, the RTX 5070 cannot outperform the RTX 4090. These comparisons must be made before enabling frame generation, so let's quickly do that now.
In reality, the RTX 4090 is, on average, 63% faster than the RTX 5070 across our 16-game sample at 1440p. But the deception gets even worse when we turn to ray tracing, as there are cases where the RTX 5070 doesn't work at all due to its much more limited 12GB VRAM buffer.
One such example is Indiana Jones and the Great Circle, a game Tim strongly recommended we add to our ray tracing tests because of its excellent use of path tracing, resulting in a truly transformative experience. So, we included it in this latest round of GPU testing. The only problem – at least for the RTX 5070 – is that it can't actually run the game under these conditions, rendering just 13 FPS on average, while the older RTX 4070 Ti Super is good for 47 fps.
This is because the RTX 5070 – a $550 GPU being released in 2025 – only comes with a measly 12GB VRAM buffer. By now, we would have hoped this kind of memory capacity was reserved for entry-level products, and that an RTX 5070 would include at least 16GB, but that's not the case.
As a result, in this example, the RTX 4090 is 462% faster, and even frame generation can't salvage a win for the RTX 5070. Also, keep in mind that this isn't even native 1440p – we are enabling DLSS Quality upscaling to try and boost performance.
You can't even load the game at 4K using these quality settings – it immediately crashes to the desktop.
Whatever opinions exist about the RTX 5090, 5080, and 5070 Ti, we can confidently say all three of those products are much better than the RTX 5070.
Things are already looking pretty bad for the RTX 5070, and while one could argue that the GeForce 50 series has been a flop so far, whatever opinions exist about the RTX 5090, 5080, and 5070 Ti, we can confidently say all three of those products are much better than the RTX 5070. For those who aren't up to speed on what the RTX 5070 actually is, let's quickly go over the specs.
For $550, you get a Blackwell GPU with 6,144 CUDA cores, 192 texture mapping units, and probably 80 ROPs. That's 31% fewer cores than the 5070 Ti and just 4% more than the original RTX 4080 released two years ago.
Interestingly, it also has 14% fewer cores than the updated RTX 4070 Super, with a core clock speed that is largely the same. However, making up for the reduced core count is a 33% increase in memory bandwidth. The RTX 5070 uses 28 Gbps GDDR7 memory on a 192-bit wide memory bus, providing 672 GB/s of bandwidth – 14% fewer cores than the 4070 Super but 33% more bandwidth, and an 8% discount at MSRP.
However, as we've already pointed out, one aspect that remains unchanged is memory capacity. Like the original RTX 4070 and the updated 4070 Super, the RTX 5070 also only features 12GB of VRAM. While the VRAM is faster, the capacity remains the same – and no, speed or bandwidth cannot compensate for capacity. It just doesn't work that way.
At this point, the RTX 5070 is looking like little more than a slightly discounted 4070 Super with $50 knocked off the MSRP. Of course, we have tested a variety of games to confirm this, so let's take a look at some of them and then go over the full performance breakdown with graphs.
Starting with Marvel Rivals at 1440p, we see that the RTX 5070 is only 3% faster than the 4070 Super and just 15% faster than the two-year-old RTX 4070. We haven't had a chance to update our 7900 GRE data, but compared to the 7800 XT, the 5070 is 16% faster and 6% slower than the 7900 XT.
At 4K, the 5070 and 4070 Super are neck and neck, delivering virtually identical performance, averaging just 44 FPS.
Next, we have Stalker 2, where the 5070 actually performs worse than the 4070 Super, coming in 9% slower at 1440p. This meant it was just 11% faster than the 7800 XT and 12% slower than the 7900 XT.
The 4K results were slightly better, but even then, the best that could be said is that the 5070 managed to match the 4070 Super, averaging just 33 FPS. Overall, a pretty disappointing showing.
Moving on to Counter-Strike 2, we see very similar results again. The RTX 5070 is essentially an RTX 4070 Super, as both delivered nearly identical performance at 1440p.
The 5070 did pull slightly ahead at 4K, but even then, it only managed a 6% performance uplift.
In Cyberpunk 2077 at 1440p, the 5070 came in slightly behind the 4070 Super, though the difference was just 2 FPS in terms of average frame rate. Still, that meant the 5070 was only 7% faster than the 7800 XT in this example.
At 4K, the 5070 performed slightly better relative to the 4070 Super, pulling ahead by a 6% margin with 51 FPS on average. This was without any RT effects enabled, using the second-highest rasterization preset.
Hogwarts Legacy is a bandwidth-sensitive game, which works in the RTX 5070's favor. At 1440p, it outperformed the 4070 Super by an impressive 21% margin, showing that under the right conditions, there are some notable gains. However, this only allowed the 5070 to match the 7800 XT.
Oddly, the margin shrank significantly at 4K, with the 5070 only 8% faster than the 4070 Super and the 7800 XT, averaging 66 FPS.
The Last of Us Part I delivered more typical results. At 1440p, the RTX 5070 performed identically to the 4070 Super, with both averaging around 90 FPS. This made the 5070 just 2% faster than the 7800 XT.
At 4K, the 5070 pulled slightly ahead, but the average frame rate was only 4% greater than that of the 4070 Super – a very underwhelming performance boost.
The last game we'll cover is Starfield, another case where the 4070 Super outperformed the new 5070 at 1440p. Granted, the performance was nearly the same, but it was still disappointing to see the 5070 coming in 3% slower.
At 4K, the 5070 was again 6% slower than the 4070 Super, averaging just 45 FPS. Another set of disappointing results.
Across the 16 games tested at 1440p, the RTX 5070 was, on average, just 1% faster than the 4070 Super. That settles it – the 5070 is basically a 4070 Super. Realistically, for this to be considered a next-gen GPU, it should have been called the 5060. But we've been down this road already with the 5070 Ti and 5080, and to some extent, even the 5090, which is really just a 4090 Ti.
It's a similar story at 4K. The RTX 5070 was, on average, just 5% faster than the 4070 Super, delivering RTX 3090-like performance. So maybe Jensen meant the 5070 would match the 3090, not the 4090, and he simply misspoke – yeah, that must be it.
We're not going to dive too deep into power consumption, as power usage tends to be a boring topic at the best of times. However, believe it or not, the 5070 is once again a 4070 Super in this regard, consuming roughly the same amount of power across the games we tested.
Now for the ray tracing benchmarks, starting with Metro Exodus Enhanced, where the 5070 matched the 4070 Super at 1440p. Well, technically, it was 4% faster, which is quickly starting to feel like a significant win for this new GeForce GPU.
At 4K, it was 8% faster, averaging 53 FPS – which is an embarrassingly low level of performance for a $550 GeForce GPU released in 2025. Of course, Radeon GPUs fare even worse, though none of them were released this year.
Next, we have Alan Wake II, and the 5070 struggles at 1440p even with upscaling, only matching the original 4070 and falling 15% behind the 4070 Super. That's a disaster, considering we're only looking at 39 FPS.
At 4K with quality upscaling enabled, the 5070 did manage to match the 4070 Super, but at just 22 FPS on average – not exactly a win.
Moving on to Cyberpunk 2077: Phantom Liberty, the RTX 5070 continues its trend of extremely underwhelming ray tracing performance, coming in 12% slower than the 4070 Super with an average of 58 FPS.
Once again, the 5070 is only able to match the 4070 Super at 4K with quality upscaling, but now we're looking at just 31 FPS on average – hardly an acceptable level of performance.
The Spider-Man Remastered results at 1440p are CPU-limited, but even so, the 5070 ended up 14% faster than the 4070 Super. Like Hogwarts Legacy, this game is very bandwidth-sensitive, which explains why the 5070 performs better here.
That said, there seems to be another bottleneck at 4K, as the 5070 drops back down to 4070 Super-like performance. While 91 FPS is still a solid result, it's hardly impressive when compared to previous-generation GPUs.
In Dying Light 2, the RTX 5070 managed to edge out the 4070 Super, but only by a mere 5% margin.
At 4K, the gap remained small, with the 5070 being just 7% faster at an average of 44 FPS – not exactly a game-changing improvement.
The RTX 5070 once again performs like a 4070 Super in Black Myth: Wukong, managing just 46 FPS at 1440p with upscaling. That said, at least it's not a Radeon GPU.
At 4K with upscaling, the RTX 5070 becomes completely useless, only matching the RTX 4070 Super at 25 FPS.
Lastly, we have Indiana Jones and the Great Circle, a late addition to this review. Tim suggested including it to highlight the RT performance of these new GeForce GPUs, and it certainly paints a bleak picture for the 12GB RTX 5070 – just 13 FPS on average at 1440p with upscaling.
Of course, you can lower the quality settings to avoid maxing out the VRAM, but these are the ray tracing settings Tim recommends for a truly transformative experience in this game.
Naturally, 4K is completely out of the question. In fact, it's so far out of the question that the game will crash and refuse to relaunch on a 12GB GPU with full RT enabled. To get back in, you'll have to start in safe mode and lower the settings.
So, how future-proof is the RTX 5070's RT performance – and perhaps its performance in general – as a 12GB GPU released in 2025? Not very, by the looks of it.
Since the RTX 5070 completely crapped out in Indiana Jones and the Great Circle with full RT enabled, we have removed that game from the average, leaving us with six titles. Even then, the 5070 still fell just behind the RTX 4070 Super – though, to be fair, overall performance was nearly identical.
At 4K, the 5070 was slightly faster than the 4070 Super, but again, overall performance was nearly identical. Even with the help of upscaling, frame rates were generally poor.
In a perfect world where MSRP actually meant something, the RTX 5070 would be a decent value product – if you overlook its obvious VRAM limitations, which will no doubt become a bigger issue in the coming years.
But ignoring that reality, the 5070 at $550 looks good – not amazing, but good. It offers 8% better value than the 5070 Ti and 20% better value than the RTX 5080. However, both of those products come with more VRAM. While it may only be the minimum amount of VRAM we'd want to see on a mid-range or better product, at least those cards meet that minimum.
The 5070 is also 13% better value than the RTX 4070 Super. Again, that's not an amazing generational uplift, but if available at MSRP, it stacks up fairly well.
However, the RTX 4070 Super saw some small discounts in 2024. If you bought one back then, the RTX 5070 only ends up being 10% better value. At that point, we'd rather give up 10% in efficiency for an extra 6 – 12 months of use, making an RTX 4070 Super purchase a year ago the smarter choice.
The RTX 5070 will have to compete with the Radeon RX 9070, so it's likely to hit the $550 MSRP sooner rather than later. That said, in Australia where I live, the RTX 5070 is expected to cost at least $1,250 AUD, which is a terrible price point. That makes it 8% more expensive in terms of cost per frame compared to the RTX 4070 Super, which was recently available for $1,100 AUD.
It also makes the RTX 5070 worse value than the Radeon RX 7900 XTX, 7900 XT, and significantly worse than the 7800 XT. The cost per frame compared to the 7800 XT is 31% higher, despite the 5070 having less VRAM – obviously a terrible trade-off.
Before wrapping up this review, let's take a quick look at how Nvidia's Founders Edition version of the RTX 5070 compares to the Asus Prime and Gigabyte Eagle models.
Under full load, the FE model peaked at a GPU temperature of 72°C, while the Eagle was significantly cooler at 63°C, and the Prime ran at 62°C. It was a similar story with memory temperatures – 76°C for the FE model, while the Eagle peaked at 64°C and the Prime at 68°C.
Oddly, the FE model reported a fan speed of 2,350 RPM, yet it didn't seem loud, measuring just 39 dBA. Typically, at that fan speed, we'd expect noise levels to be well above 40 dBA. Meanwhile, the Eagle and Prime were much quieter at 35 dBA and 34 dBA, respectively.
The Eagle consumed 20W less than the FE model, which led to a slightly lower clock speed of 2,760 MHz – odd considering this is the OC version of the Eagle, yet it clocks lower than Nvidia's reference model. The Prime exhibited a similar pattern, though its power consumption was reported to be higher despite having a slightly lower core clock.
We are not impressed with the GeForce RTX 5070. It's a compromised product that will struggle to fully utilize the RTX feature set due to its limited 12GB VRAM buffer. And while this may not be a major issue right now, it's likely to become one within the realistic lifespan of this product.
In terms of general performance, there's nothing exciting here. The RTX 5070 is essentially a refreshed RTX 4070 Super with $50 knocked off the MSRP. It can occasionally compete with the RTX 4070 Ti Super, but again, with just 12GB of VRAM, it's not a favorable comparison.
Realistically, you would have been better off buying an RTX 4070 Super months ago if you wanted this level of performance. Six months ago, the 4070 Super was available for $585, and a year ago, it was selling for $600. Waiting an entire year to save $50 while only gaining 5% more performance hardly seems worth it – another reason why the RTX 5070 is so underwhelming.
For those looking to spend around $550 on a GPU right now, there isn't a better option – at least not yet. However, the Radeon RX 9070 series launches tomorrow, and AMD is confident they can deliver models at that price. The RX 9070 could be faster, and it certainly offers more VRAM, making it a serious competitor. We have a detailed review of the new Radeons coming, and a dedicated look at FSR4 coming up.
In other words, if you're considering a $550 GPU purchase, it would be wise to wait a little longer to see which card is truly worth your money. That said, we suspect that spending an extra $50 on the 9070 XT will be the best move
The RTX 5070 hasn't impressed us – in fact, it's extremely underwhelming. We'll wait to see what AMD brings to the table before making any recommendations at this price point.
Since we published this review, we have run additional benchmarks and new reviews for graphics cards aimed at the same price bracket. Here are featured articles you may be interested in:

About
Ethics Statement
Terms of Use
Privacy Policy
Manage Ad Consent

TECHSPOT : Tech Enthusiasts, Power Users, Gamers
TechSpot is a registered trademark and may not be used by third parties without express written permission.
TechSpot is represented by Future PLC. Learn about advertising opportunities.
© 1998 - 2025 TechSpot, Inc. All Rights Reserved."
"NVIDIA","RTX 5070Ti ","Nvidia GeForce RTX 5070 Ti review: bad start, decent GPU","https://en.pressbee.net/show3746160.html?title=nvidia-geforce-rtx-5070-ti-review-bad-start-decent-gpu","I’m a bit late to the Nvidia GeForce RTX 5070 Ti, which means that this review – while already a dismal failure in the glowing robot eyes of the Internet nowness machine – can at least factor in the context of the GPU’s first few days on sale. Said context can be summarised thusly: Ah. Umm. Errrrrrrr.
No, early life has not been kind to the RTX 5070 Ti. Inadequate stocks and the lack of an Nvidia-made Founders Edition, unlike those of the RTX 5090 and RTX 5080, have cut the chances of finding one at the intended £729 / $749 RRP to pretty much nada. Not to mention how some models are shipping with missing render output units, potentially hurting performance. It’s a saddening start, made all the more unfortunate by the fact that the RTX 5070 Ti really isn’t a bad graphics card in itself.
  Read More Details  Finally We wish PressBee provided you with enough information of ( Nvidia GeForce RTX 5070 Ti review: bad start, decent GPU ) 
 Also on site : WWE Saturday Night’s Main Event Results & Winners: Cody Rhodes returns to aid Jey Uso & takes out John Cena; new member added to Seth Rollins' faction & more   ‘Pod Save America’ host says he struggled over whether to talk about Biden decline   Bears' Caleb Williams Receives More Harsh Criticism from Cam Newton"
"NVIDIA","RTX 5070Ti ","ASUS Prime NVIDIA Geforce RTX 5070 Ti GPU Review","https://www.cgmagonline.com/review/hardware/nvidia-geforce-rtx-5070-ti-gpu/","ASUS Prime NVIDIA Geforce RTX 5070 Ti GPU Review
ASUS Prime NVIDIA Geforce RTX 5070 Ti GPU
To put it bluntly, the NVIDIA Geforce RTX 5070 TI is an interesting card, even if it is overshadowed by its bigger brothers. The NVIDIA 50-series launch has been a bit of a challenge. On one hand, the new GPUs are some of the most powerful and offer plenty of features that make them forward-thinking. On the other hand, the price and generational uplift have been a bit of a disappointment, especially for the average gamer or creator who just wants an upgrade without having to sell a kidney to do so.
But now that the flagship NVIDIA RTX 5090 and NVIDIA RTX 5080 are out of the way, we finally have one of the more affordable options—the NVIDIA RTX 5070 Ti. On many levels, it feels more like a cut-down RTX 5080, bringing many of the same features at a much more affordable price tag. While it may not be the slam dunk many people had hoped for, the RTX 5070 Ti delivers where it matters most for those looking to upgrade from the 30-series at a respectable price.
Since there is no Founders Edition of the NVIDIA RTX 5070 Ti, NVIDIA sent over the ASUS Prime variant, set to stock clock speeds, for testing. The card follows the design trends of many third-party cards this generation, featuring a triple-fan design and a 2-slot size, all in a sleek black metal look that appears sharp without being garish. It also uses the 16-pin power connector and, overall, is a sharp-looking GPU. Don’t get me wrong—I love the look of the 50-series Founders Editions, and while this card doesn’t quite reach that level, ASUS has delivered a GPU that would look good in any system.
Looking at the specs and overall performance levels promised, NVIDIA’s GeForce RTX 5070 Ti carves out a compelling niche in the Blackwell-generation GPU lineup, balancing mid-tier accessibility with technical upgrades that narrow the gap to its higher-end siblings—the RTX 5080 and RTX 5090.
Designed for gamers seeking robust 1440p performance (and even solid 4K with DLSS 4 included) without venturing into flagship pricing, the 5070 Ti features 8,960 CUDA cores and 16 GB of GDDR7 VRAM on a 256-bit memory interface, delivering 896 GB/s bandwidth—a 78 percent improvement over its RTX 4070 Ti predecessor. While overshadowed by the RTX 5080’s 10,240 CUDA cores and 20 GB of VRAM, or the RTX 5090’s staggering 12,288 cores and 24 GB of VRAM, the 5070 Ti leverages its fourth-generation RT and fifth-generation Tensor cores to punch above its weight class in ray-traced workloads.
Priced at $749 US, the NVIDIA RTX 5070 Ti occupies a strategic midpoint between the $549 RTX 5070 and the $999 MRSP of the RTX 5080. While it lacks the 5080’s second sixth-generation decoder or the 5090’s industry-leading 3,400 AI TOPS, its 1,406 AI TOPS and support for DLSS 4 multi-frame generation offer tangible gains for gamers prioritizing smooth 1440p gameplay over cutting-edge specs.
We have covered what DLSS 4 brings to the table in both our RTX 5090 and RTX 5080 reviews, so I will only touch on it briefly here. Central to DLSS 4 is its ability for multi-frame generation and its new transformer model for upscaling, giving GPUs much more potential to deliver smoother frame rates, even at higher resolutions.
Unlike DLSS 3’s single interpolated frame, this technology synthesizes up to three AI-generated frames between each traditionally rendered frame, effectively quadrupling baseline performance. The transformer model also addresses longstanding critiques of AI upscaling, particularly in ray-traced environments. By refining ray reconstruction algorithms, DLSS 4 reduces artifacts such as light flicker and shadow noise, delivering a more photorealistic experience.
In our testing of the RTX 5090 and RTX 5080, DLSS 4 proved to be an exciting addition, and despite how surprising it may sound, it delivered notable improvements in frame rates—especially when incorporating ray tracing. But what matters most is how well the RTX 5070 Ti actually performs and whether it comes close to the levels showcased at CES 2025. With the card in hand and slotted into our test bench, we set out to determine if this GPU is worth the investment and if it truly is the mainstream option gamers and creators have been hoping for from the Blackwell generation.
As we saw with our previous 50-series reviews, we did our testing utilizing two identical systems, both running an AMD Ryzen 7 9800X3D in an ASUS ROG Crosshair Hero motherboard, paired with 32 GB DDR5 G.Skill RAM running at EXPO 6000. For storage, the systems are equipped with a PCI Gen 4 SK Hynix 2 TB M.2 SSD, with an NZXT 1,200W PSU in one and a be quiet! 1,200W PSU in the other, with one using an NZXT Kraken AIO and the other using a Corsair Titan AIO.
Jumping into our synthetic benchmarks, the NVIDIA RTX 5070 Ti delivers solid results compared to the 40-series—even the RTX 4080. In almost every test we ran, the RTX 5070 Ti either outperformed or was neck and neck with the last generation’s second-highest-tier GPU, consistently outpacing the RTX 4070 Ti by a healthy margin. It even managed a nearly 30 percent uplift in 3DMark Port Royal and a 20 percent uplift in 3DMark Time Spy. Coming out of these tests, the RTX 5070 Ti has managed to impress, especially when compared to the RTX 4080—a card that launched at a higher price point than this one, so for once, NVIDIA delivers a win for the value lovers out there. 
When looking at the AI benchmarks, the RTX 5070 Ti once again manages to deliver solid results across the board, pulling ahead of most of the 40-series, along with the AMD Radeon 7900 XTX. It should be noted that the 16GB GDDR7 is a great thing to see, especially compared to the 12GB GDDR6X seen in the RTX 4070 Ti, giving more legroom to play with more advanced models, although much like the RTX 5080, to push any of the biggest LLM’s and other options on the market, it still may hit a bottleneck.
Jumping over to content creation, the RTX 5070 Ti brings all the advancements of the Blackwell architecture, even if they are somewhat slimmed down compared to its bigger siblings. You still get the dual ninth-generation encoders found on the RTX 5080, but you are limited to a single NVDEC decoder, which pushes some of the workload onto the CPU when managing multicam workflows.
The overall performance in editing software like DaVinci Resolve and Adobe Premiere Pro was fantastic, with the card easily handling complex timelines while maintaining fast rendering speeds. It does fall short of the RTX 5090 due to having only dual encoders, but it still offers a solid overall experience. Additionally, it brings all the advancements in software and AI-driven effects, making it easier than ever to create content, remove noise and eliminate backgrounds—features that outperform previous generations.
With that out of the way, it’s time to dive into the gaming side of things, starting with 1080p performance. Out of the gate, the NVIDIA RTX 5070 Ti delivers solid numbers across the board, easily pulling ahead of the RTX 4070 Ti in all the games we tested. When frame generation is factored in, the comparison becomes even more lopsided, with the 5070 Ti handily outperforming the 4070 Ti in every benchmark. In some cases, it even surpasses the RTX 4080 and RTX 4090 when DLSS 4 multiframe generation is supported, such as with Cyberpunk 2077.
Even though most of what we saw was positive for the RTX 5070 Ti, there are some titles where the GPU fell behind the AMD Radeon RX 7900 XTX, such as Assassin’s Creed Valhalla and Returnal. Since neither game supports DLSS, this result makes sense, but it is still disappointing—especially for people looking to switch from an AMD card to the latest from NVIDIA. Additionally, we have yet to see what the new RX 9070 cards can deliver, and they do not seem far off at this point.
Moving over to 1440p, the story remains relatively consistent, and the NVIDIA RTX 5070 Ti continues to show solid overall performance across the range of titles we tested. The key takeaway here remains the improvements with DLSS 4, particularly the advancements in multi-frame generation. When DLSS is factored in, the RTX 5070 Ti punches well above its weight class, comparing favourably to the RTX 4080 and pulling well ahead of the RTX 4070 Ti. In titles like Cyberpunk 2077 and F1 24, the difference is significant, with the 5070 Ti delivering a 54 percent and 17 percent uplift compared to the last-generation card.
Looking at 4K, we start to see the limits of the NVIDIA RTX 5070 Ti when compared to the bigger, more powerful GPUs in the 50-series lineup. In most games, when you don’t enable ray tracing, the card manages to deliver well above 60 frames per second, with only a few titles struggling, such as Cyberpunk 2077, Metro Exodus and Black Myth: Wukong. When ray tracing is enabled, things get a bit trickier, with some titles like Cyberpunk 2077 dropping to sub-30 FPS levels.
This is where I want to mention that I would consider the NVIDIA Geforce RTX 5070 Ti a solid 1440p card that can handle 4K. Thanks to DLSS 4, even some of the titles that struggle manage to pull ahead and deliver impressive results. Cyberpunk 2077 jumps from a paltry 29.49 FPS to a staggering 214 FPS. Black Myth: Wukong climbs from 31 to 53 FPS. While still under 60 FPS, it’s a noticeable improvement that makes for a much smoother experience.
Now, don’t get me wrong—DLSS isn’t a silver bullet. While you will see improvements in frame rates, depending on the game, it can result in visual issues, input lag or an overall degraded experience, so results may vary. During our testing, I didn’t see major issues, but whenever you throw frame generation into the mix, results can be unpredictable—so keep that in mind.
Over on the power draw side of things, during our testing, we did not see the RTX 5070 Ti exceed 285 watts, making it slightly less power-hungry than the RTX 5080 and well below what the RTX 5090 draws. It was also incredibly quiet, remaining almost completely silent throughout all our testing, even when pushed in demanding titles at 4K. ASUS has done a solid job creating a quiet and efficient GPU with what they were given, and it even avoided the frustrating PSU whine we’ve seen on some other GPUs we’ve tested recently.
At an MSRP of $749, there is a lot to like about the NVIDIA Geforce RTX 5070 Ti GPU. It is a powerful GPU that brings all the advancements found in Blackwell at a much more affordable buy-in price compared to either the RTX 5090 or the RTX 5080. If you can find the card at that price, it offers solid value and is a good upgrade for anyone still using the 30-series or even those currently working with the RTX 4070 Ti who find it lacking in some areas. 
The issue is that while the MSRP may be attractive if it’s anything like past cards in the 50-series lineup, it could be a challenge to find it at that price. Stores and other sellers may push the price higher if supply fails to meet demand. Also, with no Founders Edition version of the GPU, buyers are left relying on partners to fill that gap. Despite their best intentions, this isn’t always the case, and prices can climb if supply falls short. Hopefully, NVIDIA can ensure that the supply is adequate because the RTX 5070 Ti is a card worth investing in.
I walked into this review unsure of what to expect, but honestly, NVIDIA impressed me. The RTX 5070 Ti feels like a great balance between cost and performance, delivering solid results when compared to the 40-series, with only a slight loss in performance compared to its bigger brother, the RTX 5080. We’ll have to see how the launch unfolds, but I have to commend NVIDIA—they have delivered a card well worth investing in, and for once, at a great performance-per-dollar price point. Now, let’s just hope stores can keep them in stock.
Brendan Frye has over a decade of experience in the gaming and media industry. As the Editor-in-Chief of CGMagazine, he also serves as a judge for gaming conventions and contributes to TV and radio shows. In his free time, he enjoys playing Souls games and watching horror films.
This post may contain affiliate links. If you use these links to buy something, CGMagazine may earn a commission. However, please know this does not impact our reviews or opinions in any way. See our ethics statement.
CGMagazine may earn a portion of sales from products that are purchased through our site as part of our affiliate partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of CGMagazine. By using this website, you signify your acceptance of these Terms of Use.
© 2021 CGMagazine Publishing Inc. All Rights Reserved."
"NVIDIA","RTX 5070Ti ","ASUS Prime GeForce RTX 5070 Ti Review","https://www.storagereview.com/review/asus-prime-geforce-rtx-5070-ti-review","The NVIDIA GeForce RTX 5070 Ti edition cards will launch on February 20th with an MSRP of $749.
The NVIDIA GeForce RTX 5070 Ti edition cards will launch on February 20th with an MSRP of $749. Since the Ti models do not have Founders Edition versions, NVIDIA provided us with an ASUS Prime GeForce RTX 5070 Ti for testing. It is positioned just above the entry-level RTX 5070 and offers performance close to the RTX 5080. This makes it an attractive option for gamers looking to upgrade or users entering entry-level AI workloads without overspending.
Equipped with the same advanced AI features as the RTX 5090 and RTX 5080, the ASUS Prime RTX 5070 Ti brings NVIDIA’s latest innovations to gamers at an affordable price. Featuring DLSS 4 with Multi-Frame Generation and Ray Reconstruction for smooth performance and stunning visuals, it ensures game responsiveness with NVIDIA Reflex 2 and Frame Warp, offering low latency for competitive play. Full ray tracing with neural rendering delivers true-to-life graphics, providing a more immersive gaming experience.
The ASUS PRIME RTX 5070 Ti features 8,960 CUDA cores, offering a notable jump from the RTX 4070 Ti’s 7,680 cores while sitting below the RTX 5080’s 10,752 cores. Alongside this increase in core count, it boasts 16GB of GDDR7 memory, a much-needed improvement over the RTX 4070 Ti’s 12GB of GDDR6. The jump to 16GB of GDDR7 memory is crucial as modern games increasingly demand more VRAM for their high-resolution textures, complex geometry, and ray-traced effects.
Furthermore, the increased bandwidth is particularly beneficial for AI-enhanced gaming features like DLSS 4 and neural rendering, which require rapid access to AI models stored in VRAM. Games utilizing NVIDIA’s AI-powered NPCs also benefit from the expanded memory capacity, as these features need additional VRAM to store their neural networks. With this combination of increased processing power and faster memory, the RTX 5070 Ti delivers excellent performance for gaming and AI-accelerated applications without the power and cost demands of flagship models.
Here are the full specs of the recent cards we have reviewed.
For a deeper dive into the Blackwell architecture and the innovations powering the 50-series GPUs, please look at our two previous reviews on the RTX 5080 and the RTX 5090 series cards.
This card measures 12 x 5 x 2 inches (304 x 126 x 50 mm), making it the same length as the Founders Editions RTX 5080 and RTX 5090, but with a thicker 2.5-slot width design.
For cooling, ASUS has implemented a triple Axial-tech fan design with dual ball bearings to enhance durability and maintain consistent airflow. This setup is complemented by a large heatsink, with heat pipes spanning the entire card and a vented backplate and sides to maximize heat dissipation. Additionally, all three fans come to a standstill when GPU temperatures are below 50°C, allowing for silent operation during less demanding tasks or light gaming. The fans automatically start up again once temperatures exceed 55°C.
For power, the unit includes, like many of the newer cards, a 12-pin to 3x 8-pin power cable suited to the card’s 300W rated power draw. The above image shows the unit’s aluminum backplate, which protects the card and contributes to the overall cooling of the rear components. The backplate also helps improve the card’s rigidity.
On the rear of the card, we can see the two-slot 304 stainless-steel backplate, but it’s important to note that this card takes up 2.5 slots due to the thickness of the cooler. For display connectivity, the card offers 1x HDMI 2.1b and 3x DisplayPort 2.1b, ensuring compatibility with a range of modern displays for high-resolution gaming or professional work.
Other notable features of this card include its switchable BIOS, controlled via a switch on the card, which toggles between “P” (performance) mode and “Q” (quiet) mode. This unit also incorporates a phase-change thermal pad designed to improve cooling performance and the longevity of the thermal material in contact with the chip. Lastly, ASUS provides GPU Tweak III software for fine-tuning the card’s performance to meet specific requirements.
To optimize the ASUS Prime GeForce RTX 5070 Ti, we used our high-performance AMD Threadripper platform. With a 64-core CPU and a custom water cooling loop, this system ensures the GPU operates at full capacity without CPU bottlenecks. The same configuration was used to test the Founders RTX 5080, RTX 5090, and RTX 4090, ensuring a fair and consistent comparison across all cards. The one subtle part to note is our tests were performed in “P” mode on this card.
Below is the complete system configuration.
StorageReview AMD Threadripper Test Platform
The Procyon AI Text Generation Benchmark simplifies AI LLM performance testing by offering a compact and consistent evaluation method. It allows for repeated testing across multiple LLM models while minimizing the complexity of large model sizes and variable factors. Developed with AI hardware leaders, it optimizes the use of local AI accelerators for more reliable and efficient performance assessments. The results measured below were tested using TensorRT.
In the text generation test, the RTX 5070 Ti, the most affordable card in the lineup, delivers performance surprisingly close to the RTX 5080 in AI text generation. The Phi benchmark scored 4,179 vs. 4,400, with a slight difference in tokens per second (192.487 vs. 209.46) and overall duration (15.77s vs. 14.91s). The Mistral and Llama3 benchmarks show similar trends, with negligible differences in token throughput. Llama2 presents a more significant gap, with the 5070 Ti scoring 4,284 vs. 4,790 and processing 75.91 vs. 83.65 tokens per second. Still, across all tests, the 5070 Ti remains close to the 5080’s performance, making it an outstanding choice for those on a budget and looking to get into AI text generation workloads.
The Procyon AI Image Generation Benchmark consistently and accurately measures AI inference performance across various hardware, from low-power NPUs to high-end GPUs. It includes three tests: Stable Diffusion XL (FP16) for high-end GPUs, Stable Diffusion 1.5 (FP16) for moderately powerful GPUs, and Stable Diffusion 1.5 (INT8) for low-power devices. The benchmark uses the optimal inference engine for each system, ensuring fair and comparable results.
In AI image generation, the RTX 5070 Ti delivers a respectable performance but falls behind the RTX 5080, which is expected. Stable Diffusion 1.5 (FP16) completes an image in 1.664 seconds compared to the 5080’s 1.344 seconds, with an overall score of 3,755 versus 4,650. The INT8 variant narrows the gap slightly, but the 5070 Ti still trails, generating images at 0.669 seconds each against the 5080’s at 0.561 seconds. The most considerable difference appears in Stable Diffusion XL (FP16), where the 5070 Ti takes 11.184 seconds per image, while the 5080 is notably faster at 8.808 seconds. Despite its lower ranking, the 5070 Ti remains a practical choice for budget-conscious users wanting to step into AI-driven image generation.
Luxmark is a GPU benchmark that uses LuxRender, an open-source ray tracing renderer, to evaluate a system’s performance in handling highly detailed 3D scenes. This benchmark is pertinent for assessing the graphical rendering prowess of servers and workstations, especially for visual effects and architectural visualization applications, where accurate light simulation is critical.
In LuxMark, the RTX 5070 Ti delivers solid performance with a minimal gap compared to the 5080. In the “Food” scene, it scores 12,073, just behind the 5080’s 13,637, and in the “Hall” scene, it scores 28,635 vs. 30,815. The difference is relatively small, indicating that the 5070 Ti still provides strong rendering capabilities at a more affordable price.
Geekbench 6 is a cross-platform benchmark that measures overall system performance. The Geekbench Browser allows you to compare any system to it.
In Geekbench, the RTX 5070 Ti, with a GPU OpenCL score of 246,875, offers solid gaming, video editing, and content creation performance. It’s a strong mid-tier option that still shows potential, delivering good power without the premium price of higher-end models like the 5080 or 5090. It’s great for 1440p or 4K gaming on high settings.
The V-Ray Benchmark measures rendering performance for CPUs, NVIDIA GPUs, or both using advanced V-Ray 6 engines. It uses quick tests and a simple scoring system to let users evaluate and compare their systems’ rendering capabilities. It’s an essential tool for professionals seeking efficient performance insights.
Lastly, in V-Ray, the RTX 5070 Ti, with a V-Ray score of 0,018 vpaths, delivers decent rendering performance, coming in slightly behind the 5080 with 9,311 vpaths, 5090 14,764 vpaths, and 4090 10,847 vpaths. While it shows potential for rendering tasks, the higher-end models offer significantly faster performance for more complex workloads.
Power consumption is a significant component of any high or low-end computing platform. Each new generation of GPU consumes more power under load, meaning larger power supplies and ample airflow for cooling. However, there is another aspect to power regarding performance: faster GPUs might spike higher, but the duration of each workload decreases.
At CES 2025, NVIDIA highlighted the improved power efficiency of the Blackwell architecture, which we were eager to see in action during an AI-driven workload. Using the Quarch Mains Analyzer in our test lab, we measured the total system power consumption while running the Procyon AI Image Generator Stable Diffusion XL FP16 test. This workload pushed each GPU to its power limits, with clear start and stop points for each generated image. This section compares the results of the new GeForce RTX 5090 and the previous-generation GeForce RTX 4090 and RTX 6000 Ada. Here is the link for a detailed review of the GeForce RTX 5080’s results.
We tested the Asus PRIME RTX 5070 Ti, with a 300W power consumption rating, for the power test. During the Procyon AI image-generating test, the system’s power consumption increased from an idle state of 231W to 601W under load, increasing by 370W. The average, under load, dropped to about 303W, which is interesting since the card is rated for 300W power consumption.
The second-to-last image was generated in just 11.1 seconds, with the system consuming 1.66Wh during that period.
One of the standout aspects of the GeForce RTX 5070 Ti is its performance. It delivers robust results in AI workloads, coming surprisingly close to the RTX 5080 in many benchmarks. This makes it an excellent option for those looking to get the most bang for their buck, especially as it comes equipped with 16GB of GDDR7, matching the GeForce RTX 5080.
Priced at $749, the RTX 5070 Ti offers an outstanding balance of performance and features, making it an attractive option for budget-conscious users. However, the card’s power consumption can exceed its rated 300W, so you might need a more robust power supply and cooling solution. Additionally, while it performs admirably, it does fall behind the RTX 5080 and higher models in more demanding AI and rendering tasks.
The ASUS Prime GeForce RTX 5070 Ti is highly recommended for those seeking a powerful yet affordable GPU. It’s ideal for 1440p or 4K gaming and entry-level AI workloads, offering exceptional value and 16GB of GDDR7 without the premium price tag of higher-end models.
Newsletter | YouTube | Podcast iTunes/Spotify | Instagram | Twitter | TikTok | RSS Feed
Products and solutions from our affiliate partners:
Subscribe to the StorageReview newsletter to stay up to date on the latest news and reviews. We promise no spam!
Copyright © 1998-2024 Flying Pig Ventures, LLC Cincinnati, Ohio. All rights reserved.
To provide the best experiences, we and our partners use technologies like cookies to store and/or access device information. Consenting to these technologies will allow us and our partners to process personal data such as browsing behavior or unique IDs on this site and show (non-) personalized ads. Not consenting or withdrawing consent, may adversely affect certain features and functions.
Click below to consent to the above or make granular choices. Your choices will be applied to this site only. You can change your settings at any time, including withdrawing your consent, by using the toggles on the Cookie Policy, or by clicking on the manage consent button at the bottom of the screen."
"NVIDIA","RTX 5070Ti ","RTX 5070 Ti vs RX 9070 XT: Which GPU is right for you?","https://www.techradar.com/computing/gpu/nvidia-rtx-5070-ti-vs-amd-rx-9070-xt-which-gpu-is-right-for-you","The most powerful midrange cards compared

When you purchase through links on our site, we may earn an affiliate commission. Here’s how it works.

Compute Units: 70Shaders: 8,960Ray processors: 70AI/Tensor processors: 280Boost clock: 2,452 MHzMemory type: GDDR7Memory pool: 16 GBMemory speed (effective): 28 GbpsMemory bandwidth: 896 GB/sBus interface: 256-bitTGP: 300WPower connector: 1 x 16-pinSlot width: Dual
Compute Units: 64Shaders: 4,096Ray processors: 64AI/Tensor processors: 128Boost clock: 2,970 MHzMemory type: GDDR6Memory pool: 16 GBMemory speed (effective): 20 GbpsMemory bandwidth: 640 GB/sBus interface: 256-bitTGP: 304WPower connector: 2x 8-pinSlot width: Dual
If you're after a little more power to push 1440p (and even 4K) gaming in your machine in 2025, then you're going to want to seriously consider the recently released Nvidia RTX 5070 Ti and the AMD RX 9070 XT. Both midrange GPUs are more capable than the likes of the respective RTX 5070 and RX 9070, meaning they can be ideal for more hardcore gamers, but which one is the best?
That's why we're comparing the RTX 5070 Ti vs RX 9070 XT in terms of their price, specs, and performance so you can buy with confidence, safe in the knowledge that you've got the perfect GPU for your setup. There are a wealth of need-to-know differences, including the drastically opposed architecture for how the hardware play (and interact with) the software you want to use.
As for how the RTX 5070 Ti and RX 9070 XT stack up against the best graphics cards, it ultimately all comes down to how the pricing stacks up against the performance on offer. In an ideal world, we would all have the best 4K graphics card that money can buy in our machines, but that's simply not realistic or viable for the vast majority of PC gamers, hence why the midrange remains a popular choice in 2025.
The Nvidia RTX 5070 Ti was released on February 20, 2025, with a starting price of $749. Unlike other graphics cards in the Blackwell lineup, there is no Founders Edition (Nvidia-made) option, meaning it's solely up to the manufacturer's partners (such as ASRock, Asus, MSI, PNY, Prime and more) to set the recommended retail price for the hardware.
Additionally, varying models, such as those that ship overclocked by default or feature larger cooling solutions, can be far more expensive, eclipsing the rate proposed by Team Green.
In contrast, the AMD RX 9070 XT debuted on March 6, 2025, with a starting price of $599, which is more comparable to the RTX 5070 (and RTX 4070) than the higher-end 70-class card from Nvidia. AMD particularly highlighted its best-in-class performance at its unveiling, considering its ""under $600"" price tag.
Similarly to the RTX 5070 Ti, there is no AMD-made reference card, so it's again strictly up to the company's partners to set the prices and maintain them.
With this considered, the AMD RX 9070 XT is the cheaper of the two by quite a considerable margin, with a full $150 cheaper price point considering the two respective MSRPs. However, the actual gap between them could be narrower when factoring in the different manufacturers' prices, depending on the feature sets, which is something to be aware of when weighing the two up.
Before we go into the myriad of technical differences between the RTX 5070 Ti vs RX 9070 XT, it's important to outline how the two graphics cards are similar. At a base level, both Nvidia's and AMD's GPUs are PCIe 5.0-compliant GPUs featuring 16GB VRAM built on a 256-bit memory bus with a roughly identical TDP.
That's about where the similarities end, however, as it's clear that Blackwell and RDNA 4 were made with very different design philosophies in mind. The RTX 5070 Ti features far faster (and denser) GDDR7 video memory compared to the older (and far slower) GDDR6 memory of the RX 9070 XT. Curiously, AMD did not see the benefits of utilzing the faster GDDR6X memory instead, which Nvidia's previous-generation graphics cards (RTX 40 series) had been built on since 2022.
Despite both being made to be PCIe 5.0 compliant, the GDDR7 VRAM of the RTX 5070 Ti has allowed for much faster effective speed of 28 Gbps (a staggering 40% increase over the RX 9070 XT's 20 Gbps). This is equally reflected in the total bandwidth available as Nvidia's card pushes up to just under the 900 GB/s mark, whereas AMD's video card can't reach the 650 GB/s range.
The largest difference between the RTX 5070 Ti's specs when compared with the RX 9070 XT is the sweeping difference in shaders and AI cores. For both, there's just shy of a 120% increase when contrasted against the RX 9070 XT, showing just how much Nvidia has backed artificial intelligence for its latest line of video cards. There's no contest when comparing the hardware in terms of raw numbers.
While there's quite a wild disparity in the price and specs separating the RTX 5070 Ti from the AMD RX 9070 XT, real-world performance testing shows their true potential. Regarding synthetic benchmarks, Nvidia's graphics card does pull ahead, but there's not quite the gap that you may expect given the lower price and (seemingly) weaker hardware under the hood.
The differences in the GeekBench 6 Compute (OpenGL) benchmark show how close these two cards can be in practice. The RTX 5070 Ti scored 243,483, with the RX 9070 XT achieving 223,065 (a difference of 9.1%). Some of the 3DMark benchmarks demonstrate a narrower gap, such as can be observed with Night Raid. The former tallied 252,604 compared to the latter's 250,811 (less than 1% difference). It's a similar story with Steel Nomad and Time Spy as well.
Gaming is where both the RTX 5070 Ti and RX 9070 XT get to shine to flex the 16GB memory pool. While 4K is (arguably) not their strong suit, both GPUs are more than capable of delivering playable frame rates in today's demanding titles.
Black Myth: Wukong is playable above 60fps on both video cards, with 69fps and 63fps averages. Cyberpunk 2077 in Ultra settings is similarly close at above 90fps from the two, with averages of 96 and 93, respectively. Certain games do benefit from the GDDR7 memory pool, however, as Returnal with Epic settings and ray tracing enabled is no problem for the RTX 5070 Ti with 67fps, but the RX 9070 XT couldn't quite achieve 4K60.
1440p gaming is where we start to see the RTX 5070 Ti and RX 9070 XT excelling. Black Myth: Wukong's average frame rates jump up to 86 and 83, respectively, with Cyberpunk 2077 in Ultra settings playable at (or above) the 180fps mark on both graphics cards. Dying Light 2 set to High Quality pushes way above 200fps with a 10-frame lead in Nvidia's favor, for as much as that matters. Additionally, there's just a single frame separating the two at 144 and 143fps apiece, with the RX 9070 XT coming out ahead here.
Creative workloads see the RTX 5070 Ti edging ahead of the RX 9070 XT, however, it's largely situational to the software as to how big of a lead there is. For example, the former scored 11,318 in PugetBench for Adobe Photoshop, whereas the latter achieved 10,936 (a difference of 3.4%). Our Handbrake 1.6 4K to 1080p H.264 encoding test saw the gap narrowed even more with respective average frame rates of 218 and 213 (a difference of 2.3%). While things are very close, Nvidia's GPU edges ahead ever so slightly in all major categories.
Based on everything we've gone over when comparing the RTX 5070 Ti vs RX 9070 XT, the answer of which is best will ultimately come down to your use needs. Solely for gaming, you're better off investing in the far cheaper (and likely much more available) AMD graphics card and benefitting from the 16GB memory pool for the same price as the 12GB RTX 5070.
However, if you're primarily wanting a more affordable GPU for creativity and productivity tasks as well as gaming then the Nvidia RTX 5070 Ti has a little more under the hood to deliver a higher-end experience across the board. Whether that's worth the extra $150 (potentially more) is up to you.
Despite having GDDR7 VRAM onboard, it doesn't seem like it's made too much of a substantial difference when gaming in either 1440p or 4K. It will (likely) be a good investment for the future of PC games, but it isn't something that's been utilized today. Then we get onto the fact that Blackwell stock is hard to come by in May 2025 whereas AMD is promising ""wide availability"" for its RDNA 4 hardware. If you want the best value for money then we recommend the 9070 XT, but if you're wanting to push things a little further, then you should go for the RTX 5070 Ti.
Sign up for breaking news, reviews, opinion, top tech deals, and more.
Formerly TechRadar Gaming's Hardware Editor, Aleksha McLoughlin is now a freelance writer and editor specializing in computing tech, video games, and E-commerce. As well as her many contributions to this site, you'll also find her work available on sister sites such as PC Gamer, GamesRadar, and Android Central. Additionally, more of her bylines can be found on Trusted Reviews, Dexerto, Expert Reviews, Techopedia, PC Guide, VideoGamer, and more.
Please logout and then login again, you will then be prompted to enter your display name.

TechRadar is part of Future US Inc, an international media group and leading digital publisher. Visit our corporate site.

©
Future US, Inc. Full 7th Floor, 130 West 42nd Street,
New York,
NY 10036."
"NVIDIA","RTX 5070Ti ","What it's like to use the RTX 5070 Ti, before and after a much-needed PC overhaul","https://www.polygon.com/review/533860/nvidia-rtx-5070-ti-review-3070-upgrade-worth-it","In aging machines, a new GPU isn’t a silver bullet for more frames
Trying to figure out what’s causing slow frame rates in a PC is sort of like solving for a mysterious leak in your home. It could be the graphics card, or it could be slow storage, outdated RAM, the CPU — one, or all of the above. While doing a complete overhaul is a surefire way to solve the problem, it’s expensive and not financially savvy in the slightest. Ideally, you isolate the issue, then methodically make upgrades as needed.
So, yeah, my PC is getting old. The Monster Hunter Wilds graphics benchmark made me realize I would need some upgrades sooner than later (ideally, before Grand Theft Auto 6 and The Witcher 4), as it chugged and textures were slow to load in. My RTX 3070 Ti is an unlikely suspect, being the newest component in my setup, and otherwise not having issues running most games pretty well at 1440p resolution. On the other hand, my 2018 Intel Core i5-9600K is probably the culprit, having fallen below the minimum required specs in many new, graphically demanding games coming out.
With the RTX 5070 Ti showing up at my doorstep, I didn’t feel like I could fairly assess it with my preexisting hardware. So, this review took me on a journey during which I explored multiple hardware scenarios, ultimately ending up with a near-total rebuild of my PC. My testing illustrates how a new GPU can inject life into an outdated build, and how sometimes that’s not the case. Additionally, I explored how much just refreshing the hardware surrounding the GPU can improve gaming performance, even if you have an old-ish graphics card. Plus, unlike fixing a leak, these are issues that I actually had fun solving.
I compared anecdotal performance in Avowed and Indiana Jones and the Great Circle, then checked out controlled benchmarks in Cyberpunk 2077 and Monster Hunter Wilds — all in 1440p and on the highest possible settings, including ray tracing when possible.
Hooking up the $749.99 Asus Prime RTX 5070 Ti to my preexisting rig required just a bit of tweaking; I needed to plug in an additional PCIe power cable into my 750 W power supply to connect to the GPU. My 3070 Ti uses just two PCIe cables from the power supply, while the RTX 50-series cards released so far require three. Once I powered it on, I reinstalled the Nvidia GPU driver and got to gaming. It was illuminating in ways I didn’t expect to run games with the RTX 5070 Ti installed, as some titles performed much better than before, while others didn’t improve enough over my RTX 3070 Ti’s performance to warrant the 5070 Ti’s high cost. It simply depends on what you play.
In Avowed, which is a very CPU-intensive game (my 2018 CPU just made the cut for minimum spec requirements), the RTX 5070 Ti made it possible to get about 10 more frames per second on average than I was getting before. It went from a relatively stable 40-50 frames per second (with dips to about 35) in the 3070 Ti to a more stable 50-60 in the RTX 5070 Ti. Performance improved more once ray tracing was turned off, of course. Based on the miniscule performance gains in this title, I’d be kicking myself for having paid for the RTX 5070 Ti.
Cyberpunk 2077 is more reliant on the GPU than other open-world games I’ve tried. And so the 5070 Ti delivers considerably improved performance despite being installed in my older system. With Frame Generation (FG, a feature exclusive to RTX 40- and newer GPUs) turned off, the 5070 Ti achieved an average of 57 frames per second in the game’s benchmark with ray tracing set to “Psycho” mode and the very intensive path-tracing graphical mode on (which does an even better job than ray tracing in terms of showing how light bounces realistically) — more than doubling what my 3070 Ti could do. With FG turned on with the 5070 Ti, the frame rate shot up to 100. Having FG at my disposal is an incredible asset (especially in this aging PC), letting me keep the quality cranked up without sacrificing frame rate. But big graphically demanding games that are primarily taxing to GPUs, and not to CPUs as well, aren’t so common.
To my surprise, Indiana Jones and the Great Circle ran well enough on my old rig with the 3070 Ti that I could have gotten through it without too much trouble. Despite bypassing errors about not having enough VRAM to run high graphical settings, it ran comfortably between 60-70 frames per second. With the RTX 5070 Ti, primed to take advantage of the new DLSS 4 update, there were plenty of new options to turn on, including path tracing, quality toggles for various ray-tracing effects, and Multi Frame Generation (MFG, exclusive to RTX 50-series cards, can display up to three AI-created frames for every one frame rendered traditionally). Performance shot up to about 130 frames per second without path tracing, and about 75 with it on. 
The most brutal test for my old system, as mentioned before, was none other than Monster Hunter Wilds. No matter the GPU, this game and its hardware benchmarks did not want to play nicely with my machine. The Intel i5 9600K is one generation below Wilds’ minimum specifications, and the downsides of not meeting it were less forgiving than I expected. Textures often loaded in slowly, and frame rates were anything but stable — even with the RTX 5070 Ti installed. Neither GPU could achieve a stable 60 frames per second with all settings turned up, with the newer GPU adding just a handful of frames onto what the 3070 Ti could do. Frame Generation in the 5070 Ti greatly improved the performance, going up to 94 frames per second.
In most cases, the RTX 5070 Ti’s improvements were noticeable in my old machine. But if I didn’t have a near-total PC rebuild on the horizon, I’d definitely regret buying it. As previously mentioned, I (responsibly for this review, but irresponsibly for my budget) lined myself up with some key upgrades to get the most out of the new GPU. I got an AMD Ryzen 7 9700X processor, 32 GB of much faster DDR5 RAM, and a modern motherboard totaling $399.99. I kept my old case, the power supply, and my PCIe 3.0 M.2 SSDs (full setup here). 
Naturally, the first thing I did with this brand-new setup was slot in the older RTX 3070 Ti. I wanted to see just how much a new CPU could uplift performance in the games I was playing, or not.
Avowed on this new system was a much smoother experience with the RTX 3070 Ti installed. The frame rate hung around in the 70s no matter the location (to reiterate, this is better than the performance I got with the 5070 Ti installed in my old system). If I was keen on keeping my GPU, or having a difficult time finding an RTX 5070 Ti in stock, I’d be ecstatic with this improvement, to the point that I might not upgrade to this 50-series generation.
That enthusiasm wore off quickly because, in Cyberpunk 2077, my upgrades made next to no difference to the frame rate. Good to know, but not surprising. Same goes for Indiana Jones; there were minor improvements, but nothing to write home about.
Monster Hunter Wilds yielded some interesting improvements with the combo of the faster processor and memory. The jittery performance and texture pop-in was eliminated, and I was thrilled to wave goodbye to that mess. That said, it averaged out to a similar, but smoother-appearing frames rate as my older PC configuration. Whereas I’d be miserable enduring the choppy gameplay I noted earlier with either GPU in my old build, I think I could tolerate this experience with the RTX 3070 Ti.
I finally let the RTX 5070 Ti get to work in the new configuration, and it felt like such a long time coming, what with all of the permutations I’d been testing (and the fun but stressful weekend of PC building). 
There were monumental frame rate improvements in all of the games I’d been testing. Avowed soared above 100 frames per second. It feels wonderful to play without staring at the frames per second counter. The 5070 Ti made short work of Cyberpunk 2077’s most intense graphical settings, running it with path tracing, “Psycho” ray-tracing settings, and Multi Frame Generation (set to two frames generated by AI) at 124 frames per second. Doubling MFG to four frames resulted in performance going up to a whopping 219 frames per second. If you have a high-refresh-rate monitor that goes above 144 Hz, the 5070 Ti will let you squeeze more value out of your tech.
Indiana Jones’ many ray-tracing features include extensive path tracing along with MFG, making it possible to experience the best visuals and a high frame rate. With all settings switched to their maximum values, I was able to get about 100 frames per second. Some people may be sensitive to the latency introduced by MFG, but so long as I kept it at 2x instead of 4x, things felt fine for me. Without MFG on, the 5070 Ti ran the game at a smooth 70 frames per second — not bad.
In the end, I solved my PC problems, and it runs better — with or without the RTX 5070 Ti. Nvidia is positioning the RTX 5070 Ti as being a great card to update to from the RTX 3070 Ti, and The Verge’s review considers it to be on the level of an RTX 4080, but cheaper. I, too, can attest that it’s a great upgrade, and that it’ll be a good component to keep around for the next five years or so. But, for anyone out there whose PC components are already five or more years old, you might have bigger problems than just a GPU. And even a powerful GPU like this one can’t magically make them go away. 
The Asus Prime RTX 5070 Ti is available to purchase from multiple retailers. It was tested using a retail unit provided by Nvidia. Vox Media has affiliate partnerships. These do not influence editorial content, though Vox Media may earn commissions for products purchased via affiliate links. You can find additional information about Polygon’s ethics policy here.
The best of Polygon in your inbox, every Friday."
"NVIDIA","RTX 5070Ti ","Nvidia GeForce RTX 5070 Ti review: a cheaper RTX 4080","https://www.theverge.com/gpu-reviews/615075/nvidia-rtx-5070-ti-review-test-benchmark","The RTX 5070 Ti delivers almost identical performance to the RTX 4080 with slightly less power draw.
If you buy something from a Verge link, Vox Media may earn a commission. See our ethics statement.
If you buy something from a Verge link, Vox Media may earn a commission. See our ethics statement.
Nvidia’s new RTX 5070 Ti graphics card is the most intriguing entry in the 50-series lineup so far, promising to outperform last-gen cards that were even more expensive. At $749, it comfortably beats AMD’s $899 RX 7900 XT, trades blows with the $999 RX 7900 XTX, and is closer than I was expecting to Nvidia’s own $999 RTX 5080. It’s a capable card for a relatively good price, and that makes the RTX 5070 Ti an ideal choice if you’re tempted to move to 4K gaming or want a card that can deliver high frame rates for 1440p. 
But like the rest of the 50-series cards, the RTX 5070 Ti is not as much of an upgrade as we’ve come to expect from Nvidia. The card is around 14 percent faster at 4K than the last-generation RTX 4070 Ti and 22 percent faster at 1440p. It’s nearly as fast as the RTX 4080 Super, which launched at $999, and it’s more than 80 percent faster at 4K than the two-generations-old RTX 3070 Ti. Those are decent numbers, but it’s mostly because the 40 series was such a big leap. 
All of this makes the RTX 5070 Ti a good deal at its $749 MSRP, but only if it stays there — and prices are already going up.
Editor’s note: We aren’t scoring the RTX 5070 Ti until the pricing situation is clear at launch on February 20th.
RTX 4080 levels of performancePower draw isn’t significantly higher than the RTX 40 series$50 less than the RTX 4070 Ti
I wish it had more than 16GB of VRAMNot a huge performance boost over the RTX 4070 TiUnlikely to stay at $749 MSRP
I’ve been testing MSI’s RTX 5070 Ti Ventus 3X OC edition, which looks almost identical to the RTX 4070 Ti Super I reviewed last year. MSI’s tried-and-true Ventus 3X OC cards use a triple-fan layout with a copper baseplate and heat pipes to cool the GPU and memory modules. 
For both my 4K and 1440p testing, I paired MSI’s RTX 5070 Ti with AMD’s latest Ryzen 9 9800X3D processor and Asus’ 32-inch 4K OLED PG32UCDP monitor. I also put the RTX 5070 Ti up against the $999 RTX 5080 — the next step up in the RTX 50 series — as well as the RTX 4080, RTX 4070 Ti, RTX 4070 Ti Super, and AMD’s Radeon RX 7900 XT.
The RTX 5070 Ti delivers what you’d expect from a modern GPU: playable frame rates at both 4K and 1440p with all the settings maxed out. While the hardware isn’t quite as sleek as Nvidia’s Founders Edition cards, MSI’s GPU will fit well into most gaming rigs.
Without DLSS or ray tracing enabled, the RTX 5070 Ti is nearly 13 percent faster than the RTX 4070 Ti Super at 4K and 1440p, enough to make it practically identical to the RTX 4080 at 4K and not far behind at 1440p. You could even overclock the RTX 5070 Ti with the click of a button in Nvidia’s app or by using MSI’s Afterburner utility and get slightly ahead of the RTX 4080 Super. 
The RTX 5070 Ti makes for a great 1440p experience and solid 4K gaming, especially if you’re willing to turn the settings down slightly or overclock the card to squeeze out a little more performance. The next step up in the RTX 50-series line, the $999 RTX 5080, is around 12 percent faster than the $749 RTX 5070 Ti at 1440p and around 15 percent faster at 4K resolution — but you have to pay an extra 33 percent in cash for not much more performance.
The RTX 5070 Ti, much like the RTX 5080, makes the most sense if you’re upgrading from the RTX 30-series models or earlier. At 4K without DLSS or ray tracing, the RTX 5070 Ti is nearly 84 percent faster than the RTX 3070 Ti. It will be a noticeable upgrade if you’re using an RTX 30-series card or older, and you’ll also get access to Nvidia’s Frame Generation technology.
Nvidia’s RTX 3070 Ti, which launched in 2021, was only suitable for very entry-level 4K gaming, in part because it shipped with just 8GB of VRAM. The limitations were especially noticeable in the Forza Motorsport benchmark, where the RTX 3070 Ti averaged a measly 33fps compared to the 109fps on the RTX 5070 Ti.
I’m hoping we won’t see those same limitations further down the line with the RTX 5070 Ti’s 16GB of VRAM. The original RTX 4070 Ti only had 12GB of VRAM, which Nvidia bumped up to 16GB with the RTX 4070 Ti Super. If there was a little more VRAM on the RTX 5070 Ti, then it would help avoid any video memory limits at 4K gaming in the future.
I’ve also been testing DLSS 4 and Nvidia’s new Multi Frame Generation technology with the RTX 5070 Ti. Multi Frame Gen uses the latest AI graphics models, powered by an updated transformer architecture, to generate up to three additional frames per traditionally rendered frame, pushing the RTX 5070 Ti’s frame rates beyond what it’s normally capable of at 4K and 1440p. 
Cyberpunk 2077 has been a great test title for DLSS 4 and Multi Frame Gen, demonstrating how Nvidia’s new technique can provide motion clarity improvements with higher frame rates. Without DLSS 4, Cyberpunk runs at an unplayable 15fps average with ultra settings and full ray tracing enabled on the RTX 5070 Ti at 4K resolution. I was able to push that all the way up to 110fps thanks to 4x Multi Frame Gen.
As I’ve argued in my previous RTX 50-series reviews, this still doesn’t feel like a real 110fps because it maintains the same input latency it has after DLSS Super Resolution is applied. Super Resolution renders the game at a lower resolution and then uses AI to upscale it — in this case, from 15fps to 33fps. Multi Frame Gen then creates the three extra frames to make the motion of gameplay look smoother, but it still feels like a sluggish 33fps, so it’s less reactive than a real 110fps when you hit a button or click to shoot.
DLSS Multi Frame Gen still makes the most sense at 1440p with the RTX 5070 Ti, simply because the base frame rates (once Super Resolution is applied) are high enough to make the input latency less of an issue.
While there still aren’t many games that support Multi Frame Gen and DLSS 4, you can force these techniques on with Nvidia’s new app. DLSS 4’s new transformer model even works on existing RTX cards, so you’ll get the benefits of improved image quality if you force DLSS 4 on in games before developers have patched them. 
Like the rest of the RTX 50 series, the RTX 5070 Ti is also a capable card if you want to do video editing or run AI workloads. I found that the RTX 5070 Ti was nearly 14 percent faster than the previous RTX 4070 Ti in PugetBench’s DaVinci Resolve test and Procyon’s AI XL (FP16) test.
Nvidia recommends a 750-watt power supply for the RTX 5070 Ti, which is only 50 watts more than for the RTX 4070 Ti and 4070 Ti Super. The 300W total graphics power is just 15 watts more than the 4070 Ti.
At 4K resolution, the RTX 5070 Ti averaged 271 watts across the nine games tested — a little more than the 245-watt average draw of the RTX 4070 Ti but less than the RTX 4080’s 282W average for almost identical performance.
I only noticed the RTX 5070 Ti hit its max power draw of 300 watts during the Metro Exodus extreme benchmark, and it also maintained the exact same max GPU temperature (68 degrees Celsius) of the RTX 4080 in my open bench testing. It’s great to see Nvidia hasn’t massively increased power or cooling requirements here. 
The RTX 5070 Ti continues the 50-series trend of modest generational improvements. At 1440p, the RTX 5070 Ti delivers a good deal more performance than the RTX 4070 Ti and 4070 Ti Super, but I was really hoping for more at 4K. That would have pushed this firmly past the RTX 4080 and 4080 Super to make 4K gaming even more affordable.
Whether the RTX 5070 Ti is a good value will come down to pricing and availability. Without a Founders Edition to put pressure on board partner pricing, I think it’s going to be very hard to find an RTX 5070 Ti at $749. Micro Center is already listing overclocked models at $999, the same price as the RTX 5080, and only a single card is listed at $749 out of the 12 available. Nvidia insisted that the MSI card we tested has an MSRP of $749, though it was listed as $909 at Micro Center just hours before this review went live. 
I suspect it will come down to availability, too. It looks like there’s a real shortage of GPUs in the market. The RTX 5090 was barely in stock, and restocks are rare right now. Even the RTX 5080 is sold out at most places, and one retailer in the UK says it won’t be getting more for at least six weeks, with some models even further out. 40-series Nvidia cards and AMD’s 7900 XT series also look thin on the ground.
AMD might help with keeping pricing under control, as long as it has stock available. It’s about to enter the market with two new GPUs that look like they’re going to compete directly with Nvidia’s RTX 5070 Ti and RTX 5070. Nvidia has even delayed the debut of its RTX 5070 until March, which suggests a GPU pricing battle is about to take place at this important part of the market. 
The RTX 5070 Ti is a good purchase if you can actually get one at $749, but it might be worth waiting a few weeks to see how AMD’s performance, price, and availability shape up against Nvidia’s latest GPU.
A weekly newsletter by David Pierce designed to tell you everything you need to download, watch, read, listen to, and explore that fits in The Verge’s universe.
© 2025 Vox Media, LLC. All Rights Reserved"
"NVIDIA","RTX 5070Ti ","Do Not Buy: NVIDIA RTX 5070 Ti GPU Absurdity","https://gamersnexus.net/gpus/do-not-buy-nvidia-rtx-5070-ti-gpu-absurdity-benchmarks-review","Do Not Buy: NVIDIA RTX 5070 Ti GPU Absurdity (Benchmarks & Review)
You should not buy the NVIDIA RTX 5070 Ti (beware of scalped prices), or really, probably not any of the high-end cards right now. The market is in chaos and conditions are at an all time low for consumers. It benefits this entire industry, including us, if everyone mindlessly consumes, but we’re telling you not to be a part of that cycle. This is not the time to buy an NVIDIA video card. The company says the 5070 Ti (beware of scalped prices) will be $750, but we don’t believe it. In fact, some partner models have already been spotted at $1,000, with plenty of others coming in at $850 to $900. We heard of one that’ll have an MSRP above $1,000. That’s more than a 4080 (watch our review), which is insane. NVIDIA calls it the RTX 5070 Ti, but it’s really more like an RTX 4080 (beware of scalped prices) V3 -- or V4 if you count the unlaunched 4070 Ti which was a 4080 12GB.
Editor's note: This was originally published on February 19, 2025 as a video. This content has been adapted to written format for this article and is unchanged from the original publication.
The TLDW up front is that the 5080 (beware of scalped prices) is about 12-16% ahead of the 5070 Ti in some of the 4K gaming scenarios we tested, the 5070 Ti is about 28-35% ahead of the 4070 Ti at 4K, but down in the 20-28% range commonly at 1440p. The lead of the 5070 Ti over the 4070 Ti Super, which is the closest recent price neighbor, is only 7.8% to 20% in a lot of 4K situations, or commonly 12-16%. One of the worst scenarios we saw was an impressively bad 3.9% uplift from the 4070 Ti Super to the 5070 Ti at 1440p.
You should not buy this video card. You shouldn’t even read this article. We’ll save you the time: NVIDIA has become fat and monopolistic and AMD has decided it won’t compete in the high-end, and Intel is not there yet, leaving everyone with a worse ecosystem.
The 5080 (beware of scalped prices) to the 4070 is a difference that has now been subdivided several times. Like the 4080 (beware of scalped prices) and 4070 Ti (watch our review), which was a 4080 12GB originally, already subdivided it. Likewise, the 4080 Super subdivided it further. The 4070 Ti Super (read our review) subdivided that again, and now, the 5070 Ti subdivides it once more.
There are so many subdivisions in this block that it’ll have an HOA by next week... 
In one of our tests, we saw an average gap between cards in this upper-end of 6.7 to 8 FPS card-to-card, depending which you want to count. That really means that the only relevant factor is price (except for maybe VRAM in the few places it diverges, where that’ll matter for professional users). But even VRAM has mostly been split into the 90-class and everything else.
The 5070 Ti is often the same performance as the 4080, which is the same as the 4080 Super. The improvement on the 4070 Ti Super, which was close to the same MSRP, is hardly meaningful. This aligns with NVIDIA’s launch strategy of either leveraging its monopolistic positioning or boring the ever-living f*** out of people with stagnated change. Meanwhile, it appears that AMD is sticking to its failed strategy of MSRP = NVIDIA - $50. Let’s get into the rest of the review.
We’re taking a look at the ASUS RTX 5070 Ti Prime model, which we assume is named like that because ASUS is trying to get Logan Paul’s attention. Or maybe it has electrolytes, which is what the gamers crave.
The RTX 5070 Ti has 8960 CUDA Cores, 16GB of GDDR7, and a 256-bit bus. The memory spec is identical to the RTX 5080 (read our review), with the 5080 having more CUDA cores but being a significant cut-down from the 5090 (read our review), which is more than what we’ve typically seen. 
We have the full specs in a separate article here. 
Pricing is going to be the biggest problem for this GPU. Supply has been non-existent for the 5090 and 5080 and prices have skyrocketed into true FOMO targeting territory. We posted a video about the fake prices just a few days ago.
And in time for that, the 5070 Ti will launch with its own glut of pricing issues. We know one board partner has an actual MSRP north of $1,000 for one of its models. This gets worse considering the lower overall price of $750, which means that the proportional hike over expectation becomes untenable.
There’s really not a lot of ground to cover here that we didn’t in the intro, so let’s just get into the data.
For the testing here, we had to remove a couple of cards from the charts to make space. Specifically, we removed the 3090 (watch our review) and 3090 Ti (watch our review). If you want the data for those cards, check our 5080 review as it has all that.  
In Final Fantasy 14 at 4K, the RTX 5070 Ti landed at 97 FPS AVG with lows at 83 and 80. That’s the same as an RTX 4080 which, under normal conditions, might sound like a generational improvement on the arbitrarily named GPUs. Instead what we have is total generational stagnation because the 5070 Ti will regularly sell at prices, at least for now, approaching what the 4080 was. Even at only a slight elevation to $850, the card is simply too close to the last generation performance equivalent.
The 7900 XTX outperforms the 5070 Ti by 7.4%, with the 5080 16% ahead and the 5090 at 87% ahead. As for the 7900 XT Hellhound, which was the cheapest partner model for a while, that’s at 82 FPS.
Generationally by name, the 5070 Ti improves on the 4070 Ti by 34%, but the 4070 Ti Super is the more recent price replacement. The lead over the 4070 Ti Super is just 13%. That’s stagnation. We have the RTX 2070 Super (watch our review) in a future test, but the 1070 (watch our review) is here if you want something older. The improvement is 332% over Pascal, now 9 years old. 
At 1440p, the 5070 Ti ran at 187 FPS AVG. 1% and 0.1% lows are where they should be based on other results, so there’s nothing exceptional here. The 5070 Ti outranks the 4070 Ti Super by just 15 FPS, or only 9%. Sadly, this is one of the better ones, but that’s still brutal and is one of the most boring improvements possible when considering the MSRP similarity -- and that’s before you go get ripped off for more.
As for other 70-class cards: The 5070 Ti’s uplift over the 4070 Ti is 24%, then 72% on the 3070 Ti (watch our review), 94% on the 3070 non-Ti (watch our review), and 307% on the 1070. The uplift has diminished for each of these as resolution came down to 1440p, despite the 5070 Ti not being anywhere close to a CPU ceiling.
AMD’s RX 7900 XTX (watch our review) leads the 5070 Ti by 13%, up from 7% at 4K. The 5070 Ti also leads the 7900 XT by just 9% here.
At 1080p, things get even worse. The 5070 Ti leads the 4070 Ti Super by only 6%, or just 15 FPS average. Over the 4070 Ti non-Super (watch our review), which remains an absurd distinction, we’re only seeing a 16% advantage on the 5070 Ti. No wonder NVIDIA wants everyone to type the framerate into a calculator and multiply it artificiality by MFG.
The new card is just 10% over the 7900 XT, then 74% over the 3070 Ti.
Black Myth: Wukong is up now, tested at 4K. We’re removing the experimental chart labeling and feel that this test has had enough public visibility to clear that bar.
The 5070 Ti ran at the exact same framerate as the RTX 4080. NVIDIA has basically re-released the 4080, and the price isn’t even that different -- and in some cases, not different at all. In other words, NVIDIA has now released three RTX 4080s. This is the third one. At some point, it just seems like this is too much.
The 7900 XTX matches the 5070 Ti here. The difference is irrelevant and unnoticeable. 
Against the 4070 Ti Super’s 45 FPS AVG, the 5070 Ti leads by 13%, or 27% over the 4070 Ti non-Super and similarly over the 7900 XT. The lead over the 3070 Ti is 81%.
At 1440p, the “RTX 4080 v3” ran at 87.2 FPS AVG, with lows at 74.9 and 70.3. The third iteration of the 4080 isn’t the best one, though: The first version of the 4080 and second version of the 4080 are within error of each other and technically ahead of v3. But on the v3, aka the 5070 Ti, NVIDIA has enabled Multi-Fiat Generation to make it more interesting, though.
The RX 7900 XTX is about the same as the 5070 Ti here, with the 4070 Ti Super just below it. The 5070 Ti is only 10% ahead of the 4070 Ti Super and 20% ahead of the 4070 Ti, with a similar lead over the 7900 XT. 
At 1080p, the 5070 Ti is again roughly tied with the 7900 XTX, although credit to NVIDIA for having improved 0.1% lows -- just not by an amount anyone would notice in play. The frametime pacing is still good on both.
The 4080 Super (read our review) and 4080 are again within error of each other and, although they’re outside of error vs. the 5070 Ti, the real error is the GPUs we launched along the way.
The 5070 Ti ends up leading the 105 FPS 4070 Ti Super result by a staggering, mind-blowing 9.7%, made only more impressive by the fact that this was already functionally the same as the 4070 Ti’s 100 FPS.
The 7900 XT (read our revisit) sits just below this.
Starfield is up now. At 4K, The RTX 5070 Ti ran at 68 FPS AVG, this time allowing the RTX 4080 the important distinction of... being technically better.
The 5080 only leads the 5070 Ti by 12% here as well.
Troublingly for the 5070 Ti in this test, the 7900 XT nearly matches the RTX 5070 Ti, with the 7900 XTX ahead of it by 13%.
The 4070 Ti Super and 4070 Ti aren’t that different from each other in this one, either.
At 1440p, the 4080 Super and 4080 are exactly tied, with the 4080 Sub-Super, or as NVIDIA calls it, the “5070 Ti,” at 101 FPS AVG. 
The 5070 Ti ends up basically tied with the 7900 XT. The 7900 XTX holds an advantage at 11% ahead. The 5070 Ti’s improvement, if you can call it that, over the 4070 Ti Super is an impressively boring 3.9%, with the lead over the 4070 Ti at 11%. That’s impressive -- mostly because we’re impressed NVIDIA could make something so impressively stagnant.
At 1080p, Starfield has the 5070 Ti below the XTX, which has a slight lead at 6%. More impressively, the 4070 Ti Super ran at 120 FPS AVG. Between the 5080, the 4080s, the 5070 Ti, and the 4070 Ti Super and 4070 Ti, NVIDIA has managed to make a video card for seemingly every individual framerate between 110 FPS and 132 FPS. Why they’d do this, we have absolutely no idea.
Dragon’s Dogma 2 is up now. This one has the ASUS RTX 5070 Ti Electrolyte, which is what the games crave, at 73.6 FPS AVG. That’s right between the 4080 Super and 7900 XTX, making the 5070 Ti the first NVIDIA RTX 4080 Super-Super on the market. We won’t be impressed until they launch the triple-S-tier.
The 5080’s lead over the 5070 Ti is just 15%. That small of an uplift is going to weigh on NVIDIA.
The 5070 Ti leads the 4070 Ti Super by 18% and the 4070 Ti by 35%. The 7900 XT sits between the two 4070 Ti variants.
1440p reduces the 5070 Ti’s relative ranking, pushing it below the 4080 and 4080 Super, the two of which are tied and within error of each other. The 4070 Ti Super’s 106 FPS AVG result also makes for a boring positioning of the 5070 Ti. Again, we’re back to a card for every couple FPS. More than ever, this means price matters more.
The 7900 XTX leads the 5070 Ti by about 6 FPS here.
At 1080p, the RTX 5070 Ti’s 151 FPS AVG basically ties it with the 7900 XTX, which itself is tied with the 4080 (which is tied with the 4080 Super, which is a waste of chart space and so isn’t shown).
Generationally, the 5070 Ti leads the 4070 Ti Super by 10% and 4070 Ti by 18%.
Resident Evil 4 is up now, first rasterized. The 4K test has the 5070 Ti at 107 FPS AVG, with the 5080 at 122 and leading by 15%. Both are behind the RX 7900 XTX at 126 FPS AVG.
The 5070 Ti is ahead of the 4070 Ti Super by 20% and 4070 Ti by 34%. The 7800 XT (watch our review) is between these and the regular 4070, followed by the older 3070 Ti at 53 FPS AVG.
At 1440p, the 5070 Ti is just below the 4080 FE. The 5080 leads the 5070 Ti by 13%. 
NVIDIA basically took what previously would have been a 51 FPS gap from 224 to 173 FPS between the 4070 Ti Super and 5080, then split the difference with the 5070 Ti. 
The 7900 XTX is ahead by 17%, with the XT just below the 5070 Ti.
We’re moving to ray tracing testing now. We’ll start with Black Myth, which is heavily NVIDIA favored. Then we’ll look at some that are mixed or lighter workloads.
At 4K with upscaling as defined in the chart header, the 5070 Ti ran at 52 FPS AVG and tied the RTX 4080 and 4080 Super. The 5080’s 59 FPS result had it 13% ahead. Over the 4070 Ti Super, we see a 15% lead for the 5070 Ti. AMD gets absolutely crushed in this test.
At 1080p and still ray traced, the RTX 5070 Ti ran at 112 FPS AVG and led the 4080. The 5080 is improved on the 5070 Ti by 9%. The proximity of cards from the 4070 through the 5080 is crazy, though: We have the 4070 at 80 FPS, then the Ti at 92, then the Ti Super at 95, then the 4080 at 106, then the 4080 Super which isn’t shown but the same, then 5070 Ti at 112, then the 5080 at 122 FPS.
Dragon’s Dogma 2 at 4K with ray tracing is next. 
In this one, the 7900 XTX is more competitive and lands at 66 FPS AVG, which is between the 5080 and 5070 Ti. It’s unfortunate that AMD gets crushed so hard in some of the other games, like Black Myth: Wukong because it does OK in the ones that are less crazy intensive. The 5070 Ti ends up basically tied with the 4080 Super, which is basically tied with the 4080. The jump over the 4070 Ti Super’s 54 FPS AVG is 18% here.
1440p positions the 5070 Ti between the 4070 Ti Super and 4080. It’s not clear why this card needs to exist, but it does. The improvement against the 4070 Ti Super is just 10.8%. The 7900 XTX runs at 108 FPS AVG and is between the 4080s and the 5080. 
At 1080p, the 5070 Ti ran at 131 FPS AVG and was roughly tied with the 7900 XTX. The 4080 leads the 5070 Ti here by a few percent. The 5070 Ti leads the 4070 Ti Super by 9%, followed by the 7900 XT in the middle, then the 4070 Ti at 16%.
In Resident Evil 4 at 4K with ray tracing and upscaling, the RTX 5070 Ti ran at 118 FPS AVG and tied the 4080 and 4080 Super exactly. It’s within run-to-run variance and error. The 7900 XTX leads in this lightweight RT workload with a 14% advantage, posting a big difference from the heavier Cyberpunk and Black Myth workloads that we run.
At 1440p with the same settings, we see a similar lineup. The 5070 Ti is again within error of the 4080 and 4080 Super, which are the same. The 7900 XTX is slightly improved. The 4070 Ti Super is slightly behind. This is uninteresting.
Efficiency remains a relatively new test for us to include in each review, so we haven’t re-run the 4070 cards for efficiency yet.
In Final Fantasy 14 at 4K, the 5070 Ti landed at 0.37 FPS/W. It pulled 264W in this test, approaching its TDP spec. The 5080 ends up slightly more efficient, with the 4080 Super equivalent. The 7900 XTX is at a large disadvantage here due to its power consumption.
In Final Fantasy 14 at 1440p, the 5070 Ti ended up at 0.73 FPS/W, which has it just below the 4080 and 5080. The 5070 Ti improves on the 4060-class cards, but also is significantly more efficient than the 7900 XTX.
In Black Myth Wukong at 1080p, we found the 5070 Ti to be between the 4080 Super and 5080 for efficiency. The 7900 XTX shows again that this is its weakness, but its particular performance deficiency in Black Myth in general is hurting it disproportionately.
In ray tracing performance with F1 24, we found the 5070 Ti to perform about the same as the 4080. The 7900 XTX scores significantly lower. Overall, here you’re seeing much lower numbers than in some of the other charts and that’s because this is a heavy ray tracing workload tested at a higher resolution.
All of this means that NVIDIA is basically just selling you an RTX 40 series card, maintaining elevated prices, and doing so while pushing DLSS4 and MFG as the only real differentiating factor on the 50-series cards. 
We already have a piece up talking about the new Transformer model versus the prior CNN model for DLSS, which you can check out for a deep dive. We also have a video with an image quality comparison, including a frame-by-fake-frame break-down of MFG, where we analyze the AI or synthetic frames against the keyframes, or the native frames. We do this at 2X and with MFG 4X.
The improvements in the Transformer model over the older CNN model for image quality are apparent; meanwhile, the generated frames serve their purpose of smoothing, but sometimes look bad. What they don’t do is turn a low frame rate like 20 FPS into something that is instantly playable. But as far as image quality, it’s highly situational; in some situations, DLSS ends up better than native because game developers have decided to ruin their games with terrible default options, which shouldn’t happen, but there are also a lot of scenarios where it looks awful. We also found bugs in the driver override features that NVIDIA has pushed to the public. This includes sometimes it incorrectly running the generated frames in the wrong places.
This quick thermal chart at steady state during a looping 4K Port Royal workload and it shows the GPU and memory results. We don’t have other 5070 Ti cards to compare, so we can’t produce a like-for-like comparison. The 5080s are only here for reference.
The 5070 Ti came in 2 dBA quieter than the Zotac 5080 Solid (read our review) and was warmer while operating at a significantly lower reported board power during the test. The ASUS cooler is favoring noise here, but is also just not particularly effective. Overall, the performance is fine -- this is more than acceptable and well below throttle territory -- but this does seem to be one of the lower cost coolers for its size.
Acoustic testing is up next. We’ll keep this brief. As a quick positive, our chamber is about to get way better for future testing -- or more accurately, our microphone equipment. We’ve recently learned that with a microphone upgrade, we can bring down the noise floor closer to 5dBA, which is crazy exciting. Currently though, we’re on our more economical equipment.
With the current noise floor of 14-14.5 dBA, we measured the ASUS 5070 Ti Prime at 29.2 dBA with this frequency spectrum plot. We observed one spike at around 300 Hz. Otherwise, it follows what we have seen in other cards. The falloff begins at 2000 Hz, levels briefly at 3500 to 5000, then continues the path. The loudest range is 1000 Hz to 1800 Hz, aside from the 300 Hz spike.
These two added lines show the Zotac 5080 and NVIDIA 5090 FE (beware of scalped prices). The coolers should track about the same as long as the fan RPM is the same between the other 5080 and 5090 alternatives by the same vendor. The NVIDIA card had a spike around 180 Hz and another at around 380. The Zotac card had one in the 430 Hz range. Broadly, the NVIDIA FE is louder than both of these.
This will change in the future as prices move, hopefully, but for now, we don’t recommend buying this card. Honestly, it’s probably just a good idea to wait in general right now. But let’s just recap why that is as quickly as possible:
Broadly speaking, we found that the 5080 performs in the range of 9% to 16% better than the 5070 Ti, depending on the resolution and game. The 7900 XTX is often better than the 5070 Ti in rasterized testing, with the range in our games spanning equivalence to 17% at the high end, but more commonly 6% to 13%. There were some instances of regression for the 7900 XTX vs the 5070 Ti, but as has been the case, it only got really crushed in Black Myth or Cyberpunk-type ray tracing. The 5070 Ti only improves on the 4070 Ti Super by a range of 2.2% to 20%, often 12-16% if you’re looking at the heavier resolutions. That is not a good improvement for a generational jump and the actual street price we expect as compared to the 4070 Ti Super’s original street pricing, especially with the instances below 10% performance. It’s an awful value. Remember that the Super series was regularly in stock for its actual MSRP.
As we’re reviewing this, we can’t see the actual street price of the 5070 Ti in advance and we don’t review the future as we don’t know where the prices will land at launch, but based on what we’ve seen from the 5080 and 5090, our strongest recommendation right now is to just generally wait to buy a video card and let the market calm down.  
Finally, the 5070 Ti is equal to the 4080 Super. We don’t care what the 4080 MSRP was, because the 4080 Super effectively overwrote it. That was $1,000 and was actually available at around $950 to $1,050 for much of its recent life.
For AMD’s part, it really needs to not screw its new GPUs up. Typically, whatever AMD says its price is, it comes down in about 1 quarter, because they usually push it way too high. It’s really disappointing that AMD said it’s not going to be targeting high-end GPUs this generation. 
Overall, we think it’s not a good time to buy a GPU and wouldn’t recommend buying a 5070 Ti.
 Copyright © 2023 GamersNexus, LLC. All rights reserved.GamersNexus.net is Owned, Operated, & Maintained by GamersNexus, LLC. 
Copyright © 2023 GamersNexus, LLC. All rights reserved.GamersNexus.net is Owned, Operated, & Maintained by GamersNexus, LLC."
"NVIDIA","RTX 5070Ti ","MSI GeForce RTX 5070 Ti Gaming Trio OC+ Review","https://www.techpowerup.com/review/msi-geforce-rtx-5070-ti-gaming-trio-oc/",""
"NVIDIA","RTX 5070Ti ","Nvidia GeForce RTX 5070 Ti Review","https://www.techspot.com/review/2955-nvidia-geforce-rtx-5070-ti/","Nvidia released the RTX 5090 and 5080 about a month ago – well, sort of. Some of you, maybe 10, were able to buy an RTX 5090, with about 80 managing to get a 5080. So, technically, they were released. Some of those RTX 5090s have already melted, meaning even fewer of you currently have one, but hey, more are surely on the way.
But putting that launch aside for a second, we now have the GeForce RTX 5070 Ti to consider. This new GPU is a cut-down version of the 5080, using the same GB203 die. The 5070 Ti is actually far more interesting because it still offers a 16GB VRAM buffer, but instead of costing $1,000 – which is an outrageous price for a 16GB graphics card in 2025 – the MSRP is set at $750.
Now, we know Nvidia's MSRP figures are about as accurate as trying to play Fortnite after a few too many drinks, but the 5070 Ti pricing should be a bit more realistic since this product is likely to face competition from AMD. Of course, it might take a few weeks for prices to settle, but if the 5070 Ti can actually hit the $750 MSRP, it will almost certainly be a much better buy than the RTX 5080.
That's because it would cost at least 25% less while packing just 17% fewer cores, though they are clocked 6% lower. There is a 25% reduction in L2 cache, but since both models use a 256-bit memory bus, memory bandwidth has only been reduced by 7% due to the use of 28Gbps GDDR7 memory instead of 30Gbps for the 5080.
In other words, the performance downgrade shouldn't be all that significant. And again, with both models featuring a 16GB VRAM buffer, the $750 price of the 5070 Ti seems far more reasonable. Essentially, the 5070 Ti replaces the 4070 Ti Super, coming in $50 cheaper while offering a slight performance upgrade. Of course, we have benchmarked both models, so let's get into the results.
For testing, Nvidia sent over the MSI Ventus 3X version of the 5070 Ti, which is overclocked above spec. However, for all testing, we ran the card at reference clock speeds, as we do with all models. For those interested, here's how the Ventus 3X performs out of the box when installed inside an ATX case in a 21°C room. After an hour, we recorded a peak core temperature of 64°C and a memory temperature of 76°C, with a fan speed of 1,700 RPM.
This allowed the card to maintain an average core clock frequency of 2,780 MHz while consuming 267 watts. Overall, it's a cool-running graphics card.
We also tested the Gigabyte Aero OC model, which, under the same conditions, peaked at just 59°C with a memory temperature of 64°C – quite a bit cooler than the Ventus 3X. However, the Aero OC is a much larger graphics card. It also achieved an average core clock frequency of 2,820 MHz, a fan speed of 1,250 RPM, and a power draw of 280 watts.
Starting with Marvel Rivals, we see that the 5070 Ti at 1440p performs similarly to the 4080 Super and, therefore, the original RTX 4080, making it just 12% slower than the 5080. It's also 14% faster than the 7900 XT and 17% faster than the 4070 Ti Super, providing a reasonable performance boost over the model it replaces.
At 4K, it remains 12% slower than the 5080 and again delivers 4080 Super-like performance, making it around 20% faster than the 4070 Ti Super and 7900 XT.
Next, we have Stalker 2, where the 5070 Ti is much less impressive – something we also observed with the 5080. Here, the 5070 Ti was just a single frame faster than the 4070 Ti Super at 1440p, making it only 8% faster than the 7900 XT. Interestingly, it was also just 11% slower than the 5080.
At 4K, it is only 5% faster than the 4070 Ti Super and 11% faster than the 7900 XT, making these results fairly disappointing.
The new GeForce 50 series has been generally underwhelming in Counter-Strike 2 at 1440p, and the 5070 Ti is no exception, beating the 4070 Ti Super by a slim 1.5% margin while being 8% slower than the 7900 XT.
We do see better results at 4K, but even here, the 5070 Ti is just 10% faster than the 4070 Ti Super and a mere 5% faster than the 7900 XT, so it's not particularly exciting.
In God of War Ragnarök, the 5070 Ti is 12% faster than the 4070 Ti Super at 1440p, as well as the 7900 XT.
At 4K, the lead extends to 17%, which, while not quite the 20% margin that would be more substantial, is still one of the better results we've seen so far.
For some reason, the GeForce 50 series has been underwhelming in Delta Force, and as a result, the 5070 Ti is actually 4% slower than the 4070 Ti Super and slightly slower than the 7900 XT.
Increasing the resolution to 4K helps somewhat, but even then, the 5070 Ti is just 4% faster than the 4070 Ti Super, meaning performance with this new generation remains virtually unchanged in this game.
The Star Wars Jedi: Survivor results are extremely unimpressive, with the 5070 Ti being just 9% faster than the 4070 Ti Super and 11% faster than the 7900 XT.
As usual, the 4K data is slightly more favorable, but even then, the 5070 Ti is just 12% faster than the 4070 Ti Super.
A Plague Tale: Requiem provides some of the more favorable data for the 5070 Ti. At 1440p, it is 20% faster than the 4070 Ti Super, 24% faster than the 7900 XT, and just 11% slower than the RTX 5080.
Oddly, however, performance slips slightly at 4K, with the 5070 Ti leading the 4070 Ti Super by 17% and the 7900 XT by 19%, while trailing the 5080 by a 13% margin.
Cyberpunk 2077 delivers another underwhelming 1440p result, with the 5070 Ti providing just a 9% uplift over the 4070 Ti Super, or 10% over the 7900 XT.
That said, the 4K results are somewhat more impressive – though not outstanding – showing an 18% performance increase over the 4070 Ti Super and a 14% lead over the 7900 XT.
The Dying Light 2 results are more positive, showing a 19% improvement at 1440p, making the 5070 Ti 17% faster than the 7900 XT.
At 4K, the margin extends to 25% over the 4070 Ti Super and 27% over the 7900 XT, making it one of the stronger results we've seen.
The Dragon Age: The Veilguard results are less promising. Here, the 5070 Ti somehow trails the 4070 Ti Super by 3%, making it just 9% faster than the 7900 XT.
That said, at 4K, it manages to edge out the 4080 Super, coming in 13% slower than the RTX 5080 but 11% faster than the 4070 Ti Super.
The 5070 Ti struggled to show significant improvement over the 4070 Ti Super in War Thunder at 1440p, delivering just 5% more performance, with an average of 378 FPS.
That margin increased to 10% at 4K, where it delivered performance comparable to the 7900 XTX, while trailing the RTX 5080 by just 10%.
Spider-Man Remastered showed almost no performance difference between the 5070 Ti and the 4070 Ti Super at 1440p, with the newer model being only 1% faster.
Even at 4K, the 5070 Ti delivered just a 5% performance uplift, though that did make it 14% faster than the 7900 XT.
In Hogwarts Legacy, the 5070 Ti was 12% faster than the 4070 Ti Super at 1440p, though it remained 15% slower than the 7900 XT.
However, at 4K, the 5070 Ti was able to match the 7900 XT and the RTX 4080, making it 14% faster than the 4070 Ti Super.
Performance in The Last of Us Part I was disappointing. The 5070 Ti was just 5% faster than the 4070 Ti Super at 1440p and 4% faster than the 7900 XT.
At 4K, it managed to beat both the 4070 Ti Super and 7900 XT by a 12% margin – consistent with other results we've seen.
Testing Star Wars Outlaws at 1440p showed only a 5% gain for the 5070 Ti over the 4070 Ti Super, though that did make it 19% faster than the 7900 XT.
Even at 4K, performance gains remained weak, with just a 6% uplift, averaging 36 FPS.
Lastly, in Starfield, the 5070 Ti was just 9% faster than both the 4070 Ti Super and the 7900 XT at 1440p, averaging 85 FPS – far from impressive.
At 4K, performance worsened relative to expectations, with the 5070 Ti just 4% faster than the 4070 Ti Super. It was, however, 10% faster than the 7900 XT, though even that result was disappointing.
Although we didn't include 1080p performance data in the benchmarks above, we did run the tests and have the average results for reference. Let's take a closer look.
Now at 1440p, the 5070 Ti was just 7% faster than the 4070 Ti Super and 8% faster than the 7900 XT, while being 12% slower than the 5080. Relative to the 5080, this looks decent, but compared to the model it replaces, the results are underwhelming.
The 1440p results mirrored the general trend seen across the benchmarks, with the 5070 Ti providing only modest gains over its predecessor.
As observed in many cases, the 4K data was slightly more favorable. The 5070 Ti was now 11% faster on average than the 4070 Ti Super, 14% faster than the 7900 XT, and 13% slower than the RTX 5080. While it stacks up relatively well against the 5080, its performance remains weak compared to the models it is replacing.
For power consumption measurements, we are using the MSI RTX 5070 Ti Ventus 3X OC model, which has been manually downclocked to match Nvidia's official specifications, as we test all GPUs at reference clock speeds. However, MSI may have adjusted settings such as voltages, which could affect efficiency. This likely explains why the 5070 Ti consumes as much power as the RTX 5080 in our testing.
In short, the 5070 Ti uses roughly the same amount of power as the RTX 4080 and 4070 Ti Super, making it significantly more efficient than the Radeon RX 7900 XT.
Now, let's take a look at ray tracing performance. We'll start with Metro Exodus Enhanced at 1440p, where the 5070 Ti is 13% faster than the 4070 Ti Super, indicating that RT performance hasn't seen a major improvement.
At 4K, we see a 24% performance increase, though gains of over 20% in rasterization testing were rare.
The Alan Wake II results are disappointing. At 1440p, the 5070 Ti was only 2% faster than the 4070 Ti Super.
At 4K, the margin increased to 11%, which is better, but the card only managed 31 FPS with upscaling enabled – making it difficult to consider this a playable experience.
Cyberpunk 2077 delivered even more disappointing results. In fact, at 1440p, the 5070 Ti was slightly slower than the 4070 Ti Super, albeit by just one frame.
Even at 4K, performance remained similar, with the 5070 Ti matching the 4070 Ti Super while being 11% slower than the original RTX 4080. These are undeniably poor results for Cyberpunk 2077.
At 1440p, the 5070 Ti is CPU-limited, allowing it to match the RTX 5080 but making it just 8% faster than the 4070 Ti Super.
Increasing the resolution to 4K helps somewhat, with the 5070 Ti now 16% faster than the 4070 Ti Super. While not an outstanding result, it is at least an improvement over the 1440p performance.
Testing Dying Light 2 at 1440p showed the 5070 Ti outperforming the 4070 Ti Super by 13%, while also being 13% slower than the RTX 5080.
At 4K, the margins increased slightly, with the 5070 Ti now 17% faster than the 4070 Ti Super and 14% slower than the RTX 5080.
Finally, in Black Myth: Wukong, the 5070 Ti was just 6% faster than the 4070 Ti Super at 1440p, averaging 57 FPS – even with quality DLSS upscaling enabled.
At 4K, the experience is even worse, with the 5070 Ti delivering a mere 31 FPS with very high-quality ray tracing and quality DLSS upscaling. This is just 7% faster than the 29 FPS recorded on the RTX 4070 Ti Super, making for an equally unplayable experience.
Although we did not include individual 1080p results in the per game breakdown, we do have the average data with quality upscaling enabled here.
At 1440p, the 5070 Ti was only 6% faster than the 4070 Ti Super on average, which is an extremely underwhelming result. It was also 9% slower than the RTX 4080 and 14% slower than the RTX 5080.
Unfortunately, the 4K results are not much better. The 5070 Ti was just 13% faster than the 4070 Ti Super and 9% slower than the 4080 Super, averaging only 51 FPS.
Overall, the 5070 Ti does little to advance ray tracing performance. The technology still requires significant hardware investment to deliver a truly enjoyable experience.
We have several pricing breakdowns to go through, starting with the official MSRP data, which, at least on Nvidia's side, has been highly unreliable in recent times. This is a major issue because if the RTX 5070 Ti were readily available at its $750 MSRP, it would be a solid option in today's market. It would offer the same level of value as the 7800 XT while providing significantly better ray tracing performance and DLSS support.
Additionally, it would represent a 16% improvement in cost per frame compared to the 4070 Ti Super – not groundbreaking for a next-gen product, but still a reasonable step forward. If you were in the market for a graphics card in the $800 price range, the 5070 Ti would be an exciting option. However, that's only if it were actually available at $750, which seems highly unlikely.
But, let's stick with Nvidia's $750 pricing narrative and compare it to the best GPU pricing available in mid-2024. As shown, the 5070 Ti holds up well, beating the value of the 7900 XT, essentially wiping that product out. It also offers 14% better value than the 4070 Ti Super, which is a decent improvement – certainly not groundbreaking after 12 months, but still a step in the right direction. And in the GPU market, progress has been hard to come by.
While there will be listings for the RTX 5070 Ti at $750, stock is expected to be extremely limited. Most models will be priced well above MSRP, with typical asking prices closer to $900. This mirrors the situation with the RTX 5080, which has a supposed MSRP of $1,000 but actually sells for at least $1,200, and the 5090, which is marketed at $2,000 but is realistically closer to $2,500.
Adjusting for these real-world prices – $1,200 for the RTX 5080 and $900 for the RTX 5070 Ti – the 5070 Ti offers the same value as the 4070 Ti Super. In other words, there is no improvement in cost per frame, which is exactly what we've come to expect from Nvidia. That said, when compared to the 7900 XT, the 5070 Ti still looks like a strong option – though mostly because AMD botched the 7900 XT's launch pricing so badly.
Ultimately, what this data tells us is, if the RTX 5070 Ti sells for $900 or more, it's a complete flop It would have been significantly better to buy an RTX 4070 Ti Super a year ago, or even an RTX 4070 Ti two years ago. Having an extra two years of use from your $800 purchase adds far more value than waiting for a card that only delivers 20% more performance at the same price.
The situation in Australia (and many other places we can assume) is even worse. Based on current retail pricing, the 7900 XT is slightly more competitive, but not by much – certainly not enough to make it a recommended option, given that it only improves cost per frame by 6% compared to the 4070 Ti Super.
The RTX 5070 Ti is supposed to have a recommended retail price (RRP) of $1,509 AUD. While some base models may be listed at that price, stock is extremely limited, and many retailers won't even have those models available. Instead, the cheapest model most buyers will find will cost 8% more, at around $1,630 AUD – though in reality, most models will be priced even higher.
What this means is that in Australia, even the least expensive RTX 5070 Ti models come in at an 11% increase in cost per frame over existing 4070 Ti Super stock. In other words, 23% more money, for 11% more performance, so that's crap.
Even at $1,509 AUD, the pricing is bad. A direct conversion from $750 USD to AUD equals around $1,180 AUD. After adding taxes, the price should be just under $1,300 AUD – essentially the current price of the 4070 Ti Super. Yet for some reason, another $200 AUD has been tacked on.
So there you have it – if sold at the $750 MSRP, the new GeForce RTX 5070 Ti would be a reasonable purchase, at least in the current market. Setting opinions aside for a moment, these are the facts: compared to the 4070 Ti Super at 4K, the 5070 Ti is, on average, 11% faster, with margins reaching up to 24% in our testing. However, gains of 20% or more were rare, as the 11% average suggests.
Ray tracing performance follows a similar pattern, averaging 13% faster with margins reaching up to 24%. In general, the 5070 Ti is typically less than 15% faster than the 4070 Ti Super.
For those still using an RTX 3070, for example, the 5070 Ti offers, on average, 103% greater performance at 1440p and 155% greater performance at 4K – a massive upgrade. However, the MSRP has increased by 50%, or 22% when adjusted for inflation. The biggest advantage is getting twice as much VRAM, which is particularly important given that the RTX 3070's 8GB buffer is frequently maxed out in modern games.
Of course, all of this is based on the $750 MSRP, which, at least at launch, is unlikely to be a reality – and may not be for several months. It will be interesting to see if AMD can capitalize on this and effectively push the 5070 series out of relevance. However, given AMD's track record, that seems unlikely. Still, this is the best opportunity they've had to strike in years.
There's not much more to say about the 5070 Ti. At $750, it would be a decent deal – not amazing or particularly exciting, but still a good option. However, if forced to pay over $800 it's no longer attractive, and a regretful purchase for those two didn't just snap up a 4070 Ti Super a year ago.
For now, we'll have to wait and see where pricing settles, likely in a few months, and, of course, what AMD ultimately brings to the table in response.

About
Ethics Statement
Terms of Use
Privacy Policy
Manage Ad Consent

TECHSPOT : Tech Enthusiasts, Power Users, Gamers
TechSpot is a registered trademark and may not be used by third parties without express written permission.
TechSpot is represented by Future PLC. Learn about advertising opportunities.
© 1998 - 2025 TechSpot, Inc. All Rights Reserved."
"NVIDIA","RTX 5070Ti ","Nvidia GeForce RTX 5070 Ti review: A proper high-end GPU","https://www.tomshardware.com/pc-components/gpus/nvidia-geforce-rtx-5070-ti-review-asus","When you purchase through links on our site, we may earn an affiliate commission. Here’s how it works.

The Nvidia RTX 5070 Ti takes the Blackwell architecture and GB203 GPU and trims down performance and price while keeping the 16GB of VRAM. It's a great 1440p native solution, or 4K with upscaling, but not a massive generational performance increase — unless you want to count the marketing-heavy multi-frame generation.
Minor generational improvement vs 4070 Ti Super
Questions about retail pricing and availability

Why you can trust Tom's Hardware




Our expert reviewers spend hours testing and comparing products and services so you can choose the best for you. Find out more about how we test.

The Nvidia GeForce RTX 5070 Ti marks the third entry for the Blackwell RTX 50-series GPUs, which Nvidia officially unveiled at CES 2025 in early January. It takes over from both the original RTX 4070 Ti as well as the newer RTX 4070 Ti Super, with the latter having replaced the 4070 Ti last January. The 5070 Ti looks better against the 4070 Ti, while it's more of an incremental upgrade from the 4070 Ti Super. The good news is that its MSRP is also $50 cheaper than the launch prices on the 4070 Ti / Super cards, coming in at $749. Faster, cheaper, and new features is a great way to make it onto our list of the best graphics cards — assuming there's sufficient supply, which remains to be seen.

Nvidia is allowing reviews of the base-MSRP 5070 Ti models today, with higher-priced variants tomorrow (and after). The official launch date for all RTX 5070 Ti cards is February 20, 2025, as well, so you can't buy one until tomorrow. We were told that there should be quite a lot more RTX 5070 Ti cards at launch than either the RTX 5090 or RTX 5080, but even so, we anticipate the first batch at least will sell out quickly, and prices will likely head north in the short term before coming back down (we hope).
We've written a lot of supplemental coverage about Nvidia's new Blackwell RTX 50-series GPUs. If you want a primer, or additional information, check out these articles:

• Blackwell architecture
• DLSS 4, MFG, and full RT testing
• Neural rendering and DLSS 4
• RTX 50-series Founders Edition cards
• RTX AI PCs and generative AI for games
• Blackwell for professionals and creators
• Blackwell benchmarking 101
You might think that a month after the RTX 5090 launch, things would have quieted down, but that's not really true. In addition to trying to test some third-party AIB (add-in board) cards, we've had several new drivers, the 5080 launches, and unfortunately, one of our sample AIB cards has been finicky (to say the least). We're still looking for a solution, which may involve sending the card back for a different sample. The 50-series launch hasn't gone smoothly, in other words.

We did a lengthier deep-dive into DLSS 4 and MFG, using the 5080 and 5090, to get a better understanding of what the tech does and doesn't offer. The short summary is that MFG is a lot like framegen, with even more marketing hype. When it gives a straight doubling in performance, from 60 to 120 fps, or from 120 to 240 fps in the case of MFG, you can make a good argument that it looks and feels ""better."" But typically, it's more like a 50~80 percent improvement, making comparisons more difficult, and if it's less than a 50% increase, it can end up feeling worse.

We haven't had sufficient time to do additional MFG testing on the 5070 Ti (yet), but we'll flesh out that section of this review in the coming days. While the official Nvidia party line is that MFG makes the 5070 Ti ""twice as fast"" as the previous generation, that's a gross exaggeration. It might be able to spit out twice as many frames — 1x rendered and 3x generated — compared to the 4070 Ti, but the actual feel of games with MFG doesn't improve nearly as much as those inflated numbers would suggest.

For additional information about Nvidia's latest Blackwell GPUs, check the links in the boxout. The RTX 5070 Ti continues the same pattern, just with fewer GPU cores and less performance than the 5080 and 5090. It offers the same feature set as the other Blackwell GPUs, like FP4 number format support (for AI) and MFG (Multi Frame Generation) for gaming. It also has the same 16GB of GDDR7 memory as the 5080, though the memory is clocked slightly lower. Let's start with the specs table and discuss how it stacks up.
In terms of raw specs, the big changes are in memory speed (and capacity versus the 4070 Ti) plus the FP4 tensor support. There are other architectural differences that we've discussed elsewhere, like how the CUDA cores are now all ""full citizens"" and support both FP32 and INT32 (only half the cores in Ada and Ampere supported INT32), and there are other changes to support new neural rendering techniques. But forget all that for a moment and just look at the die sizes and transistor counts.

RTX 5070 Ti offers a decent step up from the original 4070 Ti, but compared to the 4070 Ti Super, it looks extremely similar. They both have the same number of transistors and die size, using the same TSMC 4N process node. The transistor counts aren't necessarily 100% accurate, as they're more of a rough estimate, but it does suggest that the architectural changes may not be that significant. It's also possible some things were dropped or simplified to make room for new features. What those things might be is more difficult to pin down.

Getting back to the specs, total compute performance looks basically the same as the 4070 Ti Super: 44 TFLOPS FP32 and 352–353 TFLOPS FP16 on the tensor cores. Clock speeds are also lower, on paper, with the 5070 Ti. As usual, however, Nvidia's stated boost clocks are quite conservative estimates, and most of the games we tested ran at much higher speeds.

Memory speed and bandwidth are both quite a bit higher with the 5070 Ti. It's 33% faster than the 4070 Ti Super in speed and bandwidth and 78% higher in bandwidth compared to the 4070 Ti, thanks to having a 33% wider interface. And, of course, it also offers 33% more VRAM capacity than the 4070 Ti.

We tossed a few earlier GPUs from the 30- and 20-series into the table for reference. If you're still using something like a 3070 Ti or 2070 Super, the 5070 Ti should offer a pretty massive improvement in performance. Is it enough to get people to upgrade? For some, we're sure the answer will be yes, but if you're already sporting something like an RTX 4070 Ti Super or even an RTX 4070, this will likely be an easy generation to sit out.
Basically, you get improved AI features, including MFG support. You also get upgraded display outputs (DisplayPort 2.1 UHBR20) and a PCIe 5.0 x16 interface, though neither of those is likely to matter much for most consumer workloads. Power requirements are a bit higher than the 4070 Ti Super, with a TGP of 300W compared to 285W on the prior generation. (TGP stands for ""Total Graphics Power,"" and it's what Nvidia calls the power of the entire graphics card, including all components. AMD uses the term TBP for Total Board Power, which ends up being mostly the same thing with a few minor nuances.)

You also get the much-disparaged 16-pin power connector, with a 3x 8-pin adapter, should you need it. This is the newer 12V-2x6 standard, but even that hasn't proven 100% reliable with the 50-series cards. The 5070 Ti shouldn't have any meltdown issues, considering it's only rated for 300W. That's better than the 5090, which seems to be following at least partially in the footsteps of the 4090. Pulling 600W through such a design just seems ill-advised, and the lack of more advanced monitoring functions hasn't helped matters.

Nvidia takes a staggered approach to GPU launches every generation, typically starting at the top and working its way down. The 5070 Ti is the third card from the Blackwell series, using the same GB203 GPU as the 5080 but with only 83% of the GPU cores enabled. Building redundancy and the ability to down-bin chips into a GPU design is common practice, so while the 5080 requires a basically perfect chip that's fully enabled, anything that can't meet the criteria can potentially be used in a 5070 Ti — and in the future, we'll undoubtedly see some 5070 or even 5060 Ti variants that use ""leftover"" GB203 chips.

Performance on paper may be up to 17% slower than the 5080, give or take, for 25% less money. And for games and tasks that are more memory dependent, the gap may only be 5~10 percent. At lower resolutions and settings, everything starts to become CPU and platform-limited, but we don't think most people buying a $750 to $1,000 GPU will want to run such low settings.

Let's check out the Asus RTX 5070 Ti Prime in more detail before hitting the benchmarks.

Current page:

Introducing the Nvidia GeForce RTX 5070 Ti


Jarred Walton is a senior editor at Tom's Hardware focusing on everything GPU. He has been working as a tech journalist since 2004, writing for AnandTech, Maximum PC, and PC Gamer. From the first S3 Virge '3D decelerators' to today's GPUs, Jarred keeps up with all the latest graphics trends and is the one to ask about game performance.

Tom's Hardware is part of Future US Inc, an international media group and leading digital publisher. Visit our corporate site.

©
Future US, Inc. Full 7th Floor, 130 West 42nd Street,
New York,
NY 10036."
"NVIDIA","RTX 5080","Are bigger gaming laptops really better? Here's how an RTX 5080 beat an RTX 5090","https://www.laptopmag.com/laptops/gaming-laptops-pcs/nvidia-rtx-5080-vs-rtx-5090-gaming-laptops","When you purchase through links on our site, we may earn an affiliate commission. Here’s how it works.

When shopping for the best gaming laptops, the expectation is that when it comes to GPUs, bigger is better. An Nvidia RTX 5090 GPU should consistently outperform an RTX 5080. That’s just how the CUDA cores crumble.
But now and again, we get a system that proves the exception to the rule.
In this case, we’re talking about the MSI Raider 18 HX AI, which features the powerful combo of an Nvidia RTX 5080 GPU and an Intel Core Ultra 9 285HX processor.
Due to the Raider’s ability to leverage up to 260 Watts of power for its CPU and GPU, it easily overpowered the RTX 5090 in the Razer Blade 16 (2025) and HP Omen Max 16.
One of the benefits of an 18-inch desktop replacement gaming laptop is that its size gives it room for a proper cooling system that can leverage as much power as possible to the CPU and GPU, which gets you the kind of performance that justifies ditching your desktop PC for a laptop instead.
The trade-off is that you don’t expect it to be portable. So if it’s more than an inch thick and weighs almost 8 pounds, we don't recommend commuting daily with it.
Sign up to receive The Snapshot, a free special dispatch from Laptop Mag, in your inbox.
But returning to the advantages, we ran the MSI Raider 18 HX, Razer Blade 16 (2025), and HP Omen Max 16 through multiple game benchmarks spanning the gamut of optimized performance, older titles, and GPU-melting new releases.
When it comes to 1080p gaming performance, the MSI Raider’s RTX 5080 GPU displayed a significant performance lead over both RTX 5090 systems.
The Raider took a full 33-frame lead on the Blade 16 in Assassin’s Creed: Mirage, and a 28 fps lead on the Omen Max 16 in Black Myth: Wukong using Cinematic graphics preset.
In fact, on most of the 14 games that we use for graphics testing, the Raider’s RTX 5080 outperformed both RTX 5090 laptops at 1080p resolution. The exceptions include Assassin’s Creed: Shadows, Dirt 5, and Metro Exodus Enhanced Edition at the Extreme graphics preset.
But for notoriously intensive games like Black Myth: Wukong, Cyberpunk 2077, Monster Hunter Wilds, and Red Dead Redemption II, the Raider once again surpassed its 5090 counterparts.
3DMark Fire Strike Ultra (Higher is better)
3DMark Time Spy Extreme (Higher is better)
Assassin's Creed: Shadows (Ultra High, 1080p, fps)
Black Myth: Wukong (Cinematic, 1080p, fps)
Metro Exodus Enhanced Edition (Extreme, 1080p, fps)
Red Dead Redemption II (Ultra, 1080p, fps)
Of course, that’s just FHD performance. At the native resolution of the three laptops, the RTX 5090 tended to do far better. Of course, the Raider’s max resolution is 3840 x 2400, while the Omen Max 16 and Blade 16 cap out at 2560 x 1600p. But we’ll use the Gigabyte Aorus Master 16 as a stand-in for the Raider.
The Aorus features a 1600p display, an RTX 5080 GPU, and a similar Intel Core Ultra 9 275HX processor. While they’re not identical CPUs, and the Raider includes more system memory, their 1080p benchmarks are in the same ballpark.
With that said, the RTX 5090 takes the lead over the lower-spec GPU when it comes to higher resolution.
We’ve seen CPU bottleneck issues with Nvidia’s high-end cards over the last few generations of gaming laptops, where performance throttles at 1080p due to the communication between the CPU and GPU when rendering frames. Some of that is clearly at work here with the 1080p performance.
However, even at a higher 1600p resolution, the RTX 5080 is still competitive. The Gigabyte Aorus Master 16 was just about as fast as both RTX 5090 laptops on Assassin’s Creed: Mirage and Assassin’s Creed: Shadows, and that trend holds even with Black Myth: Wukong and Cyberpunk 2077.
The only games where we saw a significant performance gap between the RTX 5080 and RTX 5090 were Dirt 5 and Far Cry 6.
On Dirt 5, both 5090 laptops outperformed the Gigabyte by at least 55 fps. But on Far Cry 6, the RTX 5080 in the Gigabyte outperformed both RTX 5090 laptops by 9-19 fps.
Assassin's Creed: Mirage (Native resolution, fps)
Assassin's Creed: Shadows (Ultra High, Native resolution, fps)
Black Myth: Wukong (Cinematic, Native resolution, fps)
Metro Exodus Enhanced Edition (Extreme, Native resolution, fps)
Monster Hunter Wilds (Native resolution, fps)
Red Dead Redemption II (Ultra, Native resolution, fps)
Shadow of the Tomb Raider (Native resolution, fps)
It’s time to choose between power and portability.
If you want the ultimate gaming experience, you’ll need a desktop replacement like the RTX 5090 version of the MSI Raider 18 HX. (It’s just $900 more than the RTX 5080 model.)
But if you want all that power while trying not to break your back, get a portable gaming laptop like the Razer Blade 16 (2025). That laptop is just 0.59 inches thick and weighs just over 4 pounds.
Now, if you’re capping your budget at exactly $4,499, you can choose between the portable Razer Blade 16 (2025) with an RTX 5090 or the mighty MSI Raider 18 HX AI with an RTX 5080.
The Blade 16 (2025) offers a slight edge in 1600p performance on some games, but at 1080p, it’s less powerful.
Or you can go for the Raider, which is more powerful at lower resolutions and has an 18-inch, 3849 x 2400, MiniLED display. After all, with Nvidia’s DLSS 4 super sampling and frame generation technology, you can get a fantastic 4K gaming experience with the Raider.
So, unless you’ve got to haul your gaming rig to a weekly LAN party, the Raider gets you more power for your dollar and an overall better gaming experience.
A former lab gremlin for Tom's Guide, Laptop Mag, Tom's Hardware, and TechRadar; Madeline has escaped the labs to join Laptop Mag as a Staff Writer. With over a decade of experience writing about tech and gaming, she may actually know a thing or two. Sometimes. When she isn't writing about the latest laptops and AI software, Madeline likes to throw herself into the ocean as a PADI scuba diving instructor and underwater photography enthusiast.
Please logout and then login again, you will then be prompted to enter your display name.

Laptop Mag is part of Future plc, an international media group and leading digital publisher. Visit our corporate site.

©
Future Publishing Limited Quay House, The Ambury,
Bath
BA1 1UA. All rights reserved. England and Wales company registration number 2008885."
"NVIDIA","RTX 5080","Review: NVIDIA GeForce RTX 5080 Founders Edition (reference)","https://www.guru3d.com/review/review-nvidia-geforce-rtx-5080-founders-edition-reference/","Graphics cards 1096
 Page 1 of 32 Published 2025-01-30 15:06 by Hilbert Hagedoorn
The GeForce RTX 5080 represents NVIDIA’s effort to deliver a graphics card that balances performance and cost. Priced at $999, it is positioned as a more accessible option within the Blackwell GPU lineup set for release between late January and early February. This model offers 84 streaming multiprocessors, totalling 10,752 CUDA cores, which is approximately half the core count of its higher-tier counterpart, the RTX 5090. Although it provides fewer cores, the RTX 5080 is built on the same 4nm process technology at TSMC, ensuring efficient power consumption for both gaming and demanding computational tasks. With a two-slot design, the RTX 5080 is suitable for a variety of system configurations, making it appealing for enthusiasts and professionals who require solid performance without extending to the top-tier budget.
The RTX 5080 features 16 GB of GDDR7 on a 256-bit bus, with memory speeds reaching up to 30 Gbps. This capacity and speed support smooth performance in modern gaming titles and resource-intensive applications such as 3D rendering or data analysis. The GPU’s transistor count stands at 45.6 billion on a 378 mm² die, reflecting NVIDIA’s engineering focus on maximizing efficiency and raw computational power. Further enhancing its capabilities, NVIDIA pairs this model with DLSS4, an AI-driven upscaling technology that leverages dedicated tensor cores. By refining both performance and image quality, this feature allows the RTX 5080 to handle demanding tasks at higher frame rates and resolutions than previous generations.
The naming convention for NVIDIA’s Blackwell series pays tribute to mathematician and statistician David Blackwell, whose significant contributions in probability and information theory laid foundational principles for complex computations. By incorporating cutting-edge technology alongside architectural improvements, the RTX 5080 is designed to cater to users who need robust performance in areas such as professional content creation, machine learning experiments, and high-resolution gaming. While the RTX 5090 may be the flagship choice for uncompromised power, the RTX 5080 still offers a blend of efficiency and capability at a more modest price point.
Built on the Blackwell GB203 GPU, the RTX 5030 sports a large die size compared to its predecessor. It integrates the GB203 GPU with 84 (84 active) Streaming Multiprocessors, giving it 10752 shader cores. Enabled with 10752 KB L1 Data Cache/Shared Memory and 65536 KB L2 Cache Size the card has 112 ROPs and 336 Texture units. The GPU is tied towards 16GB DDR7 memory clocking in at 30 Gbps (960 GB/sec).
Memory technology also sees a leap forward. The RTX 5000 series features 16 GB of GDDR7 VRAM with a 256-bit interface, running at healthy 30 Gbps. This setup offers a total bandwidth of up to 960 GB/s. A larger cache and new memory compression techniques enhance data flow and efficiency. These improvements support high-resolution gaming, real-time rendering, complex simulations, and more, promising smoother performance across various applications like 4K gaming and professional design. 
The GeForce RTX 5080 has a Total Board Power rating of 360 Watts. While this represents the maximum draw under full load, typical power consumption during gaming is expected to be somewhat lower, something we'll check out in this review. The Founders Edition model will feature at least a dual-slot cooler design to manage the intense thermal output effectively. Anyway, are you ready for the review? Good, next page please where we'll show you the product in detail.

 Share this content


 Twitter
                                


 Facebook
                                


 Reddit
                                


 WhatsApp
                                


 Email
                                


 Print
                                



 Review: MSI GeForce RTX 5080 Vanguard SOC Launch edition


Review: MSI GeForce RTX 5090 32G SUPRIM LIQUID SOC 
Copyright ©1997-2025 guru3dPowered by Contentteller® CMS System"
"NVIDIA","RTX 5080","Nvidia GeForce RTX 5080 review: More like a 4080 Super Super","https://arstechnica.com/gadgets/2025/01/nvidia-geforce-rtx-5080-review-more-like-a-4080-super-super/","A good 4K card, a decent value on paper, but not the upgrade it could be.
    
Nvidia's GeForce RTX 4080 graphics card was faster than the RTX 3080 card it replaced. But it was also faster than the RTX 3080 Ti, 3090, and 3090 Ti. One of the good things about a new graphics card generation is that the new cards bring the last generation's inaccessibly expensive high-end performance down to cards that more people can actually afford.
That's not the case with the new $999 RTX 5080, which beats the previous-generation RTX 4080 Super by a little bit and the older RTX 4080 by a slightly larger bit but doesn't come close to beating or even replicating the performance of the outgoing 4090.
Nvidia points to its new DLSS Multi-Frame Generation technology as a mitigating factor here, leaning on its AI-generated frames to close the gap that the 5080's raw rendering performance can't close on its own. And sure, it's nice that this card can do that. On paper, the 5080 is also technically a good value compared to the flagship RTX 5090—between 60 and 75 percent of the performance for half the price (though talking about the MSRP of any of these cards at launch is strictly theoretical, given allegedly short supply and the demand from both actual buyers and scalpers looking to make a buck).
But the 5080 really feels a lot more like a 4080 Super Super—meaningfully better than the 4080 but still in the same general performance category. It's an upgrade, especially for anyone coming from a 30-series or older GPU, but it's a disappointing break from Nvidia's past precedent.
Of Nvidia's mid-generation Super refresh for the 40-series last year, the 4080 Super was already the mildest improvement over the original card, with just a few hundred extra CUDA cores and very mild clock speed increases. Its biggest and most important improvement was that it brought the old 4080's original $1,299 price tag down to a somewhat less offensive (and, again, strictly theoretical) price of $999.
The strategy with the 5080 looks pretty similar. You get a mild increase in core count (up 10.5 percent over the original 4080 and only 5 percent over the 4080 Super), plus the same 16GB of RAM on the same 256-bit memory interface. Switching from GDDR6X to GDDR7 does get you a decent bump to memory bandwidth, though, on the order of just over 30 percent compared to the 4080 and 4080 Super.
But all of that is incredibly mild and incremental compared to the big across-the-board jumps in core count, memory bus width, memory size, and memory bandwidth that you get when you go from the RTX 4090 to the RTX 5090. The 5080's core counts and memory bandwidth also stay well below the RTX 4090's. This has the benefit of not blowing up the 5080's power requirements—at 360 W, it's only 40 W higher than the 4080, and in our actual testing, the 5080 didn't actually consume that much more power on average—but it also means that the 4090 and 5090 continue to stand apart from the rest of the lineup, inaccessible to anyone with less than a couple thousand dollars to spend on a GPU.
As for the physical design of Nvidia's Founders Edition version of the 5080, it's identical to that of the 5090, just as the 4080 looked like the 4090. The 5080 does feel lighter than the 5090, which makes some sense—a card with much lower power consumption and half as many CUDA cores doesn't need the same kind of cooling apparatus. But the dimensions and connectors on both cards are exactly the same.
In our testing, the 5080 ran a little warmer than the 4080 did under load, but only by about 5° Celsius. The 5080 Founders Edition runs 11° or 12° cooler than the 5090 Founders Edition, so it should be a bit easier to keep your system cool with one of these installed.
Our gaming testbed is identical to the one we used for our RTX 5090 review. Compared to reviews from 2023 and 2024, we're using a faster CPU (the Ryzen 7 9800X3D) and a beefed-up power supply. We've also added some newer games to our test suite and discarded a few that weren't challenging the cards enough.
As with the 5090, we focused on 4K tests for this review. The 5080 is half the price of the 5090, but you could still buy a good 1080p gaming PC or a pair of game consoles for the same money—it ought to be capable of playing modern games smoothly at high settings and resolutions. (If you are curious about 1440p performance, look at the benchmarks we've run with DLSS enabled—we run these in ""Quality"" mode, which at 4K means a 1440p internal resolution.)
In our 5090 review, we observed that its performance improvement over the 4090 generally scaled linearly with its CUDA core count and its power consumption—all three usually increased by between 30 and 35 percent, depending on the game.
The 5080 did a bit better than this, with average frame rates that were generally between 10 and 20 percent faster than the regular 4080 and between 5 and 13 percent faster than the 4080 Super. A combination of cores, marginally higher clock speeds, and memory bandwidth increases help give it a boost.
But especially compared to the 4080 Super, these numbers look less like a generational performance increase and more like what you could get out of a 4080 Super with a particularly good overclock. We point this out not because people are upgrading their GPUs every generation but because people with an older 30-series, 20-series, or even 10-series GPU who have been waiting to upgrade won't really be rewarded for waiting for this generation.
DLSS Multi-Frame Generation is meant to bolster the card's performance numbers, and as with the 5090, they do look impressive on paper. As we covered in that review, Multi-Frame Generation (MFG) is exclusive to the 50-series, and it's an amped-up version of the Frame Generation technology that was introduced in the 40 series. Both versions take two rendered frames and use AI to interpolate extra frames in between, sort of like a supercharged version of TV motion smoothing. The 40-series cards can generate one frame this way; the 50-series cards can generate up to three.
Hardware Unboxed on YouTube has a good in-depth look at MFG and its trade-offs, which are mostly the same as the trade-offs with regular FG. It's useful when your rendered frame rate is already fairly high, giving the algorithm plenty of data to use when generating frames and making individual errors or artifacts harder for your eyes to spot. But it's not a cure-all for low frame rates. It can also increase latency because user input is only being polled at your actual rendered frame rate, and running the Frame Generation algorithm actually reduces that frame rate by a bit.
The RTX 5080 is fast enough that it can hit the frame rates needed to make MFG look good, but it will still be game-dependent and scene-dependent. It's a good way to augment rendering performance, but contrary to Nvidia's messaging, it's still not a substitute for better rendering performance.
This is not really the generational leap that GPU buyers were probably hoping for from an RTX 5080. It is the third-fastest GPU on the market, but it's far short of the 4090, and it performs more like a second refresh of 2022's RTX 4080.
The worst thing about the 5080 is that it suggests that every 50-series GPU other than the 5090 will be a very mild upgrade over its 40-series counterpart, with Nvidia leaning heavily on Multi-Frame Generation for the typical generational performance improvements. That will trickle down to cards like the 5070 and 5060 series, the kinds of cards that most people actually buy and use.
Multi-Frame Generation can put up big frame rate numbers, but as with the old Frame Generation, you want a good base frame rate for the best results; improvements to traditional rendering performance still matter. This is OK for the 5080, which is still fast enough to deliver high base frame rates, especially with an assist from DLSS upscaling. But we'd expect this to be a bigger problem for the RTX 5070, which is the only card announced so far to come with fewer CUDA cores than its Super predecessor.
The main takeaway is that the 5080 isn't the kind of generational upgrade that Nvidia has usually delivered up until now, where a new GPU could match or beat the rendering performance of previous-generation cards at higher performance tiers. The days when the GTX 1060 could beat the GTX 980? Long gone. Even in the 40-series, the RTX 4070 could usually match or beat the RTX 3080; that kind of jump looks extremely unlikely this time around.
The best argument for grabbing a 5080 right now, if you can find one at $999 in the first place, is that the only cards that come close to its performance are either way more expensive than they're supposed to be (the 4080 Super) or not cheap enough to justify the Nvidia-specific features you give up (AMD's Radeon 7900 XTX).
Ars Technica has been separating the signal from
          the noise for over 25 years. With our unique combination of
          technical savvy and wide-ranging interest in the technological arts
          and sciences, Ars is the trusted source in a sea of information. After
          all, you don’t need to know everything, only what’s important."
"NVIDIA","RTX 5080","Review: Nvidia GeForce RTX 5080 Founders Edition","https://www.wired.com/review/nvidia-geforce-rtx-5080-fe-review","All products featured on WIRED are independently selected by our editors. However, when you buy something through our retail links, we may earn an affiliate commission.
Nvidia’s newest 50 Series GPUs are slowly trickling out, and as usual, the very top-end RTX 5090 (7/10, WIRED Recommends) is the graphics card that everyone is talking about. It boasts extreme 4K gaming, the latest in AI-powered gaming enhancements, and a power draw to match.
I can’t blame you for not wanting to spend $2,000 on a GPU; that’s enough to build a midrange gaming PC on its own. At just $1,000 for the RTX 5080 Founders Edition, this still-expensive step-down card will be the model that more people seriously consider, even if it's still a splurge. It's a better choice from a performance perspective, meeting people where they already are in terms of monitor resolution, game choice, and existing power supplies.
But how does it fare against the more expensive card, and how does it handle some of the more popular and evergreen games? Well enough to my eyes. If you're building your next high-end gaming PC and are looking for a high-end video card to match, this might be exactly what you're looking for—if you can find one for sale.
The form factor of the RTX 5080 is identical to its more powerful counterpart, with a true two slot design that should fit in most cases very comfortably. I really appreciate the size reduction overall, and I hope AIC cards follow suit.
Where the RTX 5090 draws an immense 575 watts, the 5080 only asks for 360 watts with the same new power connector. Like the RTX 5090 FE, the RTX 5080 includes an adapter, and I imagine most partner cards will as well.
That means a lower overall system power requirement, with Nvidia recommending just 850 watts for the Founders Edition. I expect this will be an easier requirement for existing rigs to meet without needing to buy a new 1,000-watt or higher PSU.
Nvidia introduced a new version of its AI-powered enhancement tools for the RTX 50 Series. These notably add support for multi-frame generation, which uses AI to generate up to three frames between. If you’re interested in learning more about the effects of using this tech on image quality, make sure to check out the RTX 5090 review.
The short version is that multi-frame generation can produce minor artifacts, particularly in areas where two objects at different depths overlap, such as looking through a fence. These are hard to spot across a whole screen though, and the higher frame rate makes the gaming experience much smoother, so the frames are onscreen less time.
I’ll start by checking out performance in Cyberpunk 2077, one of the more demanding games that currently supports multi-frame generation.
All products featured on WIRED are independently selected by our editors. However, we may receive compensation from retailers and/or from purchases of products through these links.
While it isn’t perfect, I’m impressed that the RTX 5080 is able to maintain a relatively smooth 60 frames per second while running at 4K, even without having to turn on multi-frame generation. I’m not averse to using the feature, as it does a great job providing extra smoothness with minimal artifacts, but I’d rather just use the card’s natural horsepower when possible.
I'm just using ray tracing here, with everything on the highest setting, but Cyberpunk 2077 has an additional setting for path tracing that’s even more demanding.
Unsurprisingly, the RTX 5080 has trouble keeping up with path tracing, the most demanding and graphically intensive form of ray tracing. Fortunately, you can break the 60-fps mark by just turning on a single AI-generated frame, which should have a minimal effect on the overall visual quality. I do appreciate the balance here of fine-tuning settings to get close, or turning on frame generation for a big jump in FPS, then comparing the outcomes on a per-game basis.
All products featured on WIRED are independently selected by our editors. However, we may receive compensation from retailers and/or from purchases of products through these links.
The RTX 5090 takes a big lead over the RTX 5080 here, but whether that extra performance is worth it will depend on your existing rig. The RTX 5090 needs a 1,000-watt or higher PSU, and it really benefits from a high-refresh 4K or 1440p screen. If those don’t describe you, know that there are additional costs associated with the highest-end card.
Where the RTX 5090 review focused on the top end of performance, I want to zoom in here on games that are less graphically demanding but more commonly played. I cranked the graphics up to Ultra wherever I could, because that’s just what you do with a new GPU. For this first round I used native resolution, but I’ll discuss supersampling as well.
These are the kind of numbers I’m excited to see as a daily gamer. These games are meat and potatoes for a lot of folks, and especially at 1440p or lower, you won’t need to tweak much to get them running smoothly.
If you’re playing at 1080p and 120 Hz or lower, I recommend supersampling, which lets you render games at 4K and then display them at your monitor’s native resolution. Running Helldivers 2 at 1080p, for example, only used a portion of the GPU’s power. While using Ultra Super-Sampling, the frame rate dropped, but the image quality was much better, and the GPU was running at its full potential.
While it may run on mobile phones, the PC version of Minecraft can still demand a lot from your GPU, particularly with the updated ray-tracing models. If you can run it smoothly, it gives the game an incredible boost from what you’re used to, letting light shine through windows and bounce off different textured surfaces.
In a forum post early in the morning on January 28, just two days before launch, Nvidia warned buyers of limited availability for the 50 Series cards.
“We expect significant demand for the GeForce RTX 5090 and 5080 and believe stock-outs may happen. Nvidia & our partners are shipping more stock to retail every day to help get GPUs into the hands of gamers.”
All products featured on WIRED are independently selected by our editors. However, we may receive compensation from retailers and/or from purchases of products through these links.
If previous releases are any indication, the Founders Editions may be even harder to come by than their AIC counterparts. I hoped they might be more plentiful this time around, but the forum warning doesn’t help. If you want a Founders Edition of either card, I’d make sure to bookmark the purchase pages on multiple retailers and check often for restocks.
In the RTX 5090 review, I mentioned that the card felt more like a showpiece, or a demonstration model for the new generation’s capabilities. I expect to see more RTX 5080s out in the world, and not just because it’s half the price. Over 50 percent of users in the Steam Hardware Survey reported using a 1080p display as their main screen, and less than 5 percent of PC gamers in the study had a 4K monitor.
For a lot of people, having an RTX 5090 would be like buying a Lamborghini when the speed limit is 15 MPH. Meanwhile, the RTX 5080 clobbered basically every commonly played game I threw at it, and its lower power consumption makes it a much easier upgrade path for more people.
The biggest problem may just be getting your hands on one, which is a real shame. The RTX 5080 is the card in the 50 Series that I think will appeal to most people, balancing performance, power requirements, and price. The Founders Edition is a premium example too, with a compact form factor and understated styling.
All products featured on WIRED are independently selected by our editors. However, we may receive compensation from retailers and/or from purchases of products through these links.
© 2025 Condé Nast. All rights reserved. WIRED may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Condé Nast. Ad Choices"
"NVIDIA","RTX 5080","RTX 5080 review: a great GPU that you probably can't get right now","https://www.polygon.com/review/540006/rtx-5080-review-nvidia-graphics-card-gpu","﻿Its AMD competitor seems great too, not that you can get that one either
It’s a strange time to be reviewing an Nvidia graphics card. The company sent me a GeForce RTX 5080 a month and a half ago, and in that time, I’ve played a lot of games and watched a lot of GPU benchmark comparison videos. But also in that time, Nvidia’s longtime competitor AMD (which has historically been the underdog of the two) released its new line of cards, including the Radeon RX 9070 XT, which is shockingly comparable to the RTX 5080 while being significantly cheaper (the 9070 XT retails for $599, while the RTX 5080 costs $999). I figured, maybe by the time I post something, you’d be able to buy either GPU.
Here’s the thing, though — you can’t. Almost as soon as Nvidia’s new cards hit the market in February, they sold out in less than an hour. The AMD cards are also in extraordinarily short supply. More troubling, both the Nvidia and AMD GPUs have ended up selling for significantly more than their retail price, either because of enterprising resellers, or because retailers can and do take advantage of the fact that people are always willing to pay quite a bit more for these cards than the list prices. But even if you were willing to pay several thousand dollars for one of these cards, you still might not be able to get one for months — or longer, it’s hard to say right now. Not that many of these cards exist in the world at this time, seemingly. [Ed note: This is not a new problem.]
I don’t have a 9070 XT to which I can compare the RTX 5080, and if you’re reading this and you care about that comparison, you’ve probably already decided which one you prefer and what you hope to get. You might be the kind of person who has also been watching comparative benchmark videos and is willing to wait however long it takes to purchase the best-performing card on the market.
Or you might be somebody more like me, who purchases what’s available and affordable when I’m in a position to upgrade. I actually bought an RTX 4070 Ti Super last fall with my own money. At that time, rumors suggested that Nvidia would release new cards in early 2025, but I was worried about President Trump’s impending tariffs, and I guessed that the 50-series cards wouldn’t actually be available to purchase for a long time after their debut anyway. It’s why I actually bought all of the parts for a brand-new PC build alongside my new GPU. (Also, at that time I didn’t know Nvidia was going to send me an RTX 5080 to review. One cannot and should not count on such things, even in our line of work.) This PC, for readers’ reference, contains an AMD Ryzen 7 9800X3D 4.7 GHz 8-core processor, a Gigabyte X670 AORUS Elite AX motherboard, 64 GB of RAM, and a 1600 W power supply. I have two 1440p monitors and, unfortunately for testing purposes, no 4K displays.
In addition to feeling desperate to upgrade my PC before Trump took office, lest I make an already expensive upgrade even pricier, I just wanted to play STALKER 2, which didn’t run at all on my previous PC (the one that contained the RTX 3070 I reviewed in 2020, a card that’s slightly below the recommended STALKER 2 system requirements). I’m not saying that STALKER 2 ran poorly on my previous machine. It did not run at all. It wouldn’t even load. Now, the PC version of STALKER 2 wasn’t exactly optimized for success at launch, but I kept on trying after multiple updates, and it didn’t go well. It seemed like an ill portent. Plus, the RTX 3070 was by far the newest component in my PC at that time.
I was planning to circle back to the RTX 3070 as my comparison point for this RTX 5080 review because I figured the differences between it and my RTX 4070 Ti Super would be somewhat negligible, or at least, not very exciting to write about. Instead, I’ve ended up realizing that my RTX 4070 Ti Super — by far the most expensive item in my new PC build — was not actually that great.
I hate to even admit this because I’m still kind of pissed off about it, but here we go. I had actually realized this problem long before the RTX 5080 arrived on my doorstep. Most games looked good on my new PC build, but Indiana Jones and the Great Circle — and let me use a technical term here — looked like dogshit. I could have, and would have, spent more time trying to figure out where the bottlenecking was happening, or why the frame rate in the game would plummet any time I tried to actually turn up the graphics settings at all to take some nice screenshots of what should have been a very pretty video game. Even on normal settings, it still tended to look shimmery and grainy and just plain wrong.
I’m now gratified to learn that I’m not alone in this experience. Googling “RTX 4070 Ti Super Indiana Jones and the Great Circle looks bad” will yield tons of different forum threads on Steam, Reddit, and elsewhere from people having the same problems that I ran into. After a lot of tweaking with settings, I managed to find a sweet spot where the game looked pretty good and the frame rate wasn’t suffering, but I felt confused and disappointed that it didn’t look way better — and that this much effort was required to feel content with the results. After all, I had just upgraded my entire PC, and Great Circle was the first major AAA, graphically intensive game that was released after my big upgrade. It felt bad.
I now feel extremely spoiled by how good my games have looked and performed in 1440p with the RTX 5080. As soon as I plugged it in and installed the new Nvidia drivers, Great Circle looked incredible, no tweaks required. With every setting maxed out, I was still able to average 120-125 frames per second in the game, even in wide-open areas with lots of moving assets on screen (with Nvidia’s DLSS Frame Generation technology on the 4x setting). STALKER 2 fared even better, averaging 160 fps on all “Epic” settings and only dropping to 120 fps when I ran into those massive anomaly areas with tons of stuff floating around in the air.
Perhaps the most satisfying point of comparison for me was Cyberpunk 2077. This is a game with a lot of shiny, reflective surfaces, and I had a blast taking pictures of them all. I compared them to some of my old screenshots when I first played the game, on my previous PC with the RTX 3070 (which was a brand-new card for me when that game was first released), and it’s fun to see the improvements. Cyberpunk 2077 has gotten a big makeover (including DLSS 4 support and path tracing) since it was first released, and now, my PC has, too, and I get to reap the benefits.
All of this said, would I have attempted to purchase an RTX 5080 on my own, had I not been sent one by Nvidia last month? I actually think I might have, but not for a long time — probably a year from now, or longer, once the card actually becomes widely available and affordable. By that point, my personal frustrations with the RTX 4070 Ti Super — itself not a particularly big jump in power compared to the RTX 4070 Ti and the RTX 4070 Super (yes, those are actually two other cards with extremely similar names) — might have motivated me to resell and trade up my GPU. Or perhaps in the intervening time, I would have troubleshooted whatever weird, unlikely bottleneck was happening with my GPU on Great Circle. I have the personal privilege, currently, of not having to figure that out anymore, because I have an RTX 5080 that makes everything look fantastic right out of the box with no work on my part at all. Even if I decide to upgrade to 4K monitors down the line, I’m not too concerned about whether the RTX 5080 can keep up.
For a regular person who’s looking to purchase a GPU right this second, I don’t know that I can recommend the RTX 5080 because you literally can’t buy it right now. I mean, you can certainly try, and if you’re as anxious about impending tariffs as I am, then maybe you should, despite the resale markup you’ll probably face. Even Nvidia’s 40-series cards are currently selling for hefty price tags, and it might be a while before even those cards get cheaper. They probably won’t until the 50-series cards are more widely available, assuming that ever happens.
This is part of why the entire competition between the Nvidia RTX 5080 and AMD RX 9070 XT is a theoretical one, at least for the moment. Unless you’re willing to pay top dollar for a wallet-gouging resale on the exact new card you want, you’re probably going to do what I did last fall, which was to purchase a decently new card that happened to be available at a bearable price during the time period that I was making said purchase. If that somehow ends up being an RTX 5080, I can promise you it’ll make AAA games look fantastic for the next couple of years. Also, I hope you’re surviving whatever is happening to you in the far future where that RTX 5080 was available to buy at a bearable price.
The best of Polygon in your inbox, every Friday."
"NVIDIA","RTX 5080","Nvidia GeForce RTX 5080 review: a new king of 4K is here","https://www.theverge.com/2025/1/23/598045/nvidia-geforce-rtx-5080-review-a-new-king-of-4k-is-here","The RTX 5080 is smaller and a little faster than the RTX 4080 Super — and still $999.
If you buy something from a Verge link, Vox Media may earn a commission. See our ethics statement.
If you buy something from a Verge link, Vox Media may earn a commission. See our ethics statement.
Nvidia’s new RTX 5080 graphics card isn’t as exciting as I was hoping it would be. While the sleek new Founders Edition redesign dramatically shrinks the size of the card compared to the RTX 4080 and RTX 4080 Super, you’re getting the same 16GB of VRAM and only small performance improvements over the previous generation of cards.
The RTX 5080, which arrives January 30th for $999, is about 15 percent faster than the RTX 4080 at 4K without the use of any AI-powered upscaling tricks. While the RTX 4080 managed to beat the RTX 3090, and the RTX 3080 dethroned the RTX 2080 Ti, the RTX 5080 isn’t enough to topple the RTX 4090. Nvidia has built up an expectation that the 80-series card will surpass the previous generation’s flagship, and that’s simply not the case this time around. 
The RTX 5080 does comfortably beat its only competition at this price point: the $999 AMD RX Radeon 7900 XTX. That makes the RTX 5080 a good card for 4K if you’re willing to drop settings in some games and more GPU than you probably need for 1440p. For meaningfully better 4K performance, you’ll have to spend $2,000 on the RTX 5090. 
If you’re upgrading to the RTX 5080 from the RTX 3080 or RTX 3080 Ti, expect to see average performance gains of more than 50 percent at 1440p and 4K. If you want more than that, you’ll have to rely on Nvidia’s new DLSS 4 and Multi Frame Gen to create more frames with AI models. As we work our way down the RTX 50-series lineup, it increasingly feels like Multi Frame Gen is going to be the main talking point.
Sleek two-slot designDLSS 4 and Multi Frame GenGood average power draw in games
16GB of VRAMNot a big performance boost over the RTX 4080
The RTX 5080 Founders Edition model looks identical to the RTX 5090. It’s dramatically thinner than the RTX 4080 it succeeds, shrinking from a three-slot design down to two slots. I really like this redesign, which includes double flowthrough fans that exhaust air above the RTX 5080 into the case instead of out of the top and back of the card.
Just like the RTX 5090, Nvidia has slightly angled the power connector on the RTX 5080 so it’s easier to fit into a variety of cases. The included dongle, which converts three eight-pin PCIe power cables to fit the card’s 12VHPWR connector, has slightly more flexible cables than the one that shipped with the 40-series cards. You can also ditch the dongle and get a 12V-2x6 or 12VHPWR cable that connects directly to your power supply.
It’s disappointing to see Nvidia has stuck with 16GB of VRAM on the RTX 5080. AMD’s RX 7900 XTX offers 24GB, and while the RTX 5080 delivers better performance for now, it may well hit video memory limits in 4K gaming in the future.
For both my 4K and 1440p testing, I’ve paired Nvidia’s RTX 5080 with AMD’s latest Ryzen 9 9800X3D processor and Asus’ 32-inch 4K OLED PG32UCDP monitor. I’ve put the RTX 5080 up against the RTX 5090 — Nvidia’s current flagship — as well as the RTX 4090, RTX 4080 and 4080 Super, AMD’s Radeon RX 7900 XTX, and the RTX 3080 Ti from 2021.
Without DLSS or ray tracing enabled, the results are largely consistent across both 4K and 1440p: the RTX 5080 is about 15 percent faster on average than the 4080 at 4K and 12 percent faster at 1440p. It’s 12 percent faster than the Radeon RX 7900 XTX at 4K and 11 percent at 1440p, and it beats the RTX 4080 Super by 11 and 10 percent, respectively.
That’s enough to make this the best $999 graphics card, but by historical standards, it’s disappointing. I really wanted to see the RTX 5080 beat the RTX 4090, but it lags the older card by about 18 percent. Maybe it was a big ask for a $999 GPU to beat a $1,599 last-gen flagship. The $1,199 RTX 4080 was faster than the $1,499 RTX 3090, but there was only a $300 gap there, not $600. Still, it’s the first time in a long time we’ve had such a paltry increase in an 80-series card.
Nvidia’s top GPU, the RTX 5090, is 50 percent faster than the RTX 5080 at 4K resolution, but it’s also double the price. That certainly leaves the door open for an RTX 5080 Ti, priced between the 5080 and 5090, that can beat the RTX 4090.
The RTX 5080 makes the most sense if you’re upgrading from the RTX 30 series or earlier. At 4K, without DLSS or ray tracing, it’s nearly 54 percent faster than the RTX 3080 Ti (56 percent at 1440p), and you’ll also be able to make full use of DLSS 4 and Multi Frame Gen if you upgrade.
Nvidia claimed the RTX 5080 would be twice as fast as the RTX 4080. For those kinds of numbers, you’ll need to turn to DLSS 4 and Multi Frame Gen.
Nvidia’s new Multi Frame Generation tech uses the latest AI graphics models, powered by an updated transformer architecture, to generate up to three additional frames per traditionally rendered frame, pushing the RTX 5080’s frame rates beyond what it’s normally capable of at 4K. 
Cyberpunk 2077 is the only game with official support for DLSS 4 and Multi Frame Gen before the RTX 5080’s official launch on January 30th. In my testing, I’ve found big frame rate increases at 4K with full ray tracing enabled, with fewer graphical glitches than Nvidia’s previous frame-generation tech thanks to the new transformer model. Without DLSS 4, Cyberpunk runs at an unplayable 19fps average with ultra settings and full ray tracing. DLSS 4 Multi Frame Gen x4 brings that up to a far more playable 128fps, but it isn’t the same as a true 128fps.
DLSS Super Resolution — which renders the game at a lower resolution and then uses AI to upscale it — bumps the frame rate up to 38fps. Multi Frame Gen creates the three extra frames to get to that 128fps mark. While motion does look smoother, in terms of input latency, it still feels like a 38fps game. Multi Frame Gen is far less noticeable when the base frame rate with DLSS Super Resolution is higher than 60fps, so at 4K, you might need to lower the quality settings to really see the benefits.
It’s this Multi Frame Gen that lets Nvidia say the RTX 5080 is twice as fast as the RTX 4080, with the previous-gen card managing 62fps on average in Cyberpunk 2077 with full ray tracing and Frame Gen x2. The RTX 5080 more than doubles this with x4 enabled.
It’s a better story at 1440p thanks to higher base frame rates. Without DLSS 4, Cyberpunk 2077 averages 39fps at 1440p with full ray tracing and ultra settings. DLSS 4 Multi Frame Gen x4 brings that all the way up to 225fps. While it’s still not a true 225fps, it feels a lot better than the 4K equivalent because DLSS Super Resolution gets the base frame rate up to 73fps before Multi Frame Gen kicks in. 
This is the ideal situation for Multi Frame Gen because you’re getting the motion clarity benefits and the game still feels more responsive. It’s obviously not as responsive as 225fps would be without Multi Frame Gen if you really dropped the settings, but in a game like Cyberpunk 2077, I think people are going to notice the motion clarity improvements more than the input latency.
When the RTX 5080 launches on January 30th, there’ll be a new DLSS override feature inside the Nvidia app that lets you force games to use its new Multi Frame Gen and transformer models. I haven’t been able to test this yet, since it’s not available, but if it works, it could be a great way to improve DLSS image quality and Frame Gen in games before developers can officially patch them.
I focus most of my GPU testing on gaming — because that’s what GeForce cards are designed for — but the RTX 5080 is also very capable of video editing or AI workloads. I found that the RTX 5080 was nearly 10 percent faster than the RTX 4080 in PugetBench’s DaVinci Resolve test, and in Procyon’s AI XL (FP16) test, it was nearly 28 percent faster.
Nvidia recommends an 850-watt power supply for the RTX 5080, which is 100 watts more than for the RTX 4080 and 4080 Super. The RTX 5080’s total graphics power is 40 watts more than the RTX 4080; it maxes out at 360 watts instead of the massive 575-watt power draw of the RTX 5090.
I’m glad to see that the RTX 5080 doesn’t significantly increase the power draw over the previous generation. I only saw it hit 360W once, in the Metro Exodus extreme benchmark. Even in Cyberpunk 2077 running full path tracing and no DLSS, it only got up to 348 watts.
More impressively, on average, the RTX 5080 drew 278 watts of power across the nine games tested without DLSS or ray tracing. That’s slightly less than the 281-watt average I found on the RTX 4080 but 12 watts more than the RTX 4080 Super. 
The RTX 5080 didn’t heat up excessively in my open bench testing. The highest temperature I recorded was in Metro Exodus, where it reached 71 degrees Celsius (160 degrees Fahrenheit). The RTX 4080 reached 68C in this same test, and the RTX 4080 Super hit a maximum of 63C.
DLSS 4 is really the star of the show so far with the RTX 50 series, but there still aren’t enough games to test it with. Multi Frame Gen has shown early promise, and it makes a lot more sense at 1440p in Cyberpunk 2077 on the RTX 5080 than it does at 4K. I’m waiting to test more games with DLSS 4 and Multi Frame Gen or even the ability to force these options on with the Nvidia app.
The RTX 5080 is the best $999 card on the market right now, beating AMD’s Radeon RX 7900 XTX by an average of 11 or 12 percent across the games I tested, without DLSS or ray tracing. It’s a significant upgrade over an RTX 30 series or earlier, with over 50 percent higher frame rates than the RTX 3080 Ti, plus access to the latest Multi Frame Generation technique. The 5080 offers about two-thirds the performance of the RTX 5090 for half the price and a little more than half the power draw. 
But Nvidia hasn’t delivered the pure performance gains I was expecting. The RTX 5080 isn’t the cheaper RTX 4090 many were hoping for. I wanted to see a more meaningful bump to 4K performance than just 15 percent over the RTX 4080 without DLSS. I’m left wondering whether Nvidia will introduce an RTX 5080 Ti down the line and how close the upcoming $749 RTX 5070 Ti will get to the RTX 5080.
A weekly newsletter by David Pierce designed to tell you everything you need to download, watch, read, listen to, and explore that fits in The Verge’s universe.
© 2025 Vox Media, LLC. All Rights Reserved"
"NVIDIA","RTX 5080","NVIDIA GeForce RTX 5080 Founders Edition Review & Benchmarks","https://gamersnexus.net/gpus/nvidia-geforce-rtx-5080-founders-edition-review-benchmarks-vs-5090-7900-xtx-4080-more","NVIDIA GeForce RTX 5080 Founders Edition Review & Benchmarks vs 5090, 7900 XTX, 4080, & More
We’re going to provide the bottom-line up front: The NVIDIA RTX 5090 (Be mindful of scalped prices) is anywhere from 30% to 68.9% better than the RTX 5080 (Be mindful of scalped prices) at 4K, depending on game, with results commonly in the 45-55% range. Against the AMD RX 7900 XTX of similar price, the 5080 (Be mindful of scalped prices) is within striking distance. They are commonly within 10% of each other. As for the generational gain of the 5080 vs. the 4080, it depends on the game. We saw 16%-20% in some tests, but we also had several results as low as 7-10%, which is about as boring as possible. 
There’s a ton more nuance to get into, but that gives you some expectations in case you got what you needed and want to dip out. One of the more interesting tests we’ll be getting into though will be efficiency, where the RTX 5080 ends up potentially making a case for itself more than the power-hungry 5090 did. Its lower power budget helps there. The thermal testing is also interesting, considering the 5080 FE drops to paste rather than liquid metal and also makes changes to the heatsink itself.
Editor's note: This was originally published on January 29, 2025 as a video. This content has been adapted to written format for this article and is unchanged from the original publication.
We’re keeping this review really simple and short (by our standards) today. 
First up, we have to disclose some office gambling behind the scenes. Jimmy has bet Jeremy that if the 5080 can beat the 4090 (Be mindful of scalped prices) at any one benchmark, he’s going to get some free tater tots. 
The NVIDIA RTX 5080 has an MSRP of $1,000 and launches on January 30th, along with the $2,000 MSRP RTX 5090. 
The 5070 Ti at $750 and 5070 at $550 will follow in February. Pre-launch pricing on Best Buy indicates that MSRP options will be slim outside of the NVIDIA Founders Edition, so adjust your personal value assessment based on what they actually show up for on launch day. The most expensive one we can see right now is $1,400 – and we’d say don’t buy that one.
The previous generation RTX 4080 launched at $1,200 (watch our review) following the infamous “unlaunch” of the “4080 12GB” – which actually became the 4070 Ti (Be mindful of scalped prices). More recently, the 4080 Super (Be mindful of scalped prices) launched at $1,000, providing a value increase despite its nearly indistinguishable performance from the base 4080.
AMD’s Radeon RX 7900 XTX is the closest price competition for the 5080, which currently starts as low as $830 but is more commonly around $930. Down from that is the 7900 XT, ranging from $650 to $710 and in a decidedly lower price bracket. AMD says its 9070 series cards will be arriving in March.
Intel GPUs haven’t scaled up to this market category yet – that’s a hopeful “yet,” for the record.
And now for a quick overview of the basic specs. For the full details, check out our announcement coverage of these cards.
The Blackwell architecture-based RTX 5080 has 10752 CUDA cores and 16GB of GDDR7 on a 256-bit bus. These specs are way down from the 5090’s configuration – almost exactly half – making for probably the biggest gap there’s ever been between the top 2 tiers on launch day.
The RTX 5080 is also a PCIe Gen5 card, but as our recent PCIe scaling benchmark with the 5090 showed, it barely makes a difference at all in games to run on Gen4 – or even Gen3.
This review will again focus on high-end GPUs, and you can see our test methodology here. As a reminder, our site is free of third-party ads and contains our game settings and test bench. To see the low-end, check out our Intel Arc B570 review.
Under a full workload for the GPU, the RTX 5080’s temperature with auto settings landed at 65-66 degrees Celsius for the GPU. Considering the change to thermal paste from liquid metal, the real impact here to reduce the temperature is in the power consumption reduction. We’ll look at RPM as well, though.
The memory temperature plotted at about 72-75 degrees Celsius for the 5080, which is well within the TjMax restrictions of the memory on the card, which should be about 105 degrees for that.
As for 5080 fan speed, it held about 1,470-1,500 RPM. That makes its acoustic profile similar to what we saw in the 5090 review, so you can check that for noise samples and a frequency spectrum. It’s a little quieter overall.
The 5090’s GPU temperature in this test was 72 degrees Celsius, with the 5090’s memory at 90 -- we weren’t happy about that, despite being within spec. Once in a hotbox case, that 90 degree result becomes a concern. Finally, the 5090’s fan ran about 100 RPM higher when left to its own devices, resulting in a 5080 FE, which despite moving away from liquid metal is not really at a disadvantage. It’s doing okay overall, especially because of the fact that the power consumption is down. 
Frequency validation is up now. This is just to ensure the card is hitting at least the spec set by NVIDIA.
The RTX 5080 clocks in this benchmark held relatively flat, although not perfectly flat, at around 2670 MHz. NVIDIA’s website claims 2.62 GHz boost clock, so it is technically exceeding the minimum spec.
The RTX 4080 ran at about 2775 MHz in this benchmark, consistent with what we saw between the 5090 and 4090 (watch our review). The frequency is higher on the 4080, but the architecture has changed and made this an indirect comparison.
The RTX 5090 bounces around between 2475 to 2670 MHz or so, mostly landing below the 5080. It’s relatively common for the fuller configurations to run lower clocks, but higher core counts.
Time to get into game testing. We’ll keep these short this time since the comparison is relatively simple.
The RTX 5080 landed at 112 FPS AVG, behind the RTX 4090’s 139 FPS AVG result and the 5090’s 182 FPS AVG result. That makes the 5090 faster than the 5080 by a massive 62%. 
AMD’s 7900 XTX directly competes with the 5080, but does allow the 5080 a lead of just under 10 FPS in this benchmark. Lows are about the same between them.
Against the 4080 and 4080 Super, the 5080 is about 15% better.
Generationally by name, the 3080 (watch our review) held a 71 FPS AVG in this benchmark, yielding a 58% uplift to the 5080. The 2080 Ti’s (watch our review) 54 FPS AVG gets basically doubled by the 5080.
As a note, some of these charts have the 1070 (watch our review), 1060 (watch our revisit), 2060 (watch our review), and other older cards on them. The 1080 Ti (watch our revisit) hasn’t been rerun yet this year.
At 1440p, the RTX 5080 held 217 FPS AVG against the 5090’s 317 FPS AVG. We’re not fully CPU-bound here, so the 5090 still has relatively high scaling at 46% ahead of the 5080. For the same comparison, the 4090 led the 4080 original by 36% and the Super by basically the same because they’re basically the same card.
The 7900 XTX (watch our review) remains neck-and-neck with the RTX 5080, with the two functionally identical from a player experience standpoint. The 5080 is technically ahead. The 7900 XTX is the strongest competition to the 5080 in this rasterized test when looking at pricing.
The 4080 and 4080 Super improve from about 200 FPS AVG to 217, or a relatively boring 8-9%.
Down the stack, we have the 7900 XT at 171 FPS AVG, the 4070 Ti at 151 FPS AVG, and the 6950 XT at 150 FPS AVG. The generational comparison to the 3080’s 129 FPS AVG gives the 5080 a 68% improvement, a bit better than observed at 4K.
Finally for Final Fantasy, 1080p has the 5090 at that same impressive 407 FPS number we mentioned last week, with the 5080 at 302 FPS AVG. Despite the heavier CPU load, the advantage remains 35% for the 5090 over the 5080; still, that’s nothing compared to what we saw at 4K.
The 4090 leads the 5080 by 24%, at 376 FPS AVG to 302. The 7900 XTX is closer than ever to the 5080 and is tied in all practical senses. It’s looking good here, especially with the price, but this ignores ray tracing testing that tends to still favor NVIDIA.
Black Myth: Wukong is up next, tested first at 4K and rasterized. This is a heavy workload for these cards, with the RTX 5090 at 86 FPS AVG and holding strong frametime pacing as indicated by the 1% and 0.1% lows. The RTX 4090 is next (at 67 FPS), followed by the RTX 5080 at 58 FPS AVG. This has the 4090 as 16% better than the 5080 and the 5090 as 48% better than the 5080.
The RTX 4080 Super (read our review) and 4080 are functionally identical and give the 5080 an underwhelming 6 FPS lead, which is about 12%. The 5080 also leads the 7900 XTX by 17%, with AMD’s card falling disproportionally behind in this test with its 49 FPS result. Lows on the 7900 XTX are commendable though and show impressive consistency.
Strong prior-gen cards include the 3090 Ti (watch our review) at 45 FPS AVG and 3080 at 36.6 FPS AVG, with the 3080 yielding an uplift of 57% to the RTX 5080 and similar for the 6950 XT (read our review).
At 1440p, the RTX 5090 leads the chart at 130 FPS AVG, making it 23% better than the 4090 and 34% better than the RTX 5080. 
The 5080 and 4080 are disappointingly close at only a 9% improvement for the 5080. The lead over the 7900 XTX also reduces down to 13% here, with lower resolution drawing them closer.
Noteworthy other cards include the 3090 Ti at 74 FPS AVG, the 7900 XT (read our revisit) at 71 FPS AVG (which isn’t bad considering its price at various points in history), the 3080 at 62 FPS AVG, and the 2080 Ti at 45 FPS AVG.
Scaling continued at 1080p, where the 5090 boosts to 160 FPS average from the 130 FPS result at 1440p. The RTX 5080 still trails the 4090, but they get close and the 4090’s advantage is reduced to 6%. 
Generationally, the 5080 leads the 4080 by 7%, down from its lead of 9% at 1440p and 12% at 4K. The 7900 XTX still trails by about 10 FPS here. It’s a shame that AMD decided not to compete at the high-end this generation, as a generational step over the current XTX may have been a serious contender to the 5080. There’s still time though.
Starfield is up at 4K. First, we have to address this absolute roast of a comment from our 5090 review. “I would like to correct one mistake GN made in this review, where they stated that there aren’t many reasons to play Starfield at 1080p but in actuality there aren’t many reasons to play this game at all.”
Ouch. Well, anyway, at 4K, the RTX 5090 held 108 FPS AVG and led the 5080 by 41%. Not the best we’ve seen, but still a large gap. For reference, the 4090’s 92 FPS AVG led the 4080’s 71 FPS by 30%. Comparatively, that makes the gap between the last generation 90 and 80-class cards smaller than the current generation.
Against the 4080, the 5080 is about 5 FPS improved. That’s not very exciting. NVIDIA’s angle will probably be pricing.
The 7900 XTX actually defeats the 5080 on a technicality in this one as well, with frametime pacing equivalent between them and the average FPS functionally identical. There is no perceptible difference between these, which is a benefit to the 7900 XTX in this rasterized scenario.
At 1440p, the 5090 climbs to 147 FPS AVG and leads the 5080’s 110 FPS result by 34%. That reduces the lead versus 4K. The 7900 XTX also climbs with a very slight but irrelevant victory over the 5080, with the lows identical when rounded. 
Against the nearly perfectly matched 4080 and 4080 Super, the 5080 holds an unimpressive 2.5% lead. That’s not very exciting.
Older cards worth highlighting include the 3080 FTW3 (RIP EVGA) at 72 FPS and 2080 Ti at 50 FPS AVG. AMD’s 7900 XT is also noteworthy for its performance when considering some of its historical pricing.
At 1080p, the RTX 5080’s 132 FPS AVG gives the 4090 a lead of 17% and the 5090 a lead of 24%. The 5080 matches the 4080 equally here, with the 4080 Super within error -- just like its entire pointless existence, sort of like Starfield at 1080p.
The XTX also nearly equals the 5080, although NVIDIA has an advantage on a technicality.
Dragon’s Dogma 2 is up now, first at 4K rasterized.
The RTX 5090 ran at 133 FPS AVG and held well-paced lows behind it. Its lead over the 4090’s 98 FPS is 35% here, with the lead over the 5080 at 57%. This is one of the largest gaps thus far. For comparison, the 4090 led the 4080 non-Super by 36%, meaning that the 4090 and 4080 were closer together than the 5090 and 5080 are to each other.
The 7900 XTX’s 77 FPS AVG has it outperforming the 4080 cards and behind the 5080 card, which leads by 10%.
The 3080 did OK in this test at 50 FPS AVG, with a 71% improvement to the 5080.
1440p has the RTX 5080 at 134 FPS AVG, achieving 71% of the performance of the RTX 5090 at about 50% of the price. The RTX 4090 leads the 5080 again, this time by around 20 FPS.
As for AMD, the 7900 XTX lands at 126 FPS AVG for a 10 FPS gap between the two cards. The 4080 lands at 122 FPS AVG, keeping the 5080 at 10% ahead.
1080p has the cards crushed together, although there’s still a gap of 49 FPS between the 5090 and 5080. That’s 30%, which is a lot for 1080p, but this isn’t necessarily a scenario to plan for.
The RTX 4080 is right next to the 5080. Although the gap is 10 FPS average, the reality is that very few people would notice 165 FPS vs. 156 FPS, particularly in this game.
This is where we would test Cyberpunk: Phantom Liberty -- if it worked. We’ve had repeated issues with getting the 50-series cards to run Cyberpunk without some sort of game-level issue occurring where it just won’t launch or crashes. We had this with the 5090, but reinstalled the game and that fixed it. We can’t state for certain that it is a fault of the 50-series cards and not some other game update, especially as Cyberpunk has had so many problems over the years, but what we can say is that it continuously kept breaking. We removed it from this review until the game is more stable. Again, we’re not sure if it’s unique to these cards or not, but we know that Jay had similar issues in his testing.
Dying Light 2 at 4K is next. The 5080 ran at 81 FPS AVG, giving the 4090 a 13% lead and the 5090 a 56% lead. That’s a large improvement for the 5090 over the 5080. The 4090 improved over the 4080 by less, at 36% here.
The 5080 leads the 7900 XTX by 12% here, with the 7900 XTX’s 73 FPS AVG result landing between the 5080 and 4080. Speaking of, the 5080 leads the 4080 cards by about 21%.
We’re seeing large gains from the 3080 to the 5080, as expected, with the 2080 Ti below even the 3080.
At 1440p, the RTX 5080’s 149 FPS AVG puts it close to the 7900 XTX’s 141, with lows between them also comparable. The 5080 is advantaged, but that gap was larger at 4K. The 5090 also has its lead cut down to 45%; although still significant, the doubled price with the 5090 jeopardizes its value for anyone not using it in VRAM-intensive work scenarios.
The 4080 cards were again almost identical, re-proving why we didn’t waste our time in that review. The 5080 leads them by 12%.
At 1080p, the 5090 led the 5080 by 44%, which is really more impressive for the 9800X3D (read our review) than anything else. That is a gigantic gap when considering how any prior CPU we’ve used for these benches would be bottlenecking the top 2-3 cards. The 4090 also maintains a lead of 19% over the 5080. 
In Resident Evil 4 at 4K, the RTX 5080 fell behind the 7900 XTX by 4 FPS. Lows are about the same, but advantaged on the 7900 XTX. The 5090’s lead over the 5080 is massive in this one, at 68.9% with these settings. That’s a huge gulf. Looking back, the 4090 led the 4080 by 47%. There are two main ways to look at this: One, the 5090 is just that good; two, the 5080 is just that unimpressive.
To figure out which it is, we can compare the 5080 to the 4080. The improvement here is 19%. NVIDIA’s best argument for itself will be the price as compared to the original 4080. The 7900 XTX wins that argument in this particular game, though.
At 1440p, the advantage in the 5090 over the 5080 is reduced to 56%, though that’s still a lot. The 4090 is still more than 50 frames per second faster than the 5080, with the 7900 XTX now 4% ahead. That’s not a huge gap, but considering the generational difference and price of the XTX, we remain disappointed that AMD has signaled it likely won’t launch a replacement this generation. Maybe they’ll reconsider.
The 5080’s lead over the 4080 drops to just 11% here.
Black Myth: Wukong is up first, with a caveat that this game is unbelievably NVIDIA favored. We have other RT tests that are more balanced, but this is one of the heaviest workloads and also happens to benefit NVIDIA. Cyberpunk would also fit that bill, but we removed it for the stability issues we mentioned.
At 4K with FSR as defined in the chart title, the 5090 held 88 FPS AVG, the 4090 ran at 65 FPS AVG, and the 5080 ran at 59 FPS AVG. That sets a 15% lead over the 4080 FE’s 51 FPS AVG. 
Some notables: The 3080 just can’t really handle these settings and is down at 28 FPS AVG. Jumping to the 4080 from the 3080 would be an 83% improvement, with the 5080 giving an uplift of 112% total from the 3080.
Sadly, the 7900 XTX is worse than the 3080 in this benchmark.
As for the 5090, it ran 51% higher framerate than the 5080 with these conditions. Sadly for Jimmy, the 5080 did not defeat the 4090 here -- but there’s hope.
At 1080p but still with the same FSR settings, the 5080 operated at 122 FPS AVG and -- what’s that? -- the 5080 outperforms the 4090 in this one by 1.5 FPS AVG. No one said the win had to be meaningful and a win is a win -- and as soon as I reported the results, I heard they were considering a rematch with the 5070.
The 5080 roughly matches the 4090 in this result, with the 5090 leading the 5080 by 30%. The 1080p restriction with FSR is what’s making this less exciting than typically.
Dragon’s Dogma 2 with RT is up next. This one is more balanced.
The RTX 5090 at 4K ran at 113 FPS AVG with maximum RT, which puts it 33% over the 4090 and 57% over the 5080. The 7900 XTX is much more competitive in this one than Black Myth, running at 66 FPS AVG and landing between the 4080 Super and 5080.
The generational uplift is 16% from the 4080 FE to the 5080 FE. 
At 1440p and still with RT, the 5080 held about a 7% lead over the 7900 XTX. That’s not bad for AMD’s last gen card in this game. As for the 4080 FE, the 5080 is about 10% ahead. Not very exciting for NVIDIA overall with the 5080 in this one. Let’s move on.
At 1080p, the 5080’s 146 FPS AVG had it only about 9 FPS over the 4080. The 5090 still keeps a 33% lead over the 5080, despite slimming down versus the 4K result’s 57% gap.
This isn’t that interesting, so let’s skip ahead.
With ray tracing for Dying Light 2 and tested at 4K and upscaling, the RTX 5080 ran at 67 FPS AVG and trailed the 4090 by over 10 FPS, with the 5090 leading the 5080 by 63%. 
Over the 4080, the 5080 leads by about 17%. The 7900 XTX is between the 3090 Ti and 4070 Ti (watch our review), trailing the 4080, and obviously therefore trailing the 5080.
At 1440p with RT and upscaling, the 5080’s 117 FPS AVG lands it 12% ahead of the 4080. The gap is shrinking here. The 7900 XTX trails somewhat significantly here and gives the 5080 an advantage of 35%.
At 1080p, the 5090 proves that there’s still headroom in the CPU (impressively) with its 224 FPS AVG, leading the 5080 by 41%, but the 4080 and 5080 get squished together with only a 9% advantage to the 5080 between them.
The 7900 XTX is closer to the 3090 Ti here, keeping the 5080 about 30% ahead.
Finally for RT, Resident Evil 4 at 4K with upscaling is up. The RTX 5090 ran at 210 FPS AVG with these settings, or a 54% improvement over the 5080’s 136. The 7900 XTX is much more competitive here, with the title generally being a lightweight RT implementation. We keep it around for that reason: If we have a heavy one like Black Myth, it helps to balance representation with a light one. The 7900 XTX looks much better here than in other RT tests.
The 4080’s 117-119 FPS AVG makes the scaling in the 5080 overall boring, unfortunately, with the improvement being hardly noticeable.
Here’s 1440p. The 5080 jumps to 205 FPS AVG, the 5090 to 290 FPS AVG, and the 7900 XTX to 195. The XTX is still competitive here, especially given the price and generational differences; likewise, the 4080 still makes the 5080 relatively uninteresting. This feels like a sidegrade.
Power efficiency testing is next, measured using power interposers and capturing only GPU power. We are not measuring total system power for this and instead isolate for the GPU. The PCIe slot is intercepted and measured as well.
Idle power consumption is far better on the RTX 5080 than the RTX 5090. Keep in mind that idle power testing is very situational to the monitor, refresh rate, and power plans -- but our test setup is identical unit-to-unit, so that’s all that matters here. They are all under the same conditions.
Under these conditions, the RTX 5080 was at 12.75W when idle on the desktop. This is close (but improved upon) the RTX 4080’s 15-16W idle power draw, both of which are far below the 4090’s 29W and 5090’s 46W power consumption. 
Final Fantasy 14 at 4K had the RTX 5080 at the top of the chart for efficiency. Comparatively, this is a great result. Its 298W draw here leaves headroom in the TDP budget, with the relatively high framerate enabling it to outperform the 269W 4080 Super for efficiency and performance alike. It also outdoes the 5090, which scored 0.34 FPS/W. The 5080 significantly outdoes the RX 7900 XTX, which ran at 0.24 FPS/W and 430W.
This appears to be a possible strong point for the RTX 5080.
1440p gives us a lot more cards on the chart since we’ve tested more mid-range devices, but with the downside that the high-end devices scale their best at 4K. That means we lose some of the framerate scaling advantage.
The 5080 is still tied for the most efficient on this chart after rounding, aligning with the 4080 Super as the most efficient GPU we’ve tested yet. It’s at 0.75 FPS/W while pulling 290W, up from the 268W of the 4080 Super but also up in framerate. The 4090 pulled 386W here, with the 5090 at 520W.
For efficiency in F1 24 at 4K and with RT, the RTX 5080 ties the 4090 and is within reasonable variance of the 4090 and 4080 Super results. The 5080 pulled 330W in this test, putting it notably lower in power consumption than the RTX 5090’s 569W draw here, but still more than the 4080 Super’s 291W. The 7900 XTX is significantly less efficient in this particular workload, hampered in part by RT but also just by its 422W draw.
Check back soon for our ITX testing with the FE cards and for transient power testing coming up. We’ll look at power excursions in that. We also have a story about MFG, or multi-frame generation, almost entirely complete and coming up. That’ll delve into NVIDIA’s new DLSS and frame generation tech in detail.
Otherwise, that’s all the numbers for now. The quick recap is:
With several partner models listed at $1,200 to $1,400 in pre-launch listings, the value isn’t compelling. We also continue to take issue with NVIDIA’s hugely misleading “benchmarks” on its website:
The 5080 is shown as being 2x the speed of the 4080 in several tests, but again, this has to do with its MFG 4X Mode that’s only available on the 50-series. We’re testing that. The boring range of 10% to maybe 20% in some situations is more common. We ranted about this in the 5090 review if you want more.
This is why NVIDIA is pushing MFG so hard: The 5080 otherwise is a boring product generationally. When the gap is sometimes 10 FPS over the 4080 Super at the same price, it’s just hard to get excited about. It seems like a tool to create 5090 sales.
As for the bets of tater tots within the office, it looks like Jimmy wins this one. They’ll have a rematch for the 5070 and 4090.
 Copyright © 2023 GamersNexus, LLC. All rights reserved.GamersNexus.net is Owned, Operated, & Maintained by GamersNexus, LLC. 
Copyright © 2023 GamersNexus, LLC. All rights reserved.GamersNexus.net is Owned, Operated, & Maintained by GamersNexus, LLC."
"NVIDIA","RTX 5080","NVIDIA GeForce RTX 5080 Founders Edition Review","https://www.techpowerup.com/review/nvidia-geforce-rtx-5080-founders-edition/",""
"NVIDIA","RTX 5080","Nvidia GeForce RTX 5080 Review","https://www.techspot.com/review/2947-nvidia-geforce-rtx-5080/","The GeForce RTX 5080 is Nvidia's new $1,000 GPU offering, coming in at half the price of the RTX 5090 with, unsurprisingly, half the specs. You get half the cores, half the memory bus, half the VRAM, and almost half the memory bandwidth.
Nvidia has created a massive gap between the 5080 and 5090, clearly aiming to upsell gamers on the much faster and far more expensive RTX 5090 for those who can afford one. Additionally, this approach leaves them plenty of room to release more costly GeForce 50 Super series cards within the next 12 months, addressing the obvious VRAM limitations present in much of the Blackwell lineup. So that's fun.
In terms of specifications, the RTX 5080 is only a minor upgrade over the RTX 4080 Super, which it is set to replace. The GeForce RTX 5080 features just 5% more cores, clocked a mere 3% higher. The only meaningful upgrade is the shift to GDDR7 memory, which on the same 256-bit wide memory bus boosts bandwidth by 30% thanks to its 30 Gbps memory speed – Déjà vu? You might as well call this the RTX 4080 Ti Super.
The previous-gen RTX 4080 Super likely wasn't heavily memory-limited, so the improvements here are expected to result in only a modest performance gains. Of course, we're about to dive into the numbers to confirm this.
Before diving into the blue bar graphs, let's take a look at how Nvidia's Founders Edition version of the RTX 5080 handles thermals compared to the RTX 4080 FE. For this test, we used The Last of Us Part I running at 4K with maxed-out settings.
After an hour of load inside an enclosed ATX case, the RTX 5080 reached a peak GPU temperature of 63°C – remarkable, given how quiet and compact this graphics card is. The fan speed peaked at 1,400 RPM and was virtually inaudible over our already very quiet case fans.
The cores maintained an average clock speed of 2,655 MHz, with an average GPU power draw of 266 watts. Meanwhile, the memory temperature peaked at 72°C, operating at a frequency of 2,500 MHz for a transfer speed of 30 Gbps.
By comparison, the RTX 4080 FE peaked at 62°C, with a memory temperature of 74°C and fans spinning at just over 1,300 RPM. It's clear that the RTX 5080 design is more efficient, at least in terms of physical size. However, when ignoring the size difference, the thermal performance between the two cards is nearly identical.
So, while the new FE model delivers great results in terms of thermals and efficiency, the real question is: how does it perform in terms of FPS? Let's find out.
Starting with Marvel Rivals at 1440p, the new RTX 5080 achieved 108 fps, making it just 8% faster than the 7900 XTX and 14% faster than the 4080 Super. It was also only 9% slower than the 4090, which isn't bad considering it should cost significantly less. Now, let's check out the 4K results.
At 4K, the 5080 looks slightly more impressive, now showing a 16% lead over the 4080 Super and a 12% advantage over the 7900 XTX. While it is faster, the improvement isn't particularly significant in this scenario.
Next, we have S.T.A.L.K.E.R. 2, where the 5080 is only 4% faster than the 4080 Super at 1440p and just 3% ahead of the 7900 XTX.
Moving to 4K doesn't change much. Here, the new 5080 is 4% faster than the 7900 XTX and 9% faster than the 4080 Super, resulting in unimpressive gains.
The 5080 struggles in Counter-Strike 2, coming in 4% slower than the 4080 Super at 1440p and a significant 13% slower than the 7900 XTX.
At 4K, the 5080 recovers slightly, beating the 4080 Super by a mere 5%, but still trails behind the 7900 XTX by 8%, making for a poor showing.
The God of War Ragnarök results were also disappointing. At 1440p, the 5080 was just 4% faster than the 4080 Super and again fell behind the 7900 XTX, though only by a few frames.
The 4K results are more promising, with the 5080 pulling ahead of the 7900 XTX by 9% and the 4080 Super by 8%. However, for a next-generation product, this is still a letdown – though at least in this case, it was faster.
For some reason, the new GeForce 50 series GPUs perform particularly poorly in Delta Force. At 1440p, the 5080 was 10% slower than the 4080 Super and 12% slower than the 7900 XTX – an abysmal result.
Even at 4K, it remained behind both the 4080 Super and 7900 XTX. Although the gaps were reduced at this resolution, the overall performance remained poor.
The Space Marine 2 results are better, but still, the RTX 5080 was only 6% faster than the 4080 Super at 1440p and 18% faster than the 7900 XTX.
At 4K, we see a 21% improvement over the 4080 Super, making it one of the better results so far. The 5080 was also 27% faster than the 7900 XTX, which is at least a more competitive showing.
Performance in Star Wars Jedi: Survivor is also disappointing. At 1440p, the 5080 was just 2% faster than the 7900 XTX and 5% faster than the 4080 Super.
The 4K results are more favorable, but even then, the 5080 was only 15% faster than the 7900 XTX and 19% faster than the 4080 Super. While a 19% lead is a step in the right direction, it's still a modest gain for a new generation.
A Plague Tale: Requiem delivers more reasonable gains compared to most other titles tested. The RTX 5080 was 15% faster than the 4080 Super at 1440p and 18% faster than the 7900 XTX.
Oddly, however, at 4K, the RTX 5080 was only 11% faster than the 7900 XTX and 15% faster than the 4080 Super – disappointing, as the margins didn't expand relative to the 1440p results.
Even in Cyberpunk 2077, performance remains underwhelming. At 1440p, the RTX 5080 was just 9% faster than the 4080 Super and 7% ahead of the 7900 XTX.
Moving to 4K helped the 5080 slightly, but it was still only 10% faster than the 7900 XTX and 16% faster than the 4080 Super. Disappointingly, these are among the better margins we've seen for the new 50 series GeForce GPU.
In Dying Light 2, the 5080 was 12% faster than the 4080 Super at 1440p and 11% ahead of the 7900 XTX – another underwhelming result.
At 4K, the margins grew slightly, with the 5080 being 18% faster than the 7900 XTX and 20% faster than the 4080 Super. While these are some of the largest gains we've seen so far, they still fall short of expectations for a new generation.
Dragon Age delivers only modest gains, with a 9% uplift over the 4080 Super at 1440p and a 15% increase over the 7900 XTX.
The 4K results aren't much better, showing just a 16% improvement over the 4080 Super and an 18% gain over the 7900 XTX. While it's not nothing, these results are highly underwhelming for a next-generation product – especially given how long we've waited for it.
In War Thunder at 1440p, the 5080 only manages to match the 4080 Super. However, this still makes it significantly faster than the 7900 XTX, as Radeon GPUs perform poorly in this title when using the default DX11 mode. Performance is considerably better with DX12, but for now, that API remains labeled as ""beta"" and is not the default.
At 4K, the RTX 5080 is just 3% faster than the 4080 Super and 12% ahead of the 7900 XTX, making for yet another set of unimpressive margins.
Performance in Spider-Man Remastered is disappointing. While 1440p results may be somewhat CPU-limited, this is not the case at 4K, yet the 5080 still only manages to match the 4080 Super and 7900 XTX.
Next up is Hogwarts Legacy, where at 1440p, the 5080 is just 12% faster than the 4080 Super but actually 5% slower than the 7900 XTX.
Increasing the resolution to 4K provides some improvement, with the 5080 now 22% faster than both the 7900 XTX and 4080 Super – arguably the most impressive margin we've seen so far.
Moving on to The Last of Us Part I, the 5080 was only able to match the 4080 Super at 1440p, making it 6% slower than the 7900 XTX.
At 4K, it managed to match the 7900 XTX and was 7% faster than the 4080 Super – another disappointing result.
The Star Wars Outlaws results are equally disappointing. At 1440p, the 5080 was just 3% faster than the 4080 Super, though it did hold a 16% advantage over the 7900 XTX.
At 4K, the 5080 was only 5% faster than the 4080 Super and 17% ahead of the 7900 XTX.
Finally, in Starfield, the 5080 was just 2% faster than the 4080 Super at 1440p while trailing the 7900 XTX by 2%.
At 4K, it inched ahead of the 7900 XTX by a mere 3%, making it 8% faster than the 4080 Super. Ultimately, these are mostly single-digit gains, but let's move on to examine the average performance across the 17 games tested.
Although we didn't go over the 1080p numbers in detail, here's the average data for those interested. Essentially, the GeForce RTX 5080 only managed to match the 4080 Super at this resolution. It seems the Blackwell architecture struggles slightly more than previous generations at lower resolutions, and this isn't always due to a CPU bottleneck.
At 1440p, we're looking at a mere 4% performance increase for the RTX 5080 over the 4080 Super – just 4% on average or 5% over the 7900 XTX. That said, it's still better than the 2% uplift we saw from the 4080 to the 4080 Super, so perhaps things are improving after all.
At 4K, the results don't improve much. Sure, the 5080 was, on average, 11% faster – so at least we hit double digits – but that's still quite underwhelming for a next-gen GPU. Compared to the original RTX 4080, it's only 14% faster, and when stacked against AMD's nearest competitor, it offers just an 8% gain over the 7900 XTX.
The good news is that power consumption is quite reasonable, at least based on our 1440p testing. Here, the 5080 consumed anywhere from 4% to 19% less power than the 4080 Super and significantly less than the Radeon RX 7900 XTX.
Unfortunately, the RTX 5080 doesn't offer anything particularly impressive when it comes to ray tracing, delivering only an 11% increase over the 4080 Super at 1440p in Metro Exodus. However, this did make it nearly 50% faster than the 7900 XTX. As we always say, if you care about RT performance, a GeForce GPU is the way to go.
At 4K, the results are similar, with the 5080 showing a 14% improvement over the 4080 Super.
The Metro Exodus results are not an anomaly. In Alan Wake II at 1440p, there is very little difference between the 4080 Super and the 5080 in RT performance.
At 4K, there is some separation, but even then, the 5080 only pulls ahead of the 4080 Super by 12% – a highly underwhelming result.
The Cyberpunk 2077 results tell the same story, with the 4080 Super and 5080 delivering comparable performance at 1440p.
At 4K, using quality upscaling with the ultra ray tracing preset, the 5080 averaged just 50 fps, making it only 6% faster than the 4080 Super.
In Marvel's Spider-Man Remastered, there was no performance difference between the 5080 and the 4080 Super at either 1440p or 4K, with both tested using DLSS quality upscaling.
We saw a 9% uplift in Dying Light 2 at 1440p for the 5080 over the 4080 Super, which, surprisingly, is one of the better RT gains observed so far.
At 4K, the margin extended slightly to 10%, but at least the 5080 was 55% faster than the 7900 XTX.
With ray tracing enabled at 1440p using quality DLSS upscaling, Black Myth: Wukong averaged just 70 fps – an 11% improvement over the 4080 Super. The 4K results followed the same pattern, making for highly disappointing gains, with no CPU bottleneck to blame for these results.
The GeForce RTX 5080 and 4080 Super delivered nearly identical performance with ray tracing turned on at 1080p, making the 5080 nearly 80% faster than the 7900 XTX.
At 1440p, the 5080 was just 5% faster than the 4080 Super, while Radeon GPUs remained largely irrelevant in this category.
Finally, at 4K with quality upscaling, the RTX 5080 was, on average, 9% faster than the 4080 Super. So much for improved RT performance.
In a perfect world where GPUs sold at MSRP – or perhaps not so perfect when we're talking about $1,000+ products – if MSRP applied across the board, the RTX 5080 wouldn't look terrible. However, it certainly doesn't feel like a next-generation product, offering 11% better value than the RTX 4080 Super.
For many months, the 4080 Super was readily available at its $1,000 MSRP, so this isn't anything new. Essentially, if you wanted this level of performance from a 16GB GeForce GPU, you could have gotten it a year ago.
Supporting that point, if we look at the best pricing from mid-2024 – many of which were available for most of the year – if the RTX 5080 launches at $1,000, it will offer just an 8% improvement in value over a product that could have been purchased at least six months earlier.
Furthermore, if ray tracing isn't a priority, the 7900 XTX arguably presents a better value. However, if the price difference is only $100, the RTX 5080 is likely the better buy. That said, considering how much older competing GPUs like the 7900 XTX and 4080 Super are, the 5080 remains an underwhelming release.
So there you have it – the GeForce RTX 4080 Ti Super. Actually, it's probably not even that good.
In terms of cost per frame, the 4080 Super offered nearly 20% better value than the 4080, whereas the RTX 5080 appears to be, at best, just 10% better value than the 4080 Super. Some may argue that the RTX 5080 should be compared to the original 4080, but that's nonsense.
The RTX 4080 was essentially a failed product – and that's not even referring to the ""unlaunched"" AD104 version. The $1,200 RTX 4080 we ultimately received was a disappointment, and most gamers agreed by not buying any. This led to stock sitting on shelves, forcing Nvidia to release the 4080 Super, which was essentially the same GPU in terms of performance but with a $200 price cut.
Given that, it only makes sense to compare the RTX 5080 with the 4080 Super. Once you do, it quickly becomes clear that there's very little new on offer. Features like multi-frame generation can be useful, but they are highly situational. For the most part, you're not missing out on much, and the most exciting DLSS 4 features will be available on previous RTX GPUs anyway (which is great).
For those still using GeForce RTX 30 series GPUs and holding out for nearly five years, the RTX 5080 is 67% faster than the RTX 3080 but comes with a 43% price increase. Adjusting for inflation, that price increase is closer to 20%. If we recalculate cost per frame with inflation in mind, the RTX 5080 has improved by 28% relative to the 3080.
So, after all this time, it's not exactly an amazing upgrade. Without question, the 5080 comes with a premium, and a similar performance jump could have been achieved at least six months ago with the 4080 Super.
Looking back at the flagship GeForce RTX 5090, it delivers a somewhat disappointing gen-on-gen boost, providing approximately 30% more performance for what will be at least a 25% price increase. However, since this is more of a ""price is no object"" product, value can be overlooked to some extent in favor of raw performance. We just wish there was more to it.
Now, if you consider that was underwhelming, then what should we make of the RTX 5080?
If the RTX 5080 launches slightly above $1,000 and closely aligns with the 4080 Super in terms of value, then it really doesn't matter which one you buy. Of course, you might as well get the 5080 since it's newer, but beyond that, there's no compelling reason to choose it – and certainly no reason to pay a premium.
For the RTX 5080 to avoid the same fate as the original 4080, sitting on shelves unsold, it simply cannot be priced above $1,000. Hopefully, its MSRP will be more reasonable than how the 5090's shaping up. However, once reviews go live, we'll get a clearer sense of how the community feels – and it's unlikely to be well received.

About
Ethics Statement
Terms of Use
Privacy Policy
Manage Ad Consent

TECHSPOT : Tech Enthusiasts, Power Users, Gamers
TechSpot is a registered trademark and may not be used by third parties without express written permission.
TechSpot is represented by Future PLC. Learn about advertising opportunities.
© 1998 - 2025 TechSpot, Inc. All Rights Reserved."
"NVIDIA","RTX 5080","Nvidia GeForce RTX 5080 Founders Edition review","https://www.tomshardware.com/pc-components/gpus/nvidia-geforce-rtx-5080-review","When you purchase through links on our site, we may earn an affiliate commission. Here’s how it works.

The RTX 5080 serves as the penultimate Blackwell GPU, with a sizeable step down from the 5090 at half the cost. More critically, outside of multi-frame generation, it's not significantly faster than the 4080 Super in most of our tests. At least it's the same price as the outgoing card.
Retail pricing and availability is a joke (addendum)

Why you can trust Tom's Hardware




Our expert reviewers spend hours testing and comparing products and services so you can choose the best for you. Find out more about how we test.

The Nvidia GeForce RTX 5080 Founders Edition takes the honor guard position for the RTX 5090. In times past, the penultimate Nvidia GPU of each generation has often been the best overall pick. But the gap between first and second place has widened significantly in the past two generations, at least for 4K gaming and other demanding workloads. The 5080 also takes over from the RTX 4080 and RTX 4080 Super, often with only modest gains. It may still be one of the best graphics cards when the dust clears, but it doesn't have the wow factor of its big brother.

Both the RTX 5080 and RTX 5090 will go on sale tomorrow, January 30, 2025. While we anticipate a lot of demand for the halo card, the 5080 will hopefully be more readily available — but probably only after the initial wave of eager buyers clears. And there's still the risk that businesses looking for affordable AI hardware might drive inventory shortages because while the 5080 can't match a 5090 in raw performance, two of them would certainly provide plenty of computing for nominally the same price.

RTX 5080 will have the same core feature set, meaning stuff like native FP4 support that could entice AI researchers and developers. But it still 'only' has 16GB of VRAM, and many AI models tend to be voracious when it comes to memory requirements — though DeepSeek has certainly shaken many of the foundational thoughts about AI training and inference, as well as Nvidia's stock price.
We've written a lot of supplemental coverage about Nvidia's new Blackwell RTX 50-series GPUs. If you want a primer, or additional information, check out these articles:• Blackwell architecture• Neural rendering and DLSS 4• RTX 50-series Founders Edition cards• RTX AI PCs and generative AI for games• Blackwell for professionals and creators• Blackwell benchmarking 101
We were extremely crunched for time on the RTX 5090 review, and things have only been slightly better on the RTX 5080. There's still a lot to dissect, and unfortunately, we can't shake the feeling that the initial Blackwell drivers are holding the cards back. The 1080p results are particularly bad at times, and Nvidia's heavy reliance on Multi Frame Generation (MFG) for the initial performance preview suggests that was probably at the forefront of the driver team's work, rather than general performance.

You can check the boxout with additional links and information on the Nvidia Blackwell and RTX 50-series GPUs. The succinct story for the RTX 5080 is that, outside of certain AI workloads and MFG, it's currently a pretty minor upgrade over the prior generation 4080 cards. (The 4080 Super was only a few percent faster, with its primary attraction being a $200 price cut compared to the vanilla model.) The specs basically say most of what you need to know.
The biggest change, outside of AI and MFG, is support for faster GDDR7 memory. The RTX 5080 has 960 GB/s of bandwidth, compared to 736 GB/s on the 4080 Super and 717 GB/s on the original 4080. So, depending on your point of reference, that's 30–34 percent more bandwidth, a pretty sizeable upgrade.

But in core processing power, ignoring the new native FP4 number format support, the upgrades are far less impressive. RTX 5080 has 84 Streaming Multiprocessors (SMs) and 10752 CUDA cores, compared to the 4080 Super's 80 SMs and the 4080's 76 SMs. Clock speeds are slightly higher in theory, but in practice, it's mostly a wash. Raw compute ends up being 8% more than the 4080 Super and 16% more than the 4080.

Most of the other specs scale with the number of SMs, so there's a similar potential 8% and 16% uplift in tensor compute for the existing FP8, FP16, and other formats. However, Blackwell adds native FP4 support (Ada relied on FP4 running as an FP8 calculation), which doubles the potential throughput if you don't need the higher precision of FP8. That's where the 1.8 petaFLOPS of compute comes from, compared to just 836 teraFLOPS on the 4080 Super.

ROPS is the same 112 count on the 5080 and 4080-class GPUs, so pixel shading throughput hasn't changed. Ray tracing, on the other hand, sees another doubling in ray/triangle intersection calculations, and Nvidia says the 5080 offers 170.6 teraFLOPS of RT compute, compared to 121 and 113 teraFLOPS of RT on the 4080 Super and 4080, respectively.

There's also a new PCIe 5.0 interface, though that shouldn't matter much for most tasks. The biggest benefit will be for multi-GPU configurations running AI and GPGPU tasks — not for gaming, which no longer has NVLink or multi-GPU support. Power consumption also sees a modest bump from 320W with the previous generation to 360W with the 5080.
The good news is that the RTX 5080 won't cost more than the outgoing RTX 4080 Super. Or that's the theory. It's really going to depend on supply and demand, and as we've seen with the dwindling inventories of RTX 4080 and 4090 parts over the past few months, there's still enough demand to push prices up if Nvidia doesn't provide an adequate supply. And, much to no one's surprise, Nvidia says the 5090 and 5080 may experience stock shortages in the coming days.

Why isn't that a surprise? Because there's a limited number of TSMC wafers to go around right now. Every GB202 or GB203 wafer that Nvidia orders mean one less GB200 wafer and Nvidia previously said its Blackwell B200 supply is already allocated for 2025. That means there's limited incentive to produce a bunch of consumer GPUs that sell for an order of magnitude less money than the most powerful data center parts.

That means we'll likely see third-party AIB (add-in board) partner cards selling for far more than the base $999 MSRP of the RTX 5080. There are already hints that some card models could cost $1,399 or more, and if there's a supply deficit, then we aren't likely to see many base-price cards after the initial stock lands. Hopefully, the shortages won't be as severe as we saw with the 3080 cards in 2020–2021 (those were driven by cryptomining), but only time will tell.

For now, let's take a closer look at the RTX 5080 Founders Edition, and then we'll hit the benchmarks.

Current page:

Introducing the Nvidia GeForce RTX 5080


Jarred Walton is a senior editor at Tom's Hardware focusing on everything GPU. He has been working as a tech journalist since 2004, writing for AnandTech, Maximum PC, and PC Gamer. From the first S3 Virge '3D decelerators' to today's GPUs, Jarred keeps up with all the latest graphics trends and is the one to ask about game performance.

Tom's Hardware is part of Future US Inc, an international media group and leading digital publisher. Visit our corporate site.

©
Future US, Inc. Full 7th Floor, 130 West 42nd Street,
New York,
NY 10036."
"NVIDIA","RTX 5090","Nvidia GeForce RTX 5090 Founders Edition review","https://www.tomshardware.com/pc-components/gpus/nvidia-geforce-rtx-5090-review","When you purchase through links on our site, we may earn an affiliate commission. Here’s how it works.

Nvidia's RTX 5090 marks the debut of the new Blackwell architecture, with new features, higher performance, more memory, and a lot more bandwidth. But the drivers could use a bit more time baking in Jensen's oven.

Why you can trust Tom's Hardware




Our expert reviewers spend hours testing and comparing products and services so you can choose the best for you. Find out more about how we test.

The Nvidia GeForce RTX 5090 Founders Edition has arrived — or at least, the reviews have arrived. It's the fastest GPU we've ever tested, most of the time, and we expect things will continue to improve as drivers mature in the coming weeks. When it lands on retail shelves, the RTX 5090 will undoubtedly reign as one of the best graphics cards around for the next several years.

The card itself — as well as AIB (add-in board) partner cards using the RTX 5090 GPU — won't go on sale until January 30. Once it does, good luck acquiring one. It's an extreme GPU with a $1,999 price tag, though there will certainly be some well-funded gamers looking to upgrade. It also features new AI-centric features, including native FP4 support, and that will very likely generate a lot of interest outside of the gaming realm. With 32GB of VRAM and 3.4 PetaFLOPS of FP4 compute, it should easily eclipse any other consumer-centric GPU in AI workloads.
We've written a lot of supplemental coverage about Nvidia's new Blackwell RTX 50-series GPUs. If you want a primer, or additional information, check out these articles:• Blackwell architecture• Neural rendering and DLSS 4• RTX 50-series Founders Edition cards• RTX AI PCs and generative AI for games• Blackwell for professionals and creators• Blackwell benchmarking 101
It's been an extremely busy month and our review initially posted as a work in progress with a tentative 4.5 stars out of 5. After additional testing, we feel that's mostly a fair assessment, with the understanding that this is the most potent GPU we're likely to see until Nvidia's next generation arrives about two years from now. However, we've now docked half a star, due to concerns specifically with the Founders Edition running hot.

If you're not interested in spending $2,000 or more on a graphics card, we hear you, and you shouldn't view that score as a recommendation to plunk down a huge wad of cash purely for gaming purposes. But there are good reasons to want an RTX 5090, outside of just playing games.

We've revamped our test suite and our test PC, wiping the slate clean and requiring new benchmarks for every graphics card in our GPU benchmarks hierarchy, and while we have reviewed the Intel Arc B580 and Arc B570 and tested some comparable offerings, there are a lot of GPUs that we still want to retest. But it's a reasonably safe bet that when the dust clears, the 5090 will stand alone at the top of the performance heap.

The Nvidia Blackwell RTX 50-series GPUs also bring some new technologies, which require separate testing. Chief among these (for gamers) is the new DLSS 4 with Multi Frame Generation (MFG). That requires new benchmarking methods, and we spent some additional time with the some DLSS 4 enabled games to get a better idea of how they look and feel.

We already know from experience that DLSS 3 frame generation isn't a magic bullet that makes everything faster and better. It adds latency, and the experience also depends on the GPU, game, settings, and monitor you're using. With MFG potentially doubling the number of AI-generated frames (DLSS 4 can generate 1, 2, or 3 depending on the setting you select), things become even more confusing. MFG as an example running at 240 FPS would mean user input only gets sampled at 60 FPS. That's not bad, but it's definitely not the same as a game like Counter-Strike 2 running at 240 FPS natively.

Here are the specifications for the RTX 5090 and its predecessors — the top Nvidia GPUs of the past several generations.
The raw specs alone give a hint at the performance potential of the RTX 5090. It has 33% more Streaming Multiprocessors (SMs) than the previous generation RTX 4090, just over double the SMs of the 3090, and 2.5 times as many SMs as the RTX 2080 Ti that kicked off the ray tracing and AI GPU brouhaha. Just as important, it has 33% more VRAM than the 4090, and the GDDR7 runs 33% faster than the GDDR6X memory used on the 4090, yielding a 78% increase in memory bandwidth.The rated boost clocks on the RTX 5090 have dropped compared to the 4090, but Nvidia's boost clocks have always been rather conservative. Depending on the game and settings used, in some cases the real-world clocks were even higher than before. Except, that's mostly at 1080p and 1440p, where CPU bottlenecks are definitely a factor and the 5090 wasn't hitting anywhere close to the maximum 575W of power use. Typical clocks ranged from 2.5 GHz to 2.85 GHz in our testing (more details on page eight).But it's not just performance and specs that have increased. The RTX 5090 has an official base MSRP of $1,999 — $400 more than the RTX 4090's base MSRP. That's probably thanks to the demand that Nvidia saw for the 4090, with cards frequently going for over $2,000 during the past two years. We suspect much of that was thanks to businesses buying the cards for AI use and research (not to mention people reportedly trying to smuggle 4090 cards into China). Those same factors will undoubtedly affect the RTX 5090.Things shouldn't be as bad as the cryptomining shortages of the RTX 30-series era, but we would be shocked if the 5090 isn't difficult to buy in the coming months at anywhere close to $2,000. Nvidia's top GPUs have traditionally been hard to acquire in the first month or two after launch, and that pattern will no doubt continue — and perhaps be even worse than the 4090 launch.
But really, what's the competition to the RTX 5090? Most people want to see how much faster it is than the RTX 4090, plus a few other high-end / extreme offerings. It's not like someone is looking at an RTX 4070, RX 7700 XT, or Arc B580 but will instead decided to spend 4–8 times as much on a 5090.

AMD's RX 7900 XTX didn't really compete with the 4090, and it certainly won't beat the 5090 — at least, not at 4K. And you really should be running a 4K or higher resolution display if you're thinking about using a 5090 for gaming. We're still running benchmarks on the new test PC, and we want to include the RTX 3090 to show what a two generation upgrade will deliver, for those using the top Ampere RTX 30-series GPUs. But that will have to fall to the GPU benchmarks hierarchy (once we're done with a few other reviews).

Other than that? We don't really see anything that will keep up with the 5090 arriving any time soon. And that's just for gaming. For AI workloads that can use the new FP4 number format, Nvidia claims the 5090 can be up to three times as fast as the 4090. It's set to dominate the GPU landscape until the inevitable RTX 6090 or whatever arrives in a couple of years — or perhaps Nvidia will do an RTX 5090 Ti or Titan Blackwell this generation.

Current page:

Introducing the Nvidia GeForce RTX 5090


Jarred Walton is a senior editor at Tom's Hardware focusing on everything GPU. He has been working as a tech journalist since 2004, writing for AnandTech, Maximum PC, and PC Gamer. From the first S3 Virge '3D decelerators' to today's GPUs, Jarred keeps up with all the latest graphics trends and is the one to ask about game performance.

Tom's Hardware is part of Future US Inc, an international media group and leading digital publisher. Visit our corporate site.

©
Future US, Inc. Full 7th Floor, 130 West 42nd Street,
New York,
NY 10036."
"NVIDIA","RTX 5090","Nvidia GeForce RTX 5090 Review","https://www.techspot.com/review/2944-nvidia-geforce-rtx-5090/","Exciting times for us computer enthusiasts as we can finally showcase the new GeForce RTX 5090 and the next generation of Nvidia GPUs, codenamed Blackwell, with the new flagship graphics card priced at $2,000.
It's been two years since Nvidia released the mighty GeForce RTX 4090, an insane $1,600 GPU that smashed the previous-generation flagship by a 60% margin – that is, 60% faster on average at 4K. This made it an extremely powerful and exciting option for high-end gaming, even if it was undeniably expensive.
So, what's on offer here, and how can Nvidia justify a $2,000 price tag for the RTX 5090?
Nvidia has faced some challenges this generation. While the RTX 50 series takes advantage of cutting-edge technologies such as PCI Express 5.0 and GDDR7 memory, the GPU is built using the same TSMC 4N process as the previous generation. Without improvements to the production node, significant performance gains would require an architectural overhaul, which isn't yet on the table.
RTX 4090 FE on the left, 5090 FE on the right
Therefore, Nvidia's solution was to create a bigger and more powerful GPU. The die is now 23% larger, featuring 33% more cores. It comes equipped with 32 GB of 28Gbps GDDR7 memory on a 512-bit wide memory bus, delivering a bandwidth of 1,792 GB/s – a hefty 78% increase over the RTX 4090.
The RTX 5090 is a powerhouse, but it comes with an even steeper price tag, making it 25% more expensive than the RTX 4090. Given that price increase, we expect it to deliver performance far beyond what the specs suggest.
Before we dive in and get into the blue bar graphs, let's take a look at how Nvidia's Founders Edition version of the RTX 5090 performs compared to the RTX 4090 FE card. For this comparison, we tested The Last of Us Part 1 at 4K with maxed-out settings.
After an hour of load inside an enclosed ATX case, the RTX 5090 reached a peak GPU temperature of 73°C, which is remarkable given how quiet and compact the card is. The fan speed peaked at 1,600 RPM and remained inaudible over our case fans, which are already very quiet.
The cores averaged a clock speed of 2,655 MHz, while GPU power averaged 492 watts. The memory temperature peaked at 88°C, with an operating frequency of 2,334 MHz, providing a transfer speed of 28 Gbps.
In comparison, the RTX 4090 FE model peaked at 68°C, with a memory temperature of 80°C, and its fans spinning just below 1,500 RPM. Clearly, the RTX 5090 runs slightly hotter and louder. However, given that the RTX 5090 consumed, on average, 35% more power during testing and is a significantly smaller card, these results are nothing short of remarkable.
We are incredibly impressed with what Nvidia has achieved here. The RTX 5090 might be the most impressive graphics card we've ever seen. You would never guess, just by looking at it, how much thermal load this cooler can handle so efficiently. It's an outstanding achievement. Now, let's see how it performs.
Starting with Marvel Rivals at 1440p, we see that the RTX 5090 delivers 30% more performance than the RTX 4090. While this is a decent performance improvement, factoring in the 25% price increase makes it considerably less exciting.
At 4K resolution, the margin increases slightly to 33%. This is a solid uplift, but the extreme price premium dampens the enthusiasm.
S.T.A.L.K.E.R. 2 isn't the most optimized game, and as a result, the RTX 5090 maxes out at 94 fps at 1440p. This makes it only 22% faster than the RTX 4090, offering a very mild performance gain.
At 4K, however, the RTX 5090 achieves a more reasonable 42% performance gain, rendering an average of 71 fps.
Next, we have Counter-Strike 2. At 1440p, the RTX 5090 is slightly slower than the RTX 4090, although the 1% lows are notably stronger. It's worth mentioning that the RTX 5090 was slower than the RTX 4090 at 1080p in multiple instances. This suggests a possible overhead issue with the Blackwell architecture, or perhaps the RTX 5090's large core count isn't being efficiently utilized at lower resolutions. Further investigation is needed here.
Even at 4K, the RTX 5090 only offers an 8% performance increase over the RTX 4090. The issue doesn't appear to be a CPU bottleneck, given the higher frame rates observed at 1440p.
Performance in God of War Ragnarök is outstanding at 1440p, hitting 268 fps on the ultra preset. However, this is only 22% faster than the RTX 4090, which is disappointing given the 25% higher cost.
At 4K, the RTX 5090 scales better, achieving a 36% performance improvement with 195 fps compared to 143 fps on the RTX 4090 – a much more favorable result.
In Delta Force, the RTX 5090 provides just 17% more performance than the RTX 4090 at 1440p. However, frame rates here are extreme and likely approaching a CPU bottleneck.
At 4K, the margin extends to 27%, rendering 160 fps. While this is an improvement, it's still not an impressive uplift, especially considering the 25% higher price and the two-year gap between releases.
Space Marine 2 is a very CPU-limited game, and at 1440p, we appear to be hitting the limits of the 9800X3D processor. Oddly, the RTX 5090 is 4% slower than the RTX 4090 here. As observed in other instances at 1080p, this could indicate an overhead issue or inefficiencies in workloads that limit the RTX 5090's performance.
At 4K, the RTX 5090 resolves this problem, delivering a 30% performance increase over the RTX 4090. While this is a decent uplift, it is undercut by the 25% price hike.
In Star Wars Jedi: Survivor, the RTX 5090 delivers just a 14% improvement over the RTX 4090 at 1440p. However, with an average of 191 fps, performance remains impressive overall.
At 4K, the RTX 5090 crosses the 100 fps threshold with 102 fps, making it 21% faster than the RTX 4090. Still, this is a disappointing margin given the higher cost.
In A Plague Tale: Requiem, the RTX 5090 delivers a 21% performance improvement over the RTX 4090 at 1440p. The results are partly CPU-limited, as suggested by similar 1% lows between the two GPUs.
At 4K, the RTX 5090 pulls ahead with a 42% performance uplift, making this one of the better margins seen in the benchmarks.
In Cyberpunk 2077: Phantom Liberty, the RTX 5090 struggles to deliver noteworthy gains at 1440p, with just a 19% improvement over the RTX 4090. The 1% lows are also similar, indicating other system limitations may be at play.
At 4K, the margin improves to 32%. While the overall performance is excellent, this result remains underwhelming. It's worth noting that the second-highest preset was used, and ray tracing was not enabled for this test.
Frame rates in Dying Light 2 using the high preset are extreme at 1440p, reaching 198 fps with the RTX 5090. However, this makes it only 24% faster than the RTX 4090.
Even at 4K, the performance gain remains modest at 25% over the RTX 4090, which scales directly with the 25% price increase.
In Dragon Age: The Veilguard, frame rates are limited to just under 130 fps at 1440p using the ultra preset, which selectively applies some ray tracing effects. While the focus of this portion of the review is on rasterization performance, ray tracing plays a role here.
When increasing the resolution to 4K, the RTX 5090 averages 96 fps, only 10% faster than the RTX 4090. This is a very disappointing result.
War Thunder runs at extremely high frame rates, even with the highest quality preset enabled. At 1440p, the performance is clearly CPU-limited, which we confirmed by testing at 1080p.
Moving to 4K removes the CPU bottleneck, but even then, the RTX 5090 is only 15% faster than the RTX 4090. Granted, with frame rates well over 300 fps, performance is more than sufficient for gameplay, but in terms of relative performance, the RTX 5090 is underwhelming here.
Marvel's Spider-Man Remastered is heavily CPU-limited at 1440p, with both the RTX 4090 and RTX 5090 capped at 222 fps.
At 4K, the CPU bottleneck is mostly removed, but the RTX 5090 still appears slightly limited, averaging 212 fps. As a result, the RTX 5090 is just 26% faster than the RTX 4090.
Hogwarts Legacy is another title that is mostly CPU-limited at 1440p, resulting in similar performance between the RTX 4090 and RTX 5090.
Increasing the resolution to 4K allows the RTX 5090 to pull ahead, delivering a 31% performance improvement. While the performance is excellent overall, the value remains questionable.
In The Last of Us Part I, the RTX 5090 provides a solid performance uplift at 1440p, where it is 28% faster than the RTX 4090, averaging 204 fps. This results in excellent overall performance.
At 4K, the RTX 5090 offers a 40% performance increase, averaging 125 fps. This is a strong result, especially when compared to most other titles.
The RTX 5090 achieves over 100 fps in Star Wars Outlaws at 1440p using the ultra preset. With ray tracing forced on, the RTX 5090 is 22% faster than the RTX 4090.
Oddly, the margin decreases at 4K, where the RTX 5090 is just 19% faster than the RTX 4090. Typically, we expect the RTX 5090 to show greater advantages at higher resolutions, but that isn't the case here.
Finally, in Starfield, the RTX 5090 is only 4% faster than the RTX 4090 at 1440p using ultra-quality settings, limiting performance to 125 fps.
At 4K, the RTX 5090 improves slightly but is still just 7% faster than the RTX 4090. There seems to be a limitation in this title that prevents the RTX 5090 from delivering the margins seen in other games at 4K.
Although we did not include 1080p data for individual games, here are the average results across the 17 games tested. As seen, both the RTX 4090 and RTX 5090 are heavily CPU-limited at this resolution, making them ideal for CPU benchmarking rather than GPU evaluation.
Even at 1440p, the RTX 5090 is often heavily limited by the CPU, resulting in just a 12% performance improvement over the RTX 4090 across the 17 games tested.
Now at 4K we can see the potential of the GeForce RTX 5090 where it delivers an average performance improvement of 27%, which looks solid on raw numbers but it's somewhat disappointing from a value perspective considering it costs 25% more than the 4090. This is why we've been joking internally, calling it the 4090 Ti as it really feels like that's what it is.
Even if the RTX 5090 maintained the same $1,600 MSRP as the RTX 4090, it would still feel underwhelming as a next-generation flagship GPU. For comparison, the RTX 4090 was on average 60% faster than the RTX 3090 Ti, while launching at a lower price. It was also 73% faster than the RTX 3090 with only a 7% price increase. By comparison, the RTX 5090's performance and value fall far short of expectations for a generational leap.
Now, let's look at power consumption. Most of our power data was recorded at 1440p, which is not ideal for measuring the full power usage of the RTX 5090, but we supplemented this with additional tests for clarity. In Starfield at 1440p, the RTX 5090 increased power consumption by 12% compared to the RTX 4090.
In Star Wars Outlaws, we observed a 17% increase in power usage at 1440p, rising from 532 watts to 624 watts. Interestingly, in Space Marine 2, where the RTX 5090 performed worse than the RTX 4090 at 1440p, power consumption decreased by 15%, demonstrating that the RTX 5090 is highly efficient when not operating at full load.
To better evaluate power usage, we re-tested the Radeon RX 7900 XTX, RTX 4090, and RTX 5090 at 4K in three games where the RTX 5090 performed well: Dying Light 2, Cyberpunk 2077, and A Plague Tale: Requiem.
In these tests, the RTX 5090 increased power consumption by 37 – 41%, depending on the game. These results align more closely with the performance gains seen in these titles. Note that this data combines both CPU and GPU power usage, as GeForce GPUs are known to increase CPU load in certain scenarios, which can reduce GPU load and, in turn, lower power consumption.
Finally, we re-ran those same power tests with a 60 fps cap, which yielded some interesting results. In A Plague Tale: Requiem, power consumption for the RTX 5090 was nearly identical to the RTX 4090, with just a 2% increase. In Cyberpunk 2077, the RTX 5090 showed an 8% increase, while in Dying Light 2, it consumed 15% more power.
Metro Exodus Enhanced remains one of the few ray tracing games that provides a truly transformative experience with ray tracing enabled, so we felt it was important to include.
As a side note before we show you the results, we've encountered issues testing Metro Exodus Enhanced with Radeon GPUs as of late. While the game has worked in the past, enabling ray tracing now causes system crashes with Radeon GPUs, regardless of whether AMD or Intel systems are used. AMD has replicated the problem and is aware of the issue, but unfortunately, a fix was not available in time for this review. As a result, we decided to exclude Radeon data and focus solely on the RTX 4090 and RTX 5090 performance.
At 1080p, the RTX 5090 was 21% faster than the RTX 4090, and at 1440p, the margin increased to 33%. We did not test 4K ray tracing performance, as most titles deliver poor and often unplayable performance at that resolution, even with upscaling. However, Metro Exodus Enhanced would likely perform well on both the RTX 4090 and RTX 5090.
In Alan Wake II, with quality upscaling enabled, the RTX 5090 was just 19% faster than the RTX 4090 at 1080p. Moving to 1440p did not significantly improve the results, with the RTX 5090 showing only an 18% performance gain.
Overall, these are weak gains for the RTX 5090, and even with ray tracing enabled, the performance only just breaks the 100 fps barrier.
Using the ultra ray tracing preset with quality upscaling, Cyberpunk 2077: Phantom Liberty shows the RTX 5090 performing comparably to the RTX 4090 at 1080p, likely due to CPU limitations.
At 1440p, the RTX 5090 pulls ahead slightly, offering an 11% performance increase with an average of 129 fps.
In Marvel's Spider-Man Remastered, performance is heavily CPU-limited at both 1080p and 1440p. This is problematic, as frame rates are capped at 128 fps at 1440p, which is a limit achieved even by the RTX 4080 Super.
While 4K benchmarks might provide more insight, the 128 fps cap at lower resolutions is concerning. Although this is solid performance overall, for those with high-refresh-rate monitors, it may not be enough. Furthermore, it's unlikely that many users spending $2,000 or more on a graphics card would settle for gaming at 60 fps, which is what would likely occur at 4K without upscaling.
In Dying Light 2 using the high ray tracing preset with quality upscaling, the RTX 5090 achieved an average of 208 fps at 1080p, making it 18% faster than the RTX 4090.
At 1440p, where CPU limitations are not a factor, the RTX 5090 was only 22% faster than the RTX 4090, making this an underwhelming result given the price premium.
With the very high ray tracing preset, the RTX 5090 delivered 123 fps at 1080p with quality upscaling, providing a 34% performance improvement over the RTX 4090.
At 1440p, the RTX 5090 maintained a similar margin, being 36% faster and rendering an average of 98 fps. While this is a reasonable step forward relative to past products, the overall performance remains less impressive, especially since upscaling is required.
We used a five-game average for the ray tracing data since Metro Exodus Enhanced had to be excluded due to the issues with Radeon GPUs. On average, the RTX 5090 was 14% faster than the RTX 4090 at 1080p with upscaling.
At 1440p, the RTX 5090 showed an average performance increase of just 17%. Notably, even with upscaling, the average frame rate at 1440p was just 123 fps – far from impressive for a graphics card priced at $2,000.
Here's how the current and previous-generation mid-range to high-end GPUs compare in terms of value, based on MSRP. At $2,000, the RTX 5090 offers only a 1.5% improvement in value per frame compared to the RTX 4090.
In other words, after more than two years, there's no meaningful improvement in cost per frame. The RTX 5090 is essentially just a faster RTX 40 series GPU.
If we consider the best retail pricing for mid-2024 and assume the RTX 5090 will sell for $2,000, the value proposition looks slightly better. However, realistically, do we believe the RTX 5090 will actually sell for $2,000? Probably not.
If anything, the retail price is likely to climb higher, making the value situation even worse. At $2,000, the RTX 5090 already represents poor value, and anything higher would make it an even tougher sell.
The GeForce RTX 5090 is now the world's fastest gaming GPU – no surprise there. What is shocking, however, is that in our testing, it was on average just 27% faster than the RTX 4090 at 4K, while costing at least 25% more.
This is why we've referred to it as the RTX 4090 Ti – because, let's be honest, that's exactly what it is. Nvidia has tried to disguise this by marketing DLSS 4 multi-frame generation as a game-changing feature, akin to dangling a shiny set of keys to distract gamers.
Speaking of DLSS 4, we haven't mentioned frame generation much in this review, despite Nvidia heavily promoting it as a key feature of the GeForce 50 series. This omission might seem odd, but we believe frame generation deserves a separate, dedicated analysis.
We're already working on an in-depth DLSS 4 review, which will explore the technology in greater detail soon. The reason we tackle topics like frame generation and upscaling separately is that testing these features properly is complex. It's less about frame rates and more about image quality and, in the case of frame generation, latency.
The reason we tackle topics like frame generation and upscaling separately is that testing these features properly is complex. It's less about frame rates and more about image quality and, in the case of frame generation, latency.
To summarize briefly, frame generation doesn't deliver what Nvidia's marketing claims. It's not a true performance-enhancing feature; you're not genuinely going from 60 fps to 120 fps. Instead, you're getting the appearance of smoother gameplay, albeit with potential graphical artifacts, but without the tangible benefits of higher frame rates – such as improved input latency.
That doesn't mean frame generation is useless or that it's not a good technology. It can be helpful in certain scenarios, but Nvidia has weaponized the feature to mislead consumers, making claims like the upcoming RTX 5070 being faster than the RTX 4090, which is fundamentally untrue.
We also strongly believe that showcasing frame generation performance in benchmark graphs is misleading. And while Nvidia would love for us to do just that, we see this as a slippery slope for gamers – a race to the bottom, where winning benchmarks would become about who can spit out the most amount of interpolated frames... input and visual quality be damned.
As it stands, DLSS 3 and DLSS 4 frame generation are best described as frame-smoothing technologies. Under the right conditions, they can be effective, but they don't truly boost FPS performance. Moreover, they're entirely unsuitable for competitive shooters or fast-paced games where the goal of high frame rates is to reduce input latency. Nvidia's narrative that all gamers will or should use frame generation couldn't be further from reality.
Moving on to another topic, about CPU performance, it's clear from the 1440p data we gathered that anyone investing in an RTX 5090 needs a high-end CPU, such as the 9800X3D. Even with the Zen 5 3D V-Cache processor, you'll frequently encounter CPU bottlenecks, especially if you aim for high refresh rates with ray tracing enabled.
Speaking of ray tracing, you're almost certainly going to find reviews where the RT performance of the RTX 5090 relative to the 4090 is more impressive than what we saw for the majority of our testing, and this will come down to the quality settings used.
Our testing focused on real-world scenarios that prioritize frame rates above 60 fps, as we believe most gamers spending $2,000 on a GPU won't settle for console-like frame rates.
But in an effort to provide a bit more context, for example, in Black Myth: Wukong, we tested at 1440p using DLSS quality upscaling, where the RTX 5090 delivered 98 fps – a 24% improvement over the RTX 4090. But if we disable upscaling, which we feel most gamers using ray tracing won't do, the frame rate of the 5090 drops to 64 fps, but this also meant that it was now 45% faster than the 4090, so a far more impressive margin here.
This is comparable to what we see at 4K using DLSS upscaling, though again we're only gaming at around 60 fps, which some gamers will find acceptable, but I personally find it less than desirable, especially when spending so much money.
Ultimately, the point is that the RTX 5090 can be 40-50% faster than the RTX 4090, depending on the game and settings. However, as demonstrated in this review, when targeting high frame rates, the difference is typically much smaller.
All things considered, the GeForce RTX 5090 is an impressive performer that falls short of meeting the expectations for a next-generation flagship GPU. It doesn't move the needle forward in terms of value or innovation and could easily fit into the GeForce 40 series lineup. If Nvidia had launched this as an RTX 4090 Ti, few would have batted an eye.
We understand that Nvidia couldn't do much given the limitations of the current process node. However, they still could have delivered a more exciting product series. Even at $1,600, the RTX 5090 would have been far more appealing – still not amazing, but much better than it is now.
Without a process node upgrade, this release doesn't come close to the leap we saw from the RTX 3090 to the RTX 4090, which was vastly more significant. It's also clear that as Nvidia cements its position as the leader in AI hardware, GeForce seems to have taken a back seat to the big money in AI (just check out this graph, it's insane).
We still expect the RTX 5090 to age well. While today's 27% average performance gain over the RTX 4090 is underwhelming, this margin will likely increase over time, potentially reaching 40% in more games.
Unfortunately, this also means the more affordable models in the GeForce RTX 50 series will probably be underwhelming, offering only minor performance gains over the GPUs they replace. Nvidia could have addressed this by providing better VRAM configurations.
For example, 12 GB on the RTX 5070 is simply unacceptable – it should have at least 16 GB. If Nvidia had done this, the RTX 5070 might have been a worthwhile upgrade over the RTX 4070 and a much more significant step up from the RTX 3070.
For those looking for a more positive take, the good news is that the RTX 5090 is faster than the RTX 4090, pushing 4K gaming closer to high-refresh-rate experiences. If you already had oodles of money to blow on a graphics card and missed out on the RTX 4090, the RTX 5090 could be a great addition to your gaming setup.
In summary, the RTX 5090 is 25% more expensive than the RTX 4090, delivers an average of 27% more performance, includes 33% more VRAM, and consumes around 30% more power. Interpret that as you like. For now, our review is complete – with a closer look at DLSS 4 coming soon  – let us know your thoughts on Nvidia's new flagship graphics card in the comments.

About
Ethics Statement
Terms of Use
Privacy Policy
Manage Ad Consent

TECHSPOT : Tech Enthusiasts, Power Users, Gamers
TechSpot is a registered trademark and may not be used by third parties without express written permission.
TechSpot is represented by Future PLC. Learn about advertising opportunities.
© 1998 - 2025 TechSpot, Inc. All Rights Reserved."
"NVIDIA","RTX 5090","Nvidia GeForce RTX 5090 FE review","https://www.pcgamer.com/hardware/graphics-cards/nvidia-geforce-rtx-5090-fe-review/","When you purchase through links on our site, we may earn an affiliate commission. Here’s how it works.

This is one of those times where I kinda want to give multiple scores. The GPU itself is a decent improvement over the RTX 4090, with more, faster memory, more cores, and a gorgeous chassis. But in terms of brute force rendering it's only incrementally faster in comparison with the performance bumps from Turing to Ampere to Ada. The future-focused AI chops and the marvel of Multi Frame Generation, however, I will praise to the high heavens. They may be 'fake frames', but it matters not a whit when you're gaming at those ultra smooth, ultra-high ray traced settings.

PC Gamer's got your back




Our experienced team dedicates many hours to every review, to really get to the heart of what matters most to you. Find out more about how we evaluate games and hardware.

There is an alternative 2025 where you get the Nvidia RTX 5090 of your dreams. That's a timeline where Nvidia has busted Apple's grip on TSMC's most advanced process nodes, managed to negotiate an unprecedented deal on silicon production, and worked some magic to deliver the same sort of generational rendering performance increases we've become used to since the RTX prefix was born.
And it's a 2025 where Nvidia hasn't slapped a $400 price hike on the most powerful of its new RTX Blackwell graphics cards.
But in this timeline, the RTX 5090 is an ultra enthusiast graphics card that is begging us to be more realistic. Which, I will freely admit, sounds kinda odd from what has always been an OTT card. But, in the real world, a GB202 GPU running on a more advanced, smaller process node, with far more CUDA cores, would have cost a whole lot more than the $1,999 the green team is asking for this new card. And would still maybe only get you another 10–20% higher performance for the money—I mean, how much different is TSMC's 3 nm node to its 4 nm ones?
The RTX 5090 is a new kind of graphics card, however, in terms of ethos if not in silicon. It's now the best graphics card you can buy, but is also a GPU designed for a new future of AI processing, and I don't just mean it's really good at generating pictures of astronauts riding horses above the surface of the moon: AI processing is built into its core design and that's how you get a gaming performance boost that is almost unprecedented in modern PC graphics, even when the core at its heart hasn't changed that much. Though it has at least changed more than the Nvidia RTX 5080 GPU has...
✅ You want the best: If you want to nail triple figure frame rates in the latest 4K games, then you're going to need the might and magic of Multi Frame Gen, and that's only available with the RTX 50-series cards. And yes, I do like alliteration. ✅ You to get in on the ground floor of neural rendering: The RTX Blackwell GPUs are the first chips to come with a full set of shaders that will have direct access to the Tensor Cores of the card. That will enable a new world of AI-powered gaming features... when devs get around to using them in released games.✅ You're after a hyper-powerful SFF rig: The Founders Edition is deliciously slimline, and while it generates a lot of heat it will fit in some of the smallest small form factor PC chassis around.
❌ You need to ask the price: With a $400 price hike over the RTX 4090, the new RTX 5090 is a whole lot of cash at its $1,999 MSRP. The kicker, however, is that you'll be lucky to find one at that price given the third-party cards are looking like $2,500+ right now.
The new RTX Blackwell GPU is… fine. Okay, that's a bit mean, the GB202 chip inside the RTX 5090 is better than fine, it's the most powerful graphics core you can jam into a gaming PC. I'm maybe just finding it a little tough not to think of it like an RTX 4090 Ti or Ada Titan. Apart from hooking up the Tensor Cores to the shaders, via a new Microsoft API, and a new flip metering doohicky in the display engine, it largely feels like Ada on steroids.
The software suite backing it up, however, is a frickin' marvel. Multi Frame Generation is giving me ultra smooth gaming performance, and will continue to do so in an impressively large number of games from day one.
The nexus point between hardware and software is where the RTX 5090 thrives. When everything's running like it should I'm being treated to an unparalleled level of both image fidelity and frame rates.
It's when you look at the stark contrast between a game such as Cyberpunk 2077 running at 4K native in the peak RT Overdrive settings, and then with the DLSS and 4x Multi Frame Gen bells and whistles enabled that it becomes hard to argue with Nvidia's focus on AI modeling over what it is now, rather disdainfully calling brute force rendering.
Sure, the 30% gen-on-gen 4K rendering performance increase looks kinda disappointing when we've been treated to a 50% bump from Turing to Ada and then a frankly ludicrous 80% hike from Ampere to Ada. And, if Nvidia had purely been relying on DLSS upscaling alone to gild its gaming numbers, I'd have been looking at the vanguard of the RTX 50-series with a wrinkled nose and a raised eyebrow at its $2K sticker price.
The nexus point between hardware and software is where the RTX 5090 thrives.
But the actual gaming performance I'm seeing out of this card in the MFG test builds—and with the DLSS Override functionality on live, retail versions of games—is kinda making me a a convert to this new AI world in which we live. I'm sitting a little easier with the idea of 15 out of 16 pixels in my games getting generated by AI algorithms when I'm playing Alan Wake 2 at max 4K settings just north of 180 fps, Cyberpunk 2077's Overdrive settings at 215 fps, and Dragon Age: Veilguard at more than 300 fps.
Call it frame smoothing, fake frames, whatever, it works from a gaming experience perspective. And it's not some laggy mess full of weird graphical artifacts mangled together in order to hit those ludicrous frame rates, either. Admittedly, there are times where you can notice a glitch caused by either Frame Gen or the new DLSS Transformer model, but nothing so game or immersion breaking that I've wanted to disable either feature and flip back to gaming at 30 fps or not run at top settings.
There are also absolutely cases where the DLSS version looks better than native res and times where those extra 'fake frames' are as convincing as any other to the naked eye. Honestly you're going to have to be really looking for problems from what I've seen so far. And I've been watching side-by-side videos while they export, where you can literally watch it move one frame at a time; the frame gen options stand up incredibly well even under such close scrutiny.
If that noggin'-boggling performance were only available in the few games I've tested it with at launch then, again, I would be more parsimonious with my praise. But Nvidia is promising the Nvidia App is going to be offering the DLSS Override feature for 75 games and apps to turn standard Frame Gen over to the new frame multiplier. And you still don't need to log in to the app to be able to flip the Multi Frame Generation switch.
And, rando PlayStation port aside, most of the games you're going to want to play over the next 12 months—especially the most feature-rich and demanding ones—will more than likely include Nvidia's full DLSS feature-set. Unless other deals take precedence… ahem… Starfield.
I will say the switch to the transformer model for DLSS hasn't been the game-changer I was expecting from the demos I witnessed at CES, but it's at the very least often better than the standard convolution neural network in terms of image quality. It's just that it will add in some oddities of its own to the mix and doesn't completely rid us of Ray Reconstruction's ghosting.
But don't get me wrong, more base level rendering grunt would always be welcome, but to get to these sorts of fps numbers with pure rendering power alone is going to take a lot of process node shrinks, more transistors than there are stars in the sky, and a long, long time. Oh, and probably cost a ton of cash, too.
Though even a little more raster power would push those AI augmented numbers up even further, and that's something which will certainly be in my mind as I put the rest of the RTX 50-series through its paces. I, for one, am a little concerned about the RTX 5070 despite those claims of RTX 4090 performance for $549.
The RTX 5090, though, is as good as it gets right now, and is going to be as good as the RTX Blackwell generation gets… until Nvidia decides it wants to use the full GB202 chip. Yields on TSMC's mature 4N node are surely pretty good this far down the line, eh?
Literally impossible to beat with any other hardware on the planet.
And, oh is it ever pretty. With all the comic girth of the RTX 3090 and RTX 4090, they are just stupid-looking cards. I'm always taken aback whenever I pull one out of its box to stick in a PC. Being able to come back to the dual-slot comfort zone is testament to the over-engineering Nvidia has done with the Founders Edition, even if both the RTX 5090 cards I've tested have been some of the squealiest, coil-whiney GPUs I've tested in recent history. But your mileage and your PSU may vary, they certainly don't sound great with the test rig's Seasonic power supply.
Despite being somewhat of a loss-leader for Nvidia, this RTX 5090 Founders Edition is also likely to be as cheap as an RTX 5090 retails for over the next year. With every other AIB version sure to be bigger, and most of them more expensive, the Founders Edition is the card you should covet. And the one you will be disappointed about when you almost inevitably miss out on what will surely be slim inventory numbers.
The GPU at its heart might not be super exciting, but the potential of all the neural rendering gubbins Nvidia is laying down the groundwork for with this generation could change that given time. Right now, however, it feels more like an extension of Ada, and with the outstanding AI augmented performance really symptomatic of where we're at in time.
Still, when it comes to the raw gaming experience of using this svelte new RTX 5090 graphics card, it's literally impossible to beat with any other hardware on the planet.
As a layman, not a huge amount seems to have changed from the Ada architecture through to the GB202 Blackwell GPU. As I've said, on the surface it feels very much like an extension of the Ada Lovelace design, though that is potentially because Blackwell is sitting on the same custom TSMC 4N node, so in terms of core counts and physical transistor space there isn't a lot of literal wiggle room for Nvidia.
There are 21% more transistors in the GB202 versus the AD102, and a commensurate 21% increase in die size. Compare that with the move from the RTX 3090 to RTX 4090, with the switch from Samsung's 8nm node to this same 4N process, Ada's top chip gave us 170% more transistors, but a 3% die shrink.
There are still 128 CUDA cores per streaming multiprocessor (SM), so the 170 SMs of the GB202 deliver 21,760 shaders. Though in a genuine change from Ada, each of those can be configured to handle both integer and floating point calculations. Gone are the dedicated FP32 units of old.
Though, interestingly, this isn't the full top-tier Blackwell GPU. The RTX 5090 has lopped off one full graphics processing cluster, leaving around 2800 CUDA cores on the cutting room floor. I guess that leaves room for a Super, Ti, or an RTX Blackwell Titan down the line if Nvidia deems it necessary.
You are getting the full complement of L2 cache, however, with near 100 MB available to the GPU. But then you are also seeing 32 GB of fast GDDR7 memory, too, on a proper 512-bit memory bus. That means you're getting a ton more memory bandwidth—78% more than the RTX 4090 could offer.
There are deeper, arguably more fundamental changes that Nvidia has made with this generation, however. Those programmable shaders have finally been given direct access to the Tensor Cores, and that allows for what the green team is calling Neural Shaders.
Previously the Tensor Cores could only be accessed using CUDA, but in collaboration with Microsoft, Nvidia has helped create the new Cooperative Vectors API, which allows any shader—whether pixel or ray tracing—to access the matrix calculating cores in both DX12 and Vulkan. This is going to allow developers to bring a bunch of interesting new AI-powered features directly into their games.
And it means AI is deeply embedded into the rendering pipeline. Which is why we do have a new slice of silicon in the Blackwell chips to help with this additional potential workload. The AI Management Processor, or AMP, is there to help schedule both generative AI and AI augmented game graphics, ensuring they can all be processed concurrently in good order.
It's that Cooperative Vectors API which will allow for features such as neural texture compression, which is touted to deliver 7x savings against VRAM usage—ostensibly part of Nvidia's dedicated push to ensure 8 GB video cards still have a place in the future. But it also paves the way for RTX Neural Radiance Cache (to enhance lighting via inferred global illumination), and RTX Neural Materials, RTX Neural Skin, and RTX Neural Faces, which all promise to leverage the power of AI models to get us ever closer to photo realism. At least get us close to the sort of image quality you'll see in offline rendered films and TV.
The new 4th Gen RT Cores aren't to be left out, and come with a couple of new units dedicated to improving ray tracing. Part of that push is something called Mega Geometry, which massively increases the amount of geometry possible within a scene. It reminds me a whole lot of when tessellation was first introduced—the moment you turn off the textures and get down to the mesh layer in the Zorah demo, which showcases the tech, you're suddenly hit by what an unfeasible level of geometry is possible in a real-time scene.
This feature has largely been designed for devs on Unreal Engine 5 utilising Nanite, and allows them to ray trace their geometry at full fidelity. Nvidia has put so much store in Mega Geometry that it has designed the new RT Cores specifically for it.
The final hardware piece of the RTX Blackwell puzzle to be dropped into the new GPU is Flip Metering. The new enhanced display engine has twice the pixel processing capability, and has been designed to take the load away from the CPU when it comes to ordering frames up for the display. The Flip Metering feature is there to enable Multi Frame Generation to function smoothly—displaying all those extra frames in between the rendered ones in good order is vital in order to stop it feeling ""lumpy"". That's not my phrase, that's a technical term from Nvidia's Mr. DLSS, Brian Catanzaro, and he should know.
In terms of the feature set, DLSS itself has also had a potentially big upgrade, too. Previously it used a convolutional neural network (CNN) as the base model for DLSS, which is an image-focused model, and made sense for something so image-focused as upscaling. But it's no longer the cutting edge of AI, so DLSS 4 has switched over to the transformer architecture you will be familiar with if you've used ChatGPT—the GPT bit stands for generative pre-trained transformer.
It's more efficient than CNN, and that has allowed Nvidia to be more computationally demanding with DLSS 4—though I've not really seen much in the way of a performance difference between the two forms in action.
Primarily it seems the transformer model was brought in to help Ray Reconstruction rid itself of the smearing and ghosting it suffers from, though it's also there for upscaling, too. Nvidia, however, is currently calling that a beta. Given my up and down experience with the transformer model in my testing, I can now understand why. It does feel very much like a v1.0 with some strange artifacts introduced for all the ones it helps remove.
I've saved the best new feature for last: Multi Frame Generation. I was already impressed with the original version of the feature introduced with the RTX 40-series, but it has been hugely upgraded for the RTX 50-series and is arguably the thing which will impress people the most while we wait for those neural shading features to actually get used in a released game.
It's also the thing which will really sell the RTX 50-series. We are still talking essentially about interpolation, no matter how much Jen-Hsun wants to talk about his GPU seeing four frames into the future. The GPU will render two frames and then squeeze up to three extra frames in between.
Using a set of new AI models it no longer needs dedicated optical flow hardware (potentially good news for RTX 30-series gamers), and is able to perform the frame generation function 40% faster and with a 30% reduction in its VRAM footprint. That flip metering system now means the GPU's display engine queues up each frame, pacing them evenly, so you get a smooth final experience.
The 5th Gen Tensor Cores have more horsepower to deal with the load, and the AMP gets involved, too, in order to keep all the necessary AI processing around both DLSS and Frame Generation, and whatever else they get up to in the pipeline, running smoothly.
The raw performance of the RTX 5090 is relatively impressive. As I've mentioned earlier, I'm seeing around a 30% improvement in 4K gaming frame rates over the RTX 4090, which isn't bad gen-on-gen. We have been spoiled by the RTX 30- and 40-series cards, however, and that does make this bump seem a little less exciting.
The main increase is all at that top 4K resolution, because below that the beefy GB202 GPU does start to get bottlenecked by the processor. And that's despite us rocking the AMD Ryzen 7 9800X3D in our test rig—I've tossed the RTX 5090 into my own rig with a Ryzen 9 7950X in it and the performance certainly drops.
And in games where the CPU is regularly the bottleneck, even at 4K, the performance delta between the top Ada and Blackwell GPUs is negligible. In Homeworld 3 the 4K performance increase is just under 9%, even worse, at 1080p the RTX 5090 actually takes a retrograde step and drops 7% in comparison.
This is a graphics card built for DLSS, and as such if you hit 4K DLSS Quality settings you're actually rendering at 1440p.
Where the GPU is the star, however, the extra 4K frame rates are matched by the overall increase in power usage. This thing will drain your PSU and I measured the card pulling down nearly 640 W at peak during our extended Metro Exodus benchmark. The commensurate performance increase does, however, follow so the performance per watt at 4K remains the same compared with the RTX 4090.
But yes, it does start to fall down when you drop to 1440p and certainly 1080p. If you were hoping to smash 500 fps at 1080p with this card we might have to have a little chat. It will still draw a ton of power at the lower resolutions, too, which means its performance per watt metrics drop by 15%.
You might say that's not such a biggy considering you'll be looking to play your games at 4K with such a beast of a GPU, but this is a graphics card built for DLSS, and as such if you hit 4K DLSS Quality settings you're actually rendering at 1440p. That 30% 4K uplift figure is then kinda moot unless you're sticking to native rendering alone.
Which you absolutely shouldn't do because Multi Frame Generation is a game-changer, in the most literal sense. The performance difference going from Native, or even DLSS Quality is stark. With Alan Wake 2 now hitting 183 fps, with 102 fps 1% low, it's a glorious gaming experience. Everything in the graphics settings can be pushed to maximum and it'll still fly.
More importantly, the latency is only marginally higher than with just DLSS settings—the work Nvidia has done to pull that down with Multi Frame Generation is a marvel. As is the Flip Metering frame pacing. This is what allows the frames to come out in a smooth cadence, and makes it feel like you're really getting that high-end performance.
Cyberpunk 2077 exhibits the same huge increase in performance, and is even more responsive than Alan Wake 2, with just 43 ms latency when I've got 4x Multi Frame Generation on the go.
And even though Dragon Age: The Veilguard is pretty performant at 4K native, I'll happily take a 289% increase in perceived frame rate, especially when the actual PC latency on that game barely moves the needle. It's 28 ms at 4K native and 32 ms with DLSS Quality and 4x MFG.
Another benefit of the DLSS and MFG combo is that it pulls down the power and thermal excesses of the RTX 5090. I've noticed around a 50 W drop in power consumption with MFG in action, and that means the temps go down, and the GPU clock speed goes up.
Still, the overall combination of high power, high performance, and a new, thinner chassis means that the GPU temperature is noticeably higher than on the RTX 4090 Founders Edition. Running through our 4K native Metro Exodus torture test, the RTX 5090 Founders Edition averages 71 °C, with the occasional 77 °C peak. That's a fair chunk higher than the top Ada, though obviously that's with a far thinner chassis.
For me, I'd take that extra little bit of heat for the pleasure of its smaller footprint. What I will say, however, is that I did experience a lot of coil whine on our PC Gamer test rig. So much so, that Nvidia shipped me a second card to test if there was an issue with my original GPU. Having now tested in my home rig, with a 1600 W EVGA PSU, it seems like the issue arose because of how the Seasonic Prime TX 1600 W works with the RTX 5090, because in my PC the card doesn't have the same constantly pitching whine I experienced on our test rig.
The RTX 5090 being a beastly GPU, I've also taken note of what it can offer creatives as well as gamers. Obviously with Nvidia's AI leanings the thing can smash through a generative AI workload, as highlighted by the way it blows past the RTX 4090 in the UL Procyon image benchmark.
Though the AI index score from the PugetBench for DaVinci Resolve test shows that it's not all AI plain sailing for the RTX 5090. GenAI is one thing, but DaVinci Resolve's use of its neural smarts highlights only a 2.5% increase over the big Ada GPU.
Blender, though, matches the Procyon test, offering over a 43% increase in raw rendering grunt. I'm confident that extra memory bandwidth and more VRAM is helping out here.
PC Gamer test rigCPU: AMD Ryzen 7 9800X3D | Motherboard: Gigabyte X870E Aorus Master | RAM: G.Skill 32 GB DDR5-6000 CAS 30 | Cooler: Corsair H170i Elite Capellix | SSD: 2 TB Crucial T700 | PSU: Seasonic Prime TX 1600W | Case: DimasTech Mini V2
The RTX 4090's 80% performance bump is living in recent memory, rent-free in the minds of gamers.
When is a game frame a real frame? This is the question you might find yourself asking when you hear talk of 15 out of 16 pixels being generated by AI in a modern game. With only a small amount of traditional rendering actually making it onto your display, what counts as a true frame? I mean, it's all just ones and zeros in the end.
So, does it really matter? For all that you might wish to talk about Multi Frame Generation as fake frames and just frame-smoothing rather than boosting performance, the end result is essentially the same: More frames output onto your screen every second. I do understand that if we could use a GPU's pure rendering chops to hit the same frame rates it would look better, but my experience of the Blackwell-only feature is that often-times it's really hard to see any difference.
Nvidia suggests that it would take too long, and be too expensive to create a GPU capable of delivering the performance MFG is capable of, and certainly it would be impossible on this production node without somehow making GPU chiplets a thing. It would be a tall order even just to match the performance increase the RTX 4090 offered over the RTX 3090 in straight rendering.
But that's the thing, the RTX 4090's 80% performance bump is living in recent memory, rent-free in the minds of gamers. Not that that sort of increase is, or should necessarily be expected, but it shows it's not completely beyond the realms of possibility. It's just that TSMC's 2N process isn't even being used by Apple this year, and I don't think anyone would wait another year or so for a new Nvidia series of GPUs.
Though just think what a die-shrink and another couple year's maturity for DLSS, Multi Frame Gen, and neural rendering in general might mean for the RTX 60-series. AMD, be afraid, be very afraid. Or, y'know, make multiple GPU compute chiplets a thing in a consumer graphics card. Simple things, obvs.
Still, if the input latency had been an issue then MFG would have been a total non-starter and the RTX Blackwell generation of graphics cards would have felt a lot less significant with its rendering performance increase alone. At least at launch. The future-gazing features look exciting, but it's far too early to tell just how impactful they're going to be until developers start delivering the games that utilise the full suite of neural shading features.
It would have certainly been a lot tougher for Nvidia to slap a $2,000 price tag onto the RTX 5090 and get away with it. With MFG it can legitimately claim to deliver performance twice that of an RTX 4090. Without it, a sole 30% 4K performance bump wouldn't have been enough to justify a 25% increase in pricing.
What I will say in Nvidia's defence on this is that the RTX 4090 has been retailing for around the $2,000 mark for most of its existence, so the real world price delta is a lot smaller. At least compared to the RTX 5090's MSRP. How many, and for how long we'll see actual retail cards selling for this $1,999 MSRP, however, is tough to say. It's entirely likely the RTX 5090's effective selling price may end up closer to the $2,500 or even $3,000 mark once the AIBs are in sole charge of sales as the Founders Edition stock runs dry.
I can see why Nvidia went with the RTX 5090 first as the proponent of Multi Frame Generation. The top-end card is going to benefit far more from the feature than cards lower down the stack, with less upfront rendering power to call on. Sure, Nvidia claims the RTX 5070 can hit RTX 4090 performance with MFG, but I'm going to want to see that in a few more games before I can get onboard with the claims.
The issue with frame generation has always been that you need a pretty high level of performance to start with, or it ends up being too laggy and essentially a bit of a mess. The most demanding games may still be a struggle for the RTX 5070 even with MFG, but I guess we'll find out soon enough come February's launch.
Until then, I'll just have to sit back and bask in the glorious performance Nvidia's AI chops are bringing in alongside the RTX 5090.
This is one of those times where I kinda want to give multiple scores. The GPU itself is a decent improvement over the RTX 4090, with more, faster memory, more cores, and a gorgeous chassis. But in terms of brute force rendering it's only incrementally faster in comparison with the performance bumps from Turing to Ampere to Ada. The future-focused AI chops and the marvel of Multi Frame Generation, however, I will praise to the high heavens. They may be 'fake frames', but it matters not a whit when you're gaming at those ultra smooth, ultra-high ray traced settings.
Dave has been gaming since the days of Zaxxon and Lady Bug on the Colecovision, and code books for the Commodore Vic 20 (Death Race 2000!). He built his first gaming PC at the tender age of 16, and finally finished bug-fixing the Cyrix-based system around a year later. When he dropped it out of the window. He first started writing for Official PlayStation Magazine and Xbox World many decades ago, then moved onto PC Format full-time, then PC Gamer, TechRadar, and T3 among others. Now he's back, writing about the nightmarish graphics card market, CPUs with more cores than sense, gaming laptops hotter than the sun, and SSDs more capacious than a Cybertruck.

PC Gamer is part of Future plc, an international media group and leading digital publisher. Visit our corporate site.

©
Future Publishing Limited Quay House, The Ambury,
Bath
BA1 1UA. All rights reserved. England and Wales company registration number 2008885."
"NVIDIA","RTX 5090","Nvidia GeForce RTX 5090 review: a new king of 4K is here","https://www.theverge.com/2025/1/23/24349619/nvidia-rtx-5090-review-test-benchmark","The next-gen GPU battle begins with Nvidia alone at the top.
If you buy something from a Verge link, Vox Media may earn a commission. See our ethics statement.
If you buy something from a Verge link, Vox Media may earn a commission. See our ethics statement.
Nvidia’s RTX 4090 was a beast of a graphics card. It was a huge GPU that delivered true 4K gaming for those who really wanted it and a huge jump in performance over the previous-generation RTX 3090. Naturally, the company’s latest top-of-the-line GPU, the GeForce RTX 5090, now has a lot of expectations riding on it when it arrives on January 30th.
But at $1,999, the RTX 5090 is steep — $400 more than the RTX 4090 Founders Edition at its launch. (It could run you even more in the form of a new power supply or higher electricity bills, especially if you want to run games like Cyberpunk 2077 in 4K.)
For those extra dollars, you’re getting hardware improvements in the form of 32GB of VRAM and a redesigned Founders Edition card that’s shrunk down enough to fit inside a small form factor PC. But the greatest benefit might actually be in the software: the RTX 5090 is the first Nvidia GPU with access to the new DLSS 4 Multi Frame Generation, which promises huge frame rate increases. We’re talking frame rates not just quadrupling but increasing by more than 8x in some cases.
That extra processing power naturally translates to better performance, making the RTX 5090 the new king of 4K gaming. It’s a $1,999 GPU for anyone who wants the best 4K gaming experience, developers interested in AI performance, and creators who want to accelerate video editing.
But the RTX 5090 isn’t as big an improvement over the RTX 4090 as that previous-generation card was over the RTX 3090, especially in games that don’t yet support DLSS 4 Multi Frame Gen. While it’s still a little too early to tell how widely developers will adopt Nvidia’s latest DLSS tech, I think it may turn out to be a lot more important than the actual hardware itself.
Sleek two-slot design32GB of VRAMDLSS 4 makes full ray tracing a reality
575W power draw$1,999 is a lot of cashNot a big performance boost over the RTX 4090 
When I first unboxed the RTX 5090, I was surprised at how small the Founders Edition card was. Although the new GPU is still as long and wide as an RTX 4090, it’s nowhere near as thick. Where the 4090 Founders Edition took up three slots and then some, the two-slot 5090 is so slim it will fit inside small form factor PCs, giving people much more flexibility this time around.
The key to the two-slot design is a new cooling system. For the Founders Edition card, Nvidia moved to double flowthrough fans that suck cooler air from below and exhaust it above the RTX 5090 into the rest of the case, rather than out the top and back of the card. The PCB moves to the center of the card, with heat pipes going out in both directions that get cool air from this new fan layout.
The RTX 5090’s power connector is slightly angled to make it easier to fit into cases where the side panel comes close to touching the GPU power cable. Nvidia bundles a dongle power adapter that uses four regular PCIe eight-pin power connectors, much like the adapter for the RTX 4090, though the cables are slightly more flexible this time. This new GPU supports an updated 12V-2x6 connector that has shorter sensing pins and slightly longer conductor terminals, but existing 12VHPWR connectors will work fine with the RTX 5090, so you don’t need to swap anything for a new cable.
Nvidia recommends a 1,000-watt power supply because the RTX 5090 can draw a massive 575 watts of power — 125W more than the RTX 4090. The 12VHPWR cables are rated at 600 watts, so there’s little room for overclocking. I was hoping that Nvidia’s RTX 50 series would focus more on efficiency rather than push the power draw even higher, especially as the RTX 4090 was already a big jump from the 350W RTX 3090. It’s disappointing that power draw in flagship GPUs has essentially reached the level of what a single cable can deliver, and I sure hope whatever comes next doesn’t require two 12VHPWR cables.
For both my 4K and 1440p testing, I’ve paired Nvidia’s RTX 5090 with AMD’s latest Ryzen 9 9800X3D processor and Asus’ 32-inch 4K OLED PG32UCDP monitor. That monitor is the perfect companion for the RTX 5090: the 240Hz refresh rate is more than sufficient for the frame rates the card can achieve at 1440p, and games look fantastic on an OLED panel with HDR enabled.
I’ve put the RTX 5090 head-to-head with the RTX 4090 and AMD’s closest competitor, the RX 7900 XTX, to see how Nvidia’s latest Blackwell architecture — which is a combination of hardware and software support improvements — performs across a variety of games. I’ve tested the latest titles like Black Myth: Wukong, Call of Duty: Black Ops 6, Assassin’s Creed Mirage, and some of the usual choices like Cyberpunk 2077, Metro Exodus Enhanced, and even Shadow of the Tomb Raider.
These games were all tested at very high or ultra settings on the RTX 5090, with a variety of ray tracing and upscaling options in games that support them, as well as games without DLSS or ray-tracing support.
The star of the 4K show here is DLSS 4 and Nvidia’s new Multi Frame Generation technique that can generate up to three additional frames per every traditionally rendered frame. It uses the latest AI graphics models, powered by an updated transformer architecture that’s similar to what’s found in ChatGPT to multiply frame rates beyond what a GPU is normally capable of. Essentially, the Tensor cores in the GPU take the rendered frame, figure out what the next one to three frames should look like, generate them, and insert them before the next rendered frame. It’s how Nvidia can promise big frame rate increases in games like Cyberpunk 2077 at 4K with full ray tracing enabled.
Nvidia is really selling the RTX 50-series on this new Multi Frame Gen technology as a result, and some of its marketing has already reignited the long-running “fake frames” argument about DLSS Frame Generation. Some PC gamers have argued that this technique, which Nvidia introduced originally with DLSS 3, is not reflective of the true performance of GPUs.
I think this debate will run for the entirety of this generation of GPUs, but with more than 80 percent of RTX owners enabling DLSS in games already, they’re likely to move to Frame Generation if they upgrade to the latest Nvidia cards.
Most of the real issues with DLSS Frame Generation come down to the impact of latency and image quality. Instead of the Convolutional Neural Networks (CNN) model used in previous versions of DLSS, Nvidia has switched to a new transformer model for DLSS 4. This means frame generation models are faster and use less VRAM. The switch also fixes some of the image ghosting and weird artifacts we’ve seen in Cyberpunk 2077, and it does all of this while massively improving frame rates with Multi Frame Gen and only adding a minuscule 6ms more latency. 
That feels like a good tradeoff to me, but Multi Frame Generation works best when the frame rate after Super Resolution upscaling is already decent. Super Resolution is another DLSS feature that helps improve frame rates and reduce input latency by rendering at a lower resolution and using AI to upscale to the output resolution. For example, a game that would render at 27fps in 4K with DLSS turned off might play at 60fps with Super Resolution. Multi Frame Generation can bump that to 200fps and greatly improve the motion clarity, but it will still feel like 60fps because it’s not the same as the reduced input latency benefits that 200fps rendered traditionally would deliver. 
Nvidia supplied early DLSS 4-compatible builds of Cyberpunk 2077 and Alan Wake 2 for testing, and I focused most of my Multi Frame Gen testing on Cyberpunk 2077. While the game is over four years old, developer CD Projekt Red still manages to add the latest upscaling and ray-tracing techniques into Cyberpunk 2077 on a regular basis, pushing modern GPUs to the max to get path tracing running. That makes it a popular choice for benchmarking and image analysis.
At 4K resolution, with full path tracing, ultra settings, and no DLSS upscaling or frame generation, Cyberpunk 2077 ran at just 34fps. That’s nearly 42 percent faster than an RTX 4090, but you really need Super Resolution and Multi Frame Generation here to help the game get way above 60fps with everything cranked up. That shows you how important software improvements have become in modern GPUs, and while DLSS 4 won’t settle the debate about “fake frames,” the improvement it brings is undeniable.
You can also choose from 2x Multi Frame Gen (DLSS 3 Frame Generation) all the way up to 4x Multi Frame Gen. I took the 34fps fully path traced version of Cyberpunk 2077 to a massive 371fps with DLSS ultra performance, 4x Multi Frame Gen, and the CNN model. If you opt for the Transformer model, you can avoid the image ghosting issues of CNN and still get 211fps at DLSS quality mode and 4x Multi Frame Gen, without needing to drop down to the DLSS performance modes that offer less image quality. That’s more than six times the base 34fps. I think it’s worth the very small performance tradeoff to stick to Transformer in Cyberpunk 2077. 
Most of my 4K testing was performed without DLSS 4 Multi Frame Gen, simply because most developers haven’t updated their games yet. While the company is promising more than 75 games and apps will upgrade to DLSS 4 Multi Frame Gen when the RTX 5090 launches on January 30th, some, like Black Myth: Wukong, won’t be upgraded until later this year. If you really can’t wait for developers to update their games, there will be an option to force DLSS Multi Frame Gen inside games through the Nvidia app.
Still, not everything here is about software improvements. Without DLSS or Frame Gen involved, the RTX 5090 was, on average, 28 percent faster than the RTX 4090 at 4K. That’s still nowhere near as big of a jump as I saw between the last two RTX generations, but it’s better than the RTX 5090 is capable of at 1440p.
The RTX 5090 managed more than 60fps at 4K in every game I tested, with Black Myth: Wukong being the most demanding at 62fps on the RTX 5090 compared to 47fps on the RTX 4090. DLSS managed to bump this up to 95fps without Frame Gen involved.
I saw the biggest performance improvements in Cyberpunk 2077 at 4K, with the RTX 5090 delivering 43 percent higher frame rates over the RTX 4090 at ultra settings without DLSS or ray tracing involved. If you compare the RTX 5090 to AMD’s $999 Radeon RX 7900 XTX, then the performance gap is a massive 70 percent but at double the price.
Forza Motorsport also ran 35 percent better on the RTX 5090 than the RTX 4090 using temporal anti-aliasing (TAA), and I even managed to hit 238fps on average in Shadow of the Tomb Raider at 4K on the RTX 5090.
I also ran some AI and video workloads on the RTX 5090 just to get a taste for what it’s capable of over the RTX 4090. The RTX 5090 was around 12 percent faster than the RTX 4090 in PugetBench’s DaVinci Resolve test, and in Procyon’s AI XL (FP16) test, it was 40 percent faster. 
At 1440p resolution, the RTX doesn’t deliver as much of a performance benefit as at 4K. Black Myth: Wukong is one of the more demanding PC games right now, and it’s the only one where the RTX 5090 failed to deliver more than 100fps at 1440p without the help of FSR or DLSS. Enabling DLSS at 75 percent in Black Myth: Wukong bumped the RTX 5090 performance to nearly 120fps — closer to making use of a 144Hz monitor — but it’s only 17 percent more than the RTX 4090.
Horizon Zero Dawn averaged 200fps on the RTX 5090 with very high settings and just TAA enabled. That’s only 13 percent more than the RTX 4090 managed with the same settings. Returnal also ran 19 percent faster without DLSS on the RTX 5090 than the RTX 4090, and it’s a similar story in Call of Duty: Black Ops 6, where the RTX 5090 reached nearly 200fps without DLSS, about 15 percent more than the RTX 4090 delivered. 
The most disappointing result was Assassin’s Creed Mirage, where the RTX 5090 only managed 8 percent better performance than the RTX 4090. 
Cyberpunk 2077 and Forza Motorsport are rare examples of the RTX 5090 delivering more than a 30 percent performance gain over the RTX 4090 at 1440p. Without DLSS enabled, the RTX 5090’s lead over the 4090 at 1440p is about 18 percent on average — a lower jump overall than the last generation, which saw the RTX 4090 beating the RTX 3090 by over 40 percent in multiple titles.
DLSS 4’s Multi Frame Gen feature will obviously help out at 1440p, too, but out of my test suite of games, only Cyberpunk 2077 had support for it ahead of launch, so I focused my Multi Frame Gen testing at 4K.
At $1,999, I don’t think many people will be picking up an RTX 5090 just for 1440p, but if you’re considering it, then it might be worth seeing how the upcoming RTX 5080 performs at this resolution.
While DLSS Multi Frame Gen is showing promising early results, I’ve been far less impressed with the RTX 5090’s power draw. I managed to get it to peak at 578 watts during a 3D Mark Time Spy Extreme 4K benchmark. Even in Cyberpunk 2077 with full ray tracing and no DLSS, it peaked at 569 watts and hovered regularly above 500 watts. The same test on the RTX 4090 peaked at 427 watts. The extra power draw is really disappointing from this generation — I was hoping for a lot more efficiency from Nvidia’s latest chips.
Even without ray tracing in Cyberpunk 2077, the RTX 5090 averages 477 watts of power draw. Power consumption will vary by game, but in titles where you’re really pushing it, it’s going to want 500 watts or more. I was hoping for more efficiency here and for the RTX 5090 to draw less power than the previous generation in games like Cyberpunk 2077. In something like Valorant, it’s obviously going to draw less than 200 watts, but most non-pros aren’t spending $1,999 on a GPU just to play esports titles.
You’ll want to invest in at least a 1,000-watt power supply for the RTX 5090 or even beyond that to give you room for future upgrades. This extra power draw will also have an impact on your electricity bills, particularly if you’re in a country like the UK where utility prices have skyrocketed in recent years.
While the power draw is disappointing, at least I didn’t notice any immediate heat issues during my testing of the RTX 5090. It hit a maximum of 70 degrees Celsius (158 degrees Fahrenheit) in an open test bench, with the fans reaching 1,500rpm during the Cyberpunk 2077 testing. The RTX 4090 hit 64C in the same test. The RTX 5090 is a relatively quiet card, and it still has a zero-rpm mode like the previous RTX 4090, where the fans stop spinning when you’re idle or not touching the GPU much. It can get as high as 50C (122F) while idle in this zero-rpm mode, though, and then the fans kick in again to cool the card down to a more reasonable 30C (86F) idle temperature. I don’t have a small form factor case to test the RTX 5090, but I’d imagine you’ll have to think carefully about cooling in a more enclosed case.
I haven’t run into many issues with the RTX 5090, but I did notice a few games crashing randomly on the pre-release drivers. Valorant often fails to launch or just crashes before a game begins. The Finals refuses to load, with an error that appears to be anti-cheat related. I don’t recall similar issues with previous generations of Nvidia GPUs, but there could be other games that also crash or refuse to run with the RTX 5090 until developers and Nvidia fix the early issues.
The RTX 5090 is undoubtedly a 4K king, but Nvidia hasn’t delivered the pure performance gains here like it did with the RTX 4090. While DLSS 4 is a boost to both image quality and Frame Generation performance that can’t be ignored, I haven’t seen enough of Multi Frame Gen across a variety of games to know how well it works in every situation. The early look at Cyberpunk 2077 is promising for the RTX 50 series, as long as other developers can deliver similar performance gains without additional latency or issues along the way. 
Of course, you should never buy something hoping that software improves it over time, but Nvidia has proven with DLSS in the past that its GPUs certainly do get better with age. I’m willing to bet that DLSS 4 Multi Frame Gen will improve a lot more games running on the RTX 50 series like it did with these early examples. DLSS 4 and its new transformer model are also arriving on existing RTX cards to improve image quality, and Nvidia has hinted we might even see Multi Frame Gen eventually appear on older generations.
$1,999 is still a lot to pay for a GPU, even if it’s no more than the RTX 4090 ended up selling for in reality. Unfortunately, it doesn’t look like AMD will even bother competing with the RTX 5090, and it’s still not clear how its new AI-powered FSR 4 upscaling will compare to DLSS 4. Nvidia stands alone at the high end of the GPU market right now, and $1,999 is the price to unlock the best that PC gaming has to offer.
If this price or the power draw puts you off of the RTX 5090, Nvidia still has the $999 RTX 5080 on the way — and the $749 RTX 5070 Ti and $549 RTX 5070 in February. On a pure performance basis, the RTX 5090 isn’t a game-changer in the way the RTX 4090 was. But if enough developers adopt DLSS 4, then this sleek GPU might well usher us into an era where software improvements matter more than hardware alone.
A weekly newsletter by David Pierce designed to tell you everything you need to download, watch, read, listen to, and explore that fits in The Verge’s universe.
© 2025 Vox Media, LLC. All Rights Reserved"
"NVIDIA","RTX 5090","NVIDIA GeForce RTX 5090 Founders Edition Review & Benchmarks","https://gamersnexus.net/gpus/nvidia-geforce-rtx-5090-founders-edition-review-benchmarks-gaming-thermals-power","NVIDIA GeForce RTX 5090 Founders Edition Review & Benchmarks: Gaming, Thermals, & Power
This is a big review, we’ll respect your time: The short version of the review of the RTX 5090 Founders Edition is that the thermal solution is surprisingly good for GPU thermals, if a bit warm for memory thermals, the rasterization gaming uplift at 4K over the RTX 4090 (watch our review) is anywhere from 20% to 50% (depending on the game), ray tracing at 4K is commonly around 27% to 35% uplift over the 4090, and performance improvement at resolutions like 1440p and 1080p see lower improvements. An additional storyline to this review is that the 9800X3D (read our review) remains completely insane in the best ways, because it was able to keep up even at 1080p without major bottlenecking in some benchmarks.
Today, we’re looking at gaming performance, efficiency, thermals, acoustics, ray tracing, and power consumption.
Editor's note: This was originally published on January 23, 2025 as a video. This content has been adapted to written format for this article and is unchanged from the original publication.
First off, we have a huge amount of content related to this card coming up since the Founders Edition model is so unique. Make sure you check back regularly over the next few days to catch our benchmarks in common mini-ITX cases, the impact of the GPU on CPU and CPU cooler thermals, and some other tests. We also have a tear-down coming up.
Normally, these Founders Edition models don’t warrant a ton of discussion. This one does, but we’ll keep it short.
The RTX 5090 Founders Edition moves to a 2-slot design and uses a dual flow-through configuration, so they’ve sandwiched the PCB centrally and offset the PCIe slot to the side and down as a result. That also means that NVIDIA needs a separate PCB for the I/O that feeds monitors, connected via a flex cable to the main PCB. To get a 2-slot cooler capable of handling 575W or more, NVIDIA is using liquid metal with a triple-walled gasket to both contain the liquid metal and prevent exposure that could change its consistency and efficacy. 
The FE model does a lot of small things to improve performance, like exhausting the air out the top of the card and away from the GPU inlet. You’ll see that in our Schlieren imaging below.
We have a full video with Malcolm Gutenberg, Lead Thermal Engineer on the FE card, breaking down the changes.
The NVIDIA RTX 5090 is supposed to be $2,000 and will have official availability on January 30th, joined by the $1,000 MSRP RTX 5080 on the same date. NVIDIA also has the 5070 Ti and 5070 launching in presumably February for $750 and $550.
The NVIDIA RTX 4090 had an MSRP of $1,600, then was regularly priced around $2,000-$2,500 due to shortages and demand, and is basically out of stock except at terrible third-party seller prices now. The RX 7900 XTX (watch our review) is AMD’s closest competitor. Pricing is around $870 to $900. The company has also bowed out of the high-end race. The 9070 and 9070 XT, AMD’s next cards, should be coming around March or so.
Intel is currently only fighting at the low-end and mid-range.
Which makes all of this somewhat weird, because there are no head-to-head competitors right now. The closest comparison is the RTX 4090, then maybe the RX 7900 XTX from AMD’s side. 
The RTX 5090 has 32GB of GDDR7 memory, which is a big change, and runs on the Blackwell architecture, which follows Ada Lovelace. It’s introduced alongside multi-frame generation (MFG) and DLSS4, which we’ll talk about later. 
The 5090 is also a true PCIe Gen5 device, but that’ll be another separate piece soon to check back for the differences.
If you want to see our testing methodology, we’ve published the test bench and the list of games and their settings here, which will let you get quick answers to what we’re doing. It doesn’t have every answer, but we’re slowly adding to it with each review cycle. 
Here’s a thermal chart running the RTX 5090 under its auto VBIOS fan curve with a Port Royal RT stress test at 4K.
GPU temperature plots at about 72 degrees Celsius once it hits steady state for overall temperature when tested in a controlled room ambient of 21 to 22 degrees Celsius. This GPU temperature is genuinely impressive considering the size of the card, and that’s important to remember. At 2 slots versus that 4-slot monster we’ve seen for the last few years, this is an excellent result given the size. Our prototype testing already told us what NVIDIA can do with a fully committed, fatter design if you’re curious what that’d look like.
Memory temperature ran warm, unfortunately, at 89 to 90 degrees Celsius. This is higher than we’d like to see, especially considering it could be warmer in certain case configurations with a higher internal ambient temperature. This is technically still within the TjMax of these memory modules as far as we could find, so there isn’t an imminent threat to the card, but this would be an area for NVIDIA to improve; our primary concern is in hotbox cases or small form factor solutions, which we’re looking into as a follow up that you should check back regularly for. While these results are higher than what we’d like to see, in most high airflow ATX cases, it is okay.
Adding GPU fan speed to this chart, the fans both hit around 1570 RPM. We should get to acoustic testing for more on this.
We took the RTX 5090 to our hemi-anechoic sound chamber to evaluate it. A good GPU temperature is an achievement at this size, but that can almost always be done by just blasting the fan speeds and compromising on noise levels.
We ran the card at the default fan RPM that the card set itself to at steady state under our standard thermal workload. 
Here’s the frequency spectrum plot. In our acoustic chamber with a noise floor of about 14-14.5 dBA on the day of testing, the RTX 5090 was measured in our passive test bench at about 32.5 dBA total. That’s at a distance of 1 meter.
The RTX 5090 had some spikes during testing, including above our frequency cutoff, but overall has a very gradual curve for the plot. The limited presence of peaks and spikes in this plot help illustrate the relative uniformity of the whirring noise, which we subjectively think helps it blend into the background more. Noise is subjective, so although this plot objectively tells us that there’s a ping at 350 Hz and a bump in the plot around 515 Hz and again around 2,000 Hz, what matters is how it sounds.
This is a sound sample for you to judge on your own. Note that this is not identical to what we’re presenting as we have boosted it for purposes of being level with our video audio. Listen for the type of noise, not the volume.
In our interview with Malcolm Gutenberg, he explained that the 5090’s thermal solution was designed to reduce recirculation using angled covers, which direct airflow. 
In this image, we're looking at the GPU straight-on, with it perpendicular to the camera frustum. This is when the fans are off but the heat load has already started. You can see the density change as the hot air leaves the card passively.
As the fans turn on, we see a sudden flare-up and movement of air to the right through the flow-through area out the back and toward the CPU tower. What's super cool here is that we can see the air kick up and out to the right at a 20-30 degree angle or so. We also see a really high flow area of air exiting from the fins at the outer edges of the heatsink design. This hyper focuses the flow and reduces recirculation around the front area of the card, which just means the whole design is incredibly efficient at getting air away from the board and into case exhaust fans.
Here's the table shifted to see more exhaust. The flow-through area has super high speed exhaust, illustrating why flow-through is so much more effective than shoving air straight into a PCB wall.
Looking at the fans spinning down at the end of a load and returning to passive cooling. Everything drifts up and away.
This next angle shows the card on the left and centered. The most interesting thing we see is this straight line of air shot out from those fins at the outer edges of the fan diameter. This is what Gutenberg was talking about in our interview, where they're capitalizing on the area of most efficacy for the fan blade.
Finally, here's the card straight-on, where we can see the amount of air shot up and out. You'll want to choose cases with some spacing between the glass panel and the card to help get the warmed air away faster.
This next line plot is to ensure the GPU is functioning properly and meets the spec NVIDIA publicly claims. NVIDIA claims the 5090 runs at 2.01 GHz base and 2.41 GHz boost, with room for that to change based on the load. Assuming the software monitoring is correct for this new architecture, we measured about 2600-2700 MHz during the test, commonly 2600-2650 MHz.
The RTX 4090 ran at about 2745 MHz in the same benchmark back when it launched and we tested it. Frequency clearly isn’t everything though, and it’s common that higher end configurations bring frequency down in some capacity. It’s also true that architectural differences also make frequency indirectly comparable.
Ultimately: The card is exceeding the specification advertised by NVIDIA, so it’s running as expected, which is good. 
In Final Fantasy 14 at 4K, the RTX 5090 ran at a comically high 182 FPS AVG, with 1% lows that were nearly identical to the average framerate of the RTX 4090. That makes it 31% higher average framerate than the RTX 4090.
For a quick value discussion: The RTX 5090 is supposed to be $2,000, with the RTX 4090’s MSRP previously being $1,600. The 4090 is not commonly available anymore for a reasonable price, though. MSRP-to-MSRP, the 5090 is 25% more expensive and 31% higher framerate in this test. The memory capacity increase benefit isn’t seen in this game either, as that’d be more of an impact in professional applications like Premiere, 3D work, or ML workloads.
The RX 7900 XTX ran at 104 FPS AVG, the same as when we tested it in December (so there’s been no change), which gives the RTX 5090 a lead in this rasterized benchmark of 74%. The 0.1% lows are about the same between all of these devices at the top-end, which mostly comes down to pacing within the game. 
Prior NVIDIA flagships include the RTX 3090 Ti (watch our review) at 88 FPS AVG, meaning that the 5090 has doubled that performance. The frametime pacing was excellent on the 3090 Ti as it closely follows the average. The 3090 (watch our review) was more or less a flagship as well and at 77 FPS AVG. The 6950 XT (watch our review) was also once a flagship, closer to the RTX 3080 (watch our review) for performance.
The RTX 2080 Ti (watch our review) held a 54 FPS AVG, meaning 5090 owners would see an increase of 237% over the 2080 Ti. 
At 1440p, the 5090 again continues the comically high framerate by running at 317 FPS AVG. This has it about 17% ahead of the RTX 4090’s 272 FPS AVG. The advantage has been trimmed here, which could be because of an encroaching CPU bottleneck and/or because of architectural changes -- 1080p will help answer that below.
For games like this, you’d need a high-end CPU and ideally more intensive resolution to really get full use of the 5090.
Since we’re bottlenecked, we’ll move along but quickly stop to look at 1080p -- just for fun.
If you thought the previous framerate was funny, cast your sights upon 407 FPS AVG at 1080p. Sorry -- that’s 407.1 FPS AVG. 
Whew. Close one. As we all know, 407 FPS AVG is below the threshold of acceptability for the modern gamer. That 0.1 FPS is critical and is what finally pushes NVIDIA into playable territory for this game.
In serious news: The RTX 4090 at 376 FPS AVG means the 5090 is still about 8% ahead. This test really is just for fun though, but is a good reminder of the limitations of even a 9800X3D to boost the ceiling. 
Black Myth: Wukong is relatively new to our test suite and is tested using the built-in benchmark. We benchmarked it at 4K for this. Currently, we consider this test in our suite to be “experimental,” meaning our confidence in it is present, but lower than other tests as we evaluate its reliability. We have been moving toward removing experimental status from it with each review.
At 4K and where we’ve only tested a handful of cards due to the intensive load, the RTX 5090 ran at 86 FPS AVG with lows at 74 and 70. This has it 28% higher in average framerate than the 67 FPS AVG of the RTX 4090. So far, we’re seeing a few titles around this 30% number at 4K. Comically, the 1% lows, which for us is an average of the slowest 1% of frames, are higher than the average framerate for any other card on this chart; in fact, even the RTX 5090’s slowest 0.1% of frames are faster than the average framerate of the RTX 4090. That’s crazy.
The 7900 XTX’s 49 FPS AVG gives the 5090 a 74% lead, with the 3090 Ti giving it an 89% lead. Improvement over the 2080 Ti is enough to feel irrelevant as a percentage, as it takes it from totally unplayable to relatively fluid.
At 1440p, Black Myth has the RTX 5090 at 130 FPS AVG, 23% improved over the RTX 4090’s 106 FPS AVG. The lows also improve. The rapid rundown against other flagships is as follows:
The 5090 has a 51% higher average framerate than the 7900 XTX, 75% higher than the 3090 Ti FTW3 (RIP EVGA), 99% higher than the RTX 3090 Master, and 189% higher than the RTX 2080 Ti former flagship.
Black Myth Wukong is heavy enough that 1080p still has some meaningful spacing, even without ray tracing. The RTX 5090 ran at 160 FPS AVG, with good frametime pacing establishing 127 FPS and 116 FPS lows. The 160 FPS result has it 20% ahead of the RTX 4090, diminishing the earlier lead (which was 28% at 4K, 23% at 1440p, and now 20%). This isn’t just a CPU limit, as we also saw in Final Fantasy, but speaks to other advantages on the 5090 especially at higher resolutions. We think the memory bandwidth is likely a large part of that additional scaling.
The RX 7900 XTX ended up 113 FPS AVG, with the 3090 Ti former flagship at 94 FPS and the 2080 Ti at 62 FPS.
Starfield is up next. We haven’t run that many cards for this at 4K, but have a lot of 1440p data. We’ll start with the more limited 4K data set.
At 4K, the RTX 5090 held 108 FPS AVG with lows that were within expectations for this game. The RTX 4090 ran 92 FPS AVG, giving the 5090 a lead of just 17%, lower than we’ve seen in some other tests.
The lead over the 7900 XTX’s 77 FPS AVG is 40%, with the lead over the 58.3 result for the 3090 Ti at 85%. The 3080 (watch our review) was down at 48 FPS AVG, with the 2080 Ti at 33 FPS AVG. AMD’s 7900 XTX and 7900 XT (read our revisit) are its highest-end cards available for the company right now, but the 6950 XT was a good deal in the back half of its life.
At 1440p, the RTX 5090 ran at 147 FPS AVG against the 132 FPS AVG of the RTX 4090. This is down to a 12% uplift. The 7900 XTX ran at 112 FPS AVG, a big improvement from its 4K result as you would expect, with the 4080 FE (watch our review) at 108 FPS AVG. The 4080 Super (read our review) would be around 1-3% better here if we had retested it.
There aren’t many reasons you’d play this game at 1080p with an RTX 5090, but just for sake of data: The 5090 ran at 165 FPS AVG here, with the 4090 at 155 FPS AVG. Although technically better for the 5090, we’re effectively at the CPU limit here.
Dragon’s Dogma 2 is up next. This is another new one that we added in 2024 and has been heavy on GPUs and CPUs alike depending on the test area.
In this limited suite of cards, we have the RTX 5090 at 133 FPS AVG, leading the RTX 4090 by 35%. This is one of the largest gains we saw in our test suite. The lows and 0.1% lows also scaled up, showing that frametime pacing wasn’t at the expense of higher FPS.
The RX 7900 XTX ran at 77 FPS AVG, with the 4080 FE at 72 FPS. Again, the 4080 Super would be about 1-3% above that.
The 2080 Ti from 2018 ran at 36 FPS AVG, and that’s without RT. The improvement to the 5090 is 267%. Climbing the flagships, the 3090 Ti’s 64 FPS AVG ends up giving the 5090 a 108% lead.
At 1440p, the RTX 5090 FE climbs in framerate to 189 FPS AVG, with extremely well-paced frametime consistency shown in the high 0.1% and 1% low values.
The 5090 ends up leading the 4090’s 156 FPS AVG by 21% and the 7900 XTX by 50%. The lead against these cards has fallen from the 4K results. 
Although we’re in territory where it’s not meaningful for the experience, it’d help us to understand the behavior by looking at 1080p. The framerate still increases, so we weren’t totally bound by the CPU. The 5090 hits 214 FPS AVG, leading the 4090 by 13%. What’s interesting is that the 4090 is now at the same framerate that the 5090 had when the 5090 was at 1440p.
Cyberpunk is up now. We’re testing the Phantom Liberty expansion in-game in the expansion area.
The RTX 5090 ran at 95 FPS AVG, with lows at an impressive 81 FPS 1% and 77 FPS 0.1%. These lows are excellent numbers and similar to what we saw in Black Myth: Wukong, where the 5090’s lows are outperforming the 4090’s average. The improvement in average FPS was large at 50%, moving from 64 FPS AVG on the RTX 4090. This is the biggest gain we’ve come across so far. Cyberpunk is very particular though and sensitive to areas of the game. Checking with Wendell, his Level1 Techs team saw similarly huge uplift.
The RTX 4090 had a large 32% lead over the RTX 4080 already. As for the older cards, the 5090 and 3090 Ti are in entirely different classes. The 2080 Ti is down at 27 FPS AVG and struggling to run, although to its credit, its frametime pacing in relation to the average is excellent -- it’s just that the framerate is low.
At 1440p, the RTX 5090 ran at 181 FPS AVG, with lows at 126 and 108. The RTX 4090 held a 137 FPS AVG, with the advantage of the 5090 being reduced to a still respectable but lower 33%. The 7900 XTX ran at 120 FPS AVG here, which has remained a good result considering the price of the 7900 XTX as compared to its neighbors. That story is totally different with RT, though.
The RTX 3090 Ti ran at 91 FPS AVG, with the 2080 Ti at 57 FPS AVG.
We were fully CPU bound at 1080p, so we’ll skip it.
Dying Light 2 at 4K is another heavy load for these GPUs. The RTX 5090 shows a familiar scenario of the 1% lows and 0.1% lows, which represent the slowest frames in our test passes, outperforming the average framerate of the RTX 4090. NVIDIA has managed to move the needle for at least the flagships, which we think is partly thanks to cache and memory configuration changes.
The 5090 leads the 4090 by 38%, another impressive jaunt not distant from what we saw with Phantom Liberty. The 7900 XTX did OK in this test as compared to the 4080. The 5090 runs 74% higher average framerate than the 7900 XTX and also costs about 127% more, depending on what price the XTX is. For professional users though, the memory benefit isn’t accounted for in almost any gaming scenarios we test and would be in other applications.
At 1440p, the RTX 5090 holds a 216 FPS AVG against the 4090’s 173. This has the 5090 25% ahead of the RTX 4090, down from its lead of 38% at 4K. We won’t burn chart time on it, but 1080p is only about 15 FPS higher, so part of this reduction in scaling is because we’re starting to approach the CPU limit.
Resident Evil 4 is up next, first rasterized and at 4K.
The RTX 5090 landed at 207 FPS AVG here, with lows running higher as a result of consistent frame pacing. The end result is a lead over the 151 FPS AVG of the 4090 by 37%, a lead over the 7900 XTX of 64%, and lead over the 4080 of 101%. Against prior flagships, the 3090 Ti landed at 89 FPS AVG, giving the 5090 an uplift of 133%.
At 1440p, the RTX 5090 continued scaling and hit almost 350 FPS AVG, with lows that are at ridiculous levels with 281 FPS 1%. This puts the 5090’s average framerate 25% ahead of the 4090’s average framerate, so we’re seeing a reduction from the 37% at 4K, consistent with what we’ve seen elsewhere.
The 7900 XTX held on at 232 FPS AVG here, followed by cards like the 3090 Ti at 162 FPS and 2080 Ti at 92 FPS AVG.
At 1080p, we see there was still scaling all the way up to almost 400 FPS AVG, which is crazy. This has reset our expectations of where the CPU ceiling is. If anything, this is showing just how good the 9800X3D is for keeping up so well.
The gap between the 5090 and 4090 is around 9% here, so we are actually hitting external limits.
And now we’re moving to ray tracing benchmarking. This contains games like Black Myth and Cyberpunk, which tend to favor NVIDIA, and games like Resident Evil, Dying Light, and Dragon’s Dogma, which give some more variety.
Black Myth is first. This is an experimental chart, so once again, our disclosure is that experimental charts have a greater risk of unexpected results as we are still researching its behaviors. This particular title is considered experimental in our test suite because its performance leans so heavily in one direction that we want to slowly accumulate results to explore it further.
The 5090 ran at 88 FPS AVG at 4K, outperforming the RTX 4090’s 65 FPS AVG result by 36%. That’s a big jump. This is with upscaling, so it’s not like-for-like with the 4K raster results.
AMD’s 7900 XTX ran at 20 FPS in this title, which is why we say it’s NVIDIA-favored. The 3090 Ti ran at 34 FPS AVG here.
Skipping 1440p and going to 1080p with FSR to get more cards on the chart, here’s where we land. The 5090 is at 158 FPS AVG here, leading the 4090’s 120 FPS AVG result by 31%. Against the 3090 Ti, the 5090 leads by 103%, and against the 2080 Ti’s 49 FPS AVG, it’s about a tripling.
The 4070 (watch our review) outperforms the 3090 Ti in this test when using FSR, with the entire top half of the cards outperforming the 7900 XTX. This test, again, is heavily favored for NVIDIA with the heavy ray tracing use.
Dragon’s Dogma 2 is up next. Again, we haven’t done a ton of 4K Ray Tracing tests here because it’s such a heavy workload normally, but the RTX 5090 ran at 113 FPS AVG with lows at 97 FPS and 94 FPS. The 4090 landed at around 85 FPS AVG, giving the 5090 an uplift of 33%. The RX 7900 XTX does better in this game compared to Black Myth, instead outperforming the RTX 4080 and 3090 Ti, the latter of which is at 55 FPS AVG.
At 1440p, the 5090 jumped to 165 FPS AVG and the 4090 held 136 FPS AVG, still keeping about a 30 FPS gap between them, or an improvement generationally of 22%. The uplift has fallen as compared to 4K, keeping with prior trends. The 7900 XTX does similarly here to last time, landing just ahead of the RTX 4080 (watch our review).
At 1080p, the 5090 continues to climb to 194 FPS AVG, reducing the generational uplift to 15% over the 4090. Let’s move on to something more interesting.
Here’s Dying Light 2 ray-traced. Again, we haven’t historically run 4K here because only the 4090 and 4080 could be argued as capable. It looks like this next generation of hardware -- and hopefully that also includes AMD’s next card -- is changing that. The RTX 5090 ran at 109 FPS AVG, leading the 80 FPS result of the 4090 by 37%. The 7900 XTX is led by 137%. AMD has publicly claimed that its next generation will significantly improve upon this, so we’ll see where they land probably closer to March.
At 1440p, the RTX 5090 ran at 176 FPS AVG and held lows of 152 and 126. The 176 result has it about 40 FPS, or 29%, ahead of the RTX 4090. The 4080 hit 104 FPS AVG with the 3090 Ti at 88 FPS. Our 2080 Ti was approaching a decent framerate, but still falling short at 46 FPS AVG.
At 1080p, the 5090 held 224 FPS AVG, mostly establishing that we weren’t bound previously by the CPU. So when it was at 4K, the scaling was a 37% generational improvement, then 29% at 1440p, and now is 24.5% at 1080p. The reduction from 1440p to 1080p isn’t as big as we might expect from other tests, probably because there remains enough GPU load to where the CPU isn’t heavily taxed.
Resident Evil 4 with Ray Tracing is up now, tested at 4K first. The 5090 ran at 210 FPS AVG using FSR as defined in the chart title. The 160 FPS RTX 4090 result establishes a 31% generational improvement favoring the 5090.
The lead over the 7900 XTX is 56%, with the improvement on the 3090 Ti at 113%.
We’ll keep this short: At 1440p, the RTX 5090’s lead falls to 23% over the 4090. This trend is consistent.
Cyberpunk with RT Ultra at 4K is heavy even for the RTX 5090 when not using some form of upscaling, which we toggle off in testing specifically because of how unreliable Cyberpunk’s sticky settings are. The 53 FPS AVG puts the 5090 35% ahead of the 4090’s 39 FPS AVG result, remaining consistent with prior tests. The poor, old 2080 Ti nearly burst into flames trying to run this, holding an 8.8 FPS AVG as it crawled across the finish line.
4K with RT Medium is interesting. Dropping from Ultra to Medium predictably increased performance, but grew the gap between the cards with a 59 FPS AVG and 40 FPS AVG result.
Now we’re getting into efficiency benchmarking and idle power consumption. For this, although we tested a lot of games, we’re going to simplify the charts and just look at a couple of game tests plus idle. These convey the whole story pretty well.
Testing is done by measuring the GPU power consumption at the PCIe cables and the PCIe slot with an interposer. Although we initially had trouble getting the card to work on the riser due to PCIe generation differences, in the final hours before going live, we found a solution to measure through the riser. This testing eliminates the remainder of system power consumption, so we’re isolating for just the GPU.
Testing idle power consumption, the RTX 5090 FE landed at 46W on the desktop with our benchmarking approach. The RTX 5090 FE measured lower in idle power draw than the Arc A580 (read our review) and about the same as the A750 (read our revisit). Even just sitting there, it’s drawing a good amount of power. Our testing uses Windows High Performance power plan for benchmarking performance, so switching to Balanced may help reduce this; however, we use that plan for all tests, so these are like-for-like comparable. We measured the RTX 4090 at 28-29W. The 5090 has relatively high idle power consumption with our test approach and this is an area where there’s clearly some room for improvement if only judging by the 4090, although the power consumption of the TDP is higher on this card.
Final Fantasy 14 at 4K is low on results since we just started using this for efficiency for this launch. The RTX 4090 was the most efficient here, at 391.7 W to produce 138-139 FPS AVG. That puts it at 0.35 FPS/W. The RTX 5090 FE was efficient as compared to the other cards we’ve tested here, but technically worse off than the 4090. Realistically, they’re about the same. Despite framerate improving by 31%, the power consumption also increased by 37%. The end result is reduced or equal efficiency versus the last generation. This might be why NVIDIA is pushing the narrative so hard that MFG improves efficiency, except that’s like saying “why compare apples to apples when you can compare apples to oranges?”
In the very least, against the 3090 Ti in a like-for-like comparison, we can see clear and massive iterative improvements.
We’re showing 1440p to get a wider selection of cards, though the lighter load won’t look better for the 5090. The RTX 5090 ended up around 520W average for this work, landing it at 0.61 FPS/W. Efficiency is down comparatively overall since we saw the performance advantage also go down when at 1440p. The card should show the best gains in heavy 4K/RT workloads like F1.
On a technicality, the RTX 5090 is the most efficient in this test. It pulled 569W on average during testing and had spikes up to 580-590W, and because of the framerate advantage over the RTX 4090 with its 428W draw, it ends up at 0.21 FPS/W instead of 0.20. This isn’t particularly exciting and we have to highlight that NVIDIA’s claims of efficiency improvements largely centered around artificially generating frames, which isn’t like-for-like because the frame itself may not be the same or comparable.
First of all, we need to start with NVIDIA’s complete bulls*** marketing. Unfortunately, NVIDIA just couldn’t help itself except to unfairly misrepresent its RTX 5090’s performance in the following slide on its site.
This image shows the RTX 5090 as being 2x faster than the RTX 4090 regularly, but as you all know from the review you just read, that’s not true. This image doesn’t say “different DLSS versions where we needlessly compare apples to oranges even though we have nothing to be shy of if we tested properly, it says “Performance.” And the accompanying caption isn’t even part of the image. Just saying “Performance” while making big 2x bars makes the 5090 look 2x better than the 4090. NVIDIA technically lists the DLSS version in the bottom, but most people don’t know what that means. Most people don’t know that writing “DLSS 4” under BOTH bars of the RTX 4090 and RTX 5090 isn’t actually the same setting. DLSS 4 does not do the same thing on both of these devices. NVIDIA’s own line of gray text that blends into the background at nearly the same color states the test configuration. This states that Frame Gen was used on the 40 series and 4X multi-frame gen was used on the 50 series, which isn’t like-for-like. NVIDIA is generating more artificial frames per real frame on the 5090 than the 4090, but they just list “DLSS 4” under the bars instead of making it clear.
NVIDIA didn’t have to do any of this, but between this insane reach of marketing and the claim CEO Jensen Huang made about an RTX 5070 performing the same as an RTX 4090, it comes across like NVIDIA feels like it isn’t good enough on its own. It has to put a bunch of makeup on the charts to be good enough.
Anyway, enough of the marketing bulls***. The recap is this:
 Copyright © 2023 GamersNexus, LLC. All rights reserved.GamersNexus.net is Owned, Operated, & Maintained by GamersNexus, LLC. 
Copyright © 2023 GamersNexus, LLC. All rights reserved.GamersNexus.net is Owned, Operated, & Maintained by GamersNexus, LLC."
"NVIDIA","RTX 5090","NVIDIA GeForce RTX 5090 Founders Edition Review","https://www.techpowerup.com/review/nvidia-geforce-rtx-5090-founders-edition/",""
"NVIDIA","RTX 5090","Review: Nvidia GeForce RTX 5090 Founders Edition","https://www.wired.com/review/nvidia-rtx-5090-fe-review","All products featured on WIRED are independently selected by our editors. However, when you buy something through our retail links, we may earn an affiliate commission.
How much did you spend on your last graphics card for your PC? Four hundred dollars? A thousand? What would inspire you to spend double that on a GPU? Nvidia thinks you may want to splurge on its new RTX 5090, which showcases the absolute top end of this generation of graphics cards for well-heeled enthusiasts.
The Founders Edition, which comes straight from the brand itself, rather than resellers like MSI and others, is meant to shatter what prior Nvidia cards are capable of, and I’m lucky enough to have one in my gaming PC for testing.
This limited-edition graphics card doesn’t just have to justify its own cost, but the cost of a monitor and power supply that can keep up with it. While I’m excited about what this technology can offer, I am especially happy to share that more affordable, strikingly similar versions of this card will be available to everyone soon.
When I reviewed the Hyte Y60 (8/10, WIRED Recommends) PC case, I mentioned that there wasn’t much room between the glass and the face of the card, and larger cards probably wouldn’t fit. I’m happy to report that the Founders Edition card fits just fine, being only slightly wider than my previous Sapphire 7900 XT ($660), and is actually a bit shorter.
The more compact size is largely thanks to a reconfigured cooler, which places the card’s main PCBs in the center. This allows the fans to blow straight through a set of fins and a vapor chamber, significantly reducing the overall footprint. With cards swelling to three and four slots over recent years, I appreciate the reset back to a two-card standard, which folks should find fits in some surprisingly compact cases.
All products featured on WIRED are independently selected by our editors. However, we may receive compensation from retailers and/or from purchases of products through these links.
For most people then, power will be the biggest obstacle. The Founders Edition is rated at 575 watts, and Nvidia recommends a 1,000-watt power supply, but some partner cards may suggest 1,200 watts. You’ll also spot a new, more compact power connector on the side, called 12V-2x6, which replaced the short-lived and ill-fated 12VHPWR connector from the RTX 40 Series cards. I found this page from power-supply manufacturer Corsair particularly helpful in explaining the change, as well as a chart showing different possible setups.
However you arrange it, you’ll need to run a full 600 watts to the card, which means anywhere from two to four dedicated cables running from your PSU. The Founders Edition includes an adapter with a short braided cable, and I imagine partner cards will follow suit, especially given the new standard and high price point of the RTX 5090.
The star of the show here is Nvidia’s updated DLSS 4 (Deep Learning Super Sampling) technology, a suite of features that leverage machine-learning tools to improve gaming performance. This includes updates to Reflex, which helps lower latency, and Super Resolution, which can help upscale games rendered at lower resolutions, among others.
Most notably, DLSS 4 includes a major update to Nvidia’s Frame Generation tech. For each frame the card fully renders from the game, it uses AI to generate additional frames to fill in the gaps. Where the previous version of DLSS 3 could render one frame, the updated version 4, running on 50 Series cards, can generate up to three.
How does it affect performance? To see that in action, I’ll zoom in on two games. The first is Cyberpunk 2077, one of the most demanding games that supports the new DLSS Frame Generation feature before launch. I cranked up the rest of the settings to max in 4K, then raised the number of AI-generated frames to measure the difference using the in-game benchmark.
All products featured on WIRED are independently selected by our editors. However, we may receive compensation from retailers and/or from purchases of products through these links.
Armed with a knowledge of how the underlying tech works, it might not surprise you to see a fairly linear leap in frame rate with each additional generated frame. Nvidia has taken advantage of a key performance metric, frames shown to the users, and side-stepped the issue of rendering those frames entirely to show an exponential performance increase. It’s not a lie or a trick: You really are seeing those frames, but the jump from last generation is more typical than these numbers would have you believe.
As you might expect from AI-generated frames, they aren’t perfect—although they’re a massive improvement over the junk memes you may have seen on social media. The system doesn’t have any trouble maintaining the right number of fingers, and it doesn’t hallucinate random heroes into existence in Marvel Rivals.
If you really hunt for them, you can occasionally spot artifacts and oddities, particularly in places where the system has to navigate unique depth challenges. You can see the difference between frame generation being on and off in the screenshot below. With frame generation turned off, the hat in the background warps as it intersects with the fence pattern, and the neon sign in the background loses some of its detail and shape where the lines cross over it.
Certainly things look pretty bad here, but this is a single zoomed-in frame of a full-screen game running at over 250 frames per second. In practice, I only ever noticed the anomalies if I was really looking for them, but keep in mind that I ran this benchmark dozens, if not hundreds of times for this review, so I spent quite a bit of time staring at the screen. That’s helped by the increased smoothness at higher frame rates, which means any oddities that do show up only appear briefly.
All products featured on WIRED are independently selected by our editors. However, we may receive compensation from retailers and/or from purchases of products through these links.
I also want to mention Star Wars Outlaws, because there are a ton of shadows, uniquely colored lights, and blaster fire that benefit massively from turning up the ray tracing settings. Unfortunately, even the powerful RTX 5090 has trouble maintaining playable frame rates with everything on High or Ultra.
Even single-frame generation is enough to take the game right to around 60 fps, and generating all three frames results in an average of 120 fps, with every setting all the way up. The experience feels smooth while you’re playing it, so it’s easy to forget you aren’t looking at real frames generated by the game, at least when the action starts up.
I would only lean on frame generation as much as needed to over the refresh rate of your monitor. If you can run without it, your game will look a bit cleaner overall, particularly when it comes to little details in odd lighting situations.
This is all very new tech. At publish, only a few games are able to leverage multiframe generation, but Nvidia says 75 games and apps will add support by January 30. I expect most games with enough graphical fidelity to take advantage of DLSS 4 will support it at launch going forward, or at least some of its features.
Even without Nvidia’s latest machine-learning tech, the RTX 5090 still puts out a tremendous amount of gaming power. I tested it on a 4K display, as well as a 3,440 x 1,440 ultrawide, on our test rig with an AMD Ryzen 9 7950X and 32GB of RAM. In general, I turned up the settings for each game as high as I could, and either used the in-game benchmark or averaged out a 5- to 10-minute typical play session.
All products featured on WIRED are independently selected by our editors. However, we may receive compensation from retailers and/or from purchases of products through these links.
Whether or not you call this a victory will depend on your refresh rate. People with 4K monitors that have a 60-Hz refresh will probably be happy with most of this, but I can imagine some folks with 120-Hz displays will have to tweak settings to consistently stay above 100 fps. It definitely feels like true 4K gaming at the highest end still isn’t quite achievable with current hardware, at least without the help of tech like frame generation that side-steps the issue.
My main gaming monitor is a 1440p ultrawide with a 120-Hz refresh rate, and I know a lot of my friends have gone the same direction. It’s easier to achieve consistently high frame rates, but it’s also a cinematic experience, on a single display, that easily handles two windows for nongaming work.
It's safe to expect 90- to 120-fps performance across most games at this resolution, which is great news for gamers looking to max out their existing monitor. Single player, cinematic-heavy games like Cyberpunk 2022 and Star Wars Outlaws are still on the cutting edge of graphical fidelity, so I’m not necessarily frustrated that they have some room to grow, especially when they look so good already. Online games and shooters like Marvel Rivals run smoothly without much help, and it’s arguably more important to have consistent frame rates in those games.
Anyone considering the RTX 5090, the Founders Edition or otherwise, should truly consider their budget first. The FE version of the card will set you back $2,000 if you buy it directly from Nvidia, and the partner cards with overclocking and liquid cooling will likely be even more expensive. You’ll also need to spend around $1,000 for a monitor that truly takes advantage of your newfound graphical power, and potentially a new 1,000-watt or 1,200-watt PSU. That means you could be looking at a $3,500 bill before you have any other parts, and regardless of performance, I have trouble imagining starting any build like that.
The RTX 5090 and RTX 5080 will hit the market at the end of January, with more budget-friendly cards arriving soon after. Without having spent time with the other RTX 50 Series cards, I can’t speak to their relative performance, but I do know their price tags look a lot more appealing. I expect these cards will support multiframe generation out of the box, and that’s awesome news if you just want to sit down and see smooth gameplay.
Previous Founders Edition releases didn't stay in stock for long, so you might have to wake up early on the 30th to snag one of these if you want one. The whole situation makes the RTX 5090 feel less like the top end of the 50 Series, and more like a showpiece.
It's the GPU I'd configure while daydreaming of a new rig, not the first part I’d select in a realistic build on PCPartPicker. If the price tag doesn’t give you a moment of pause, then by all means, enjoy your new GPU. For everyone else, I’d wait and see what the rest of the new GPUs look like before you leap.
All products featured on WIRED are independently selected by our editors. However, we may receive compensation from retailers and/or from purchases of products through these links.
© 2025 Condé Nast. All rights reserved. WIRED may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Condé Nast. Ad Choices"
"NVIDIA","RTX 5090","The RTX 5090 is the best and most unnecessary GPU you can buy","https://www.polygon.com/gaming/545482/rtx-5090-review-nvidia-graphics-card-gpu","Games run great on it, but the $2,000 price hurts
The most powerful graphics card you can buy right now is actually pretty worthless if you don’t have the right PC gaming setup. Nvidia’s GeForce RTX 5090 is at its best when paired with a 4K monitor that can reach high refresh rates, like 144 Hz or 240 Hz. Anything less than that diminishes the value of an already ridiculously expensive card.
A graphics card as beefy as the RTX 5090 needs to chew on the toughest games to justify its $1,999 price tag. I’m talking about Alan Wake 2 with everything set to max, or Cyberpunk 2077 with all the ray tracing on. It’s a card for people who don’t mind paying to not have to fiddle with graphics settings. Chances are, you already know if that’s you or not.
The RTX 5090 didn’t transform my experience with games — it mostly saved me time. I’ve been using a card sent to me for review by Nvidia, and over the last month I haven’t had to think about frame rate. Even a game as unstable as Monster Hunter Wilds ran almost flawlessly on my PC — which is equipped with an Intel Core i9-12900K 5.2 GHz 16-core processor, 32 GB of RAM, and a 1000 W power supply. I’m also running everything on an 4K 32-inch MSI OLED monitor. All the reports of stuttering and abysmal frame rates didn’t affect me. And that’s really the whole reason you’d buy the most expensive graphics card in the world right now: to boot up any game and know it’ll have enough overhead to run just fine.
That doesn’t mean I’d recommend it, though. Even if it were possible to find one in stock, the RTX 5090 isn’t worth it for almost anyone in need of an upgrade or looking to build a gaming PC. There are far more reasonable options out there, particularly in Nvidia’s previous generations of cards.
That’s largely because you don’t actually need an RTX 5090 to benefit from several of its best features. The 40-series cards can use both DLSS 4 and Frame Generation, which leaves the RTX 5090’s primary advantages in Multi Frame Generation and raw horsepower. In other words: The RTX 5090 is mostly a peek at what future hardware iterations could be capable of at more reasonable prices. It’s proof that Nvidia’s AI-powered upscaling technologies, DLSS and Frame Generation, can get even better than they already were. Both of them really do feel like magic when they work, and they’re only becoming more important as it gets harder and harder to afford the latest hardware.
For example: I cranked up every single setting in Cyberpunk 2077 as high as it would go. My frame rate chugged as the RTX 5090 churned out scenes with real-time reflections in the puddles and cars whizzing by. But unless I wanted to spend hours people-watching in Night City, the game was unplayably slow. With DLSS 4 and Multi Frame Generation on, however, it went from a choppy mess in motion to a slightly sluggish but playable game that was hitting upward of 120 frames per second. I could run through the busy market full of NPCs and neon signage without a hitch. If I was willing to put up with a slight delay between my button presses and the actions on screen — similar to how it feels to stream a game over the cloud — Cyberpunk 2077 was playable at a high frame rate on settings that were mostly a novelty in the past.
It still wasn’t smooth enough to justify relatively minor improvements in the lighting over less absurd graphics settings. The more you rely on Multi Frame Generation to insert AI-created frames, the more input lag is introduced, meaning your button presses will have a noticeable delay. In situations where you need to drastically increase the frame rate to make the game visually playable, you’ll have to sacrifice responsiveness. And it hurts a fast-paced first-person shooter like Cyberpunk 2077 the most. It was certainly impressive seeing the game run well above 60 frames per second on a high-refresh-rate monitor. Normally, that would require a lot of graphical sacrifices to pull off. But just because it was playable doesn’t mean it was tolerable. It’s a feature much better suited for when you’re not aiming for super-high frame rates, which is why I see no reason to invest in an RTX 5090 and an expensive monitor for it. And remember: Nvidia’s other 50-series cards can use Multi Frame Generation too, so the high frame rate life doesn’t require the top-of-the-line GPU.
DLSS 4 is a similar story. This version of the upscaling tech uses a new AI model that basically makes details on objects look a little less blurry, especially in motion. On my 4K monitor, I had to stop and squint in Alan Wake 2 and Marvel Rivals to really notice a difference. Things like raindrops sliding down a window or blades of grass had slightly sharper details in DLSS 4 over DLSS 3, but my frame rates were about the same. These are the kinds of improvements worth waiting 10 minutes to download new drivers that enable it on your 40-series card, rather than spending thousands of dollars on an RTX 5090.
The only tiny caveat to that is Ray Reconstruction, a high-end feature for games with the fanciest ray tracing, which is also known as path tracing. Path tracing makes even the RTX 5090 sweat from all the calculations it has to do to accurately track how light bounces around a scene. Lesser cards can barely handle it. It’s the kind of thing you turn on, admire for 30 seconds, and turn off again to reclaim the chunk of frames it eats up. Ray Reconstruction uses AI-powered denoisers to make the job significantly easier, which causes it to take a much smaller bite out of your frame rate. While you can definitely see and feel the difference in games like Cyberpunk 2077 — headlights cast sharper cones of light on the road as you drive — it feels like the first step in making path tracing less of a luxury feature.
DLSS 4 and Multi Frame Generation overshadow a lot of the raw power of the RTX 5090. Everything from Diablo 4 to Assassin’s Creed Shadows ran exceptionally well on it, easily hitting over 60 fps. That was the same case in Alan Wake 2 and Marvel Rivals. Both games gained about 20 to 30 fps over my RTX 4090, and were able to sit comfortably at 60 fps even during intense action. The slight increases weren’t particularly noticeable without staring at the fps counter, though. In practice, all it really meant was that there were fewer times where the frame rate would dip low enough for me to notice it. That kind of overhead is nice to have, especially when you’re trying to hit solid fps at 4K, but it’s not enough to justify hunting an RTX 5090 down and shelling out the money for it, especially when anyone with an RTX 4090 or an RTX 5080 will be able to take advantage of DLSS and regular Frame Generation already.
The RTX 5090 is the most powerful GPU right now, but it’s also the most niche GPU right now. It’s not a substantial upgrade from the RTX 4090, nor is it the most cost-effective upgrade for those who have an RTX 30-series or older card. It’s a high-end GPU for people who don’t need a review to tell them whether it’s worth it or not. For everyone else, it’s really just a concept for where graphics tech is headed. Eventually, a much cheaper card with these features will come around. You’re better off acting like it doesn’t exist while you take advantage of all the software improvements Nvidia made along with it.
The best of Polygon in your inbox, every Friday."
"NVIDIA","RTX 5090","MSI RTX 5090 Suprim SOC review","https://www.pcgamer.com/hardware/graphics-cards/msi-rtx-5090-suprim-soc-review/","When you purchase through links on our site, we may earn an affiliate commission. Here’s how it works.

Now that MSI and Nvidia have figured out how to stop this monstrous GPU from blackscreening every time you play a game it's the most powerful graphics card I've ever used. But not by a margin big enough to explain away that ~$1,000 price premium above MSRP.

PC Gamer's got your back




Our experienced team dedicates many hours to every review, to really get to the heart of what matters most to you. Find out more about how we evaluate games and hardware.

This thing is a monster. Obviously in the same way the RTX 5090 Founders Edition is a monster in terms of raw performance; we are still talking about the most powerful consumer GPU available to humankind after all. But more than that, the MSI RTX 5090 Suprim is probably the biggest graphics card I've ever had the misfortune to balance delicately on the ickle PCB we use to measure GPU power draw.
I am slightly afraid for the continued happiness of our test rig with this frankly enormous card placed delicately atop it. In fairness, the XFX RX 9070 XT Merc is almost the same size, so it's not like MSI or Nvidia are alone out there with hoofing great graphics cards.
Thankfully the package does come with a wee stand in order to take some of the strain off a standard PCIe bracket when installed horizontally, but in all honesty it's still going to have its work cut out for it given the heft of this thing. It's a large card, I guess is what I'm saying.
With the decreased size of the comparatively diminutive RTX 5090 Founders Edition, it's maybe something of a surprise to see MSI's RTX 5090 Suprim coming in with such a big shroud. But all of the third-party cards in this generation seem to be trending towards the chonky compared with the FE cards.
GPU: Nvidia GB202CUDA cores: 21760Boost clock speed: 2,580 MHz (Extreme performance)Memory: 32 GB GDDR7Memory bus: 512-bitTGP: 600 WPrice: If you have to ask... it's theoretically $2,750, but really more like $3,000 at best, and likely more if you can find one today
Nvidia does say its partners are free to use the PCB from the Founders Edition if they wish, but I would expect the standard GPU mainboard is far cheaper. I also expect the partners needed to have their overall cooling and shroud designs in place well before they knew exactly what targets they had in terms of keeping things chill for Blackwell.
The upshot, however, is that the Suprim is *ahem* supremely cool because of that monstrous heatsink. It can also deliver much higher clock speeds and therefore a chunk more performance than the Nvidia reference card. Given that this MSI version of the RTX 5090 comes in well above the $1,999 MSRP Nvidia originally set for the card, you would hope for high frame rates at least.
Its near $2,750 MSI MSRP is not a huge surprise given what's happened with the rest of the cards in this generation so far, but still, with the MSI RTX 5090 Suprim being another $750 on top of the standard MSRP, it's a little tough to swallow. More so given that even if you could find one in stock today, it would likely be priced far higher even than that.
There's a reason this review is so much later than the January 31 launch date... it was hella b0rked.
This pricing would have been even harder to cope with if you'd spent that much around launch. Should you have paid your near-$3,000 sticker price for the card, got it home and installed it in your rig without snapping off the PCIe slot, there's a good chance you would have been left seriously unimpressed when you booted up your PC.
I know I was when I first started benchmarking this card. There's a reason this review is so much later than the January 31 launch date for the third-party RTX 5090 cards, and it's not because I've since been inundated with new GPUs—though I 100% have been—it's because it was hella b0rked.
In some games I was getting the expected great performance, over and above the Founders Edition, but in others I'd either get crashed back to desktop, suffer a blackscreen until the GPU driver restarted, or get a total blackscreen freeze, which would need a total system restart.
Not a good look for a brand new $2,750+ graphics card.
A month, some blank looks, and a lot of shrugged shoulders later, MSI released a new VBIOS, Nvidia released a new driver, and all seems to be as it should. I now have the RTX 5090 Suprim as it ought to have been at launch: a highly overclocked, super-cool, absolutely vast GPU that is able to devour high-end 4K games for breakfast and come back for seconds.
At its heart is the GB202 graphics processor from Nvidia; the top RTX Blackwell GPU promising a future of Neural Rendering and a today filled with AI-powered Multi Frame Generation shamelessly bumping up frame rates and smoothing out a ton of the latest modern games. If you want the full beans I've gone into detail in the main RTX 5090 review on the new Nvidia RTX Blackwell architecture.
This Suprim SOC is an overclocked card, and I measured a near 200 MHz factory overclock in real terms, though the stated boost clock numbers suggest MSI was only aiming for around 150 MHz off the production line. As we've seen from successive generations of GeForce GPUs, however, those stated numbers rarely match a generally more positive reality. And so, possibly thanks to that swollen cooling array, I'm seeing an average clock speed in Metro Exodus of 2,728 MHz vs the Founders Edition at 2,530 MHz.
And even at that speed it's 22% cooler than the slight Founders Edition card. Sadly, all that extra frequency doesn't actually translate to much in the way of extra performance. On average I'm seeing a little under 5% higher frame rates at 4K, which is as good as it gets, with 1440p frame rates just 3% higher on average.
But, being a 2025 GPU it's still got some overclocking headroom left in there, and I was easily able to add another 240 MHz offset to the equation, which sees the Metro Exodus average clock speed hike up to 2,942 MHz. Which gets you another 2% higher frame rates. Which is… not significant, I'll grant you.
PC Gamer test rigCPU: AMD Ryzen 7 9800X3D | Motherboard: Gigabyte X870E Aorus Master | RAM: G.Skill 32 GB DDR5-6000 CAS 30 | Cooler: Corsair H170i Elite Capellix | SSD: 2 TB Crucial T700 | PSU: Seasonic Prime TX 1600W | Case: DimasTech Mini V2
If you're interested in running an RTX 5090 then chances are that you're not too worried about power draw, but it is worth noting the overclocked MSI card chows down a ton more power than the Founders Edition. The FE pulls 637 W at max, while the Surpim in gaming mode (there's a dual-BIOS switch if you don't want all the performance, for some reason) draws 687 W, and with my +240 MHz overclock it hits 697 W.
Still, we are talking about the absolute fastest graphics card I've ever strapped into the PC Gamer test rig, which has to count for something.
✅ You absolutely must have an ultra-cool RTX 5090: If you're the sort to whom money means nothing, then the Suprim SOC is the most powerful RTX 5090 we've tested, and far cooler than the FE.
❌ You can find literally any RTX 5090 for less: There's a lot of headroom in the GB202 GPU, and I would expect pretty much any third-party cooler will allow you to hit the same level of clock speeds the Suprim can offer without much effort.
And it probably would if the $750 price premium wasn't there staring me in the face all the while I've been testing this card. Spending 38% more for—at best—7% higher frame rates is frankly ludicrous. I'll give you that the RTX 5090 is largely out of stock now, and with the lower spec MSI RTX 5090 Gaming Trio on Amazon for $4,885 right now, if you could find it at that $2,750 sticker price it would seem like a bit of a bargain.
But honestly, I am not here for the grossly inflated pricing that has accompanied the GPU class of '25 so far, whether from scalpers on Ebay or Amazon, or from retailers or greedy AIBs.
However it rolls out once things have calmed down some, the extra pricing of the RTX 5090 Suprim still isn't justified by its actual performance. Even a couple hundred dollars would have been a push, but $750 more than the Founders card? Nah, that's just way too much for what you're getting here.
Now that MSI and Nvidia have figured out how to stop this monstrous GPU from blackscreening every time you play a game it's the most powerful graphics card I've ever used. But not by a margin big enough to explain away that ~$1,000 price premium above MSRP.
Dave has been gaming since the days of Zaxxon and Lady Bug on the Colecovision, and code books for the Commodore Vic 20 (Death Race 2000!). He built his first gaming PC at the tender age of 16, and finally finished bug-fixing the Cyrix-based system around a year later. When he dropped it out of the window. He first started writing for Official PlayStation Magazine and Xbox World many decades ago, then moved onto PC Format full-time, then PC Gamer, TechRadar, and T3 among others. Now he's back, writing about the nightmarish graphics card market, CPUs with more cores than sense, gaming laptops hotter than the sun, and SSDs more capacious than a Cybertruck.
Please logout and then login again, you will then be prompted to enter your display name.

PC Gamer is part of Future plc, an international media group and leading digital publisher. Visit our corporate site.

©
Future Publishing Limited Quay House, The Ambury,
Bath
BA1 1UA. All rights reserved. England and Wales company registration number 2008885."
"NVIDIA","RTX 5090","ASUS GeForce RTX 5090 TUF Review","https://www.techpowerup.com/review/asus-geforce-rtx-5090-tuf/",""
"AMD","RX 9070 XT","AMD Radeon RX 9070 XT and RX 9070 review: Excellent value, if supply is good","https://www.tomshardware.com/pc-components/gpus/amd-radeon-rx-9070-xt-review","When you purchase through links on our site, we may earn an affiliate commission. Here’s how it works.

The AMD Radeon RX 9070 XT provides strong mainstream performance at a great price. The RDNA 4 architecture delivers significant generational improvements in AI and ray tracing performance, without sacrificing rasterization performance. The biggest concerns will be retail availability and pricing — and the lack of any true high-end solutions.
Excellent mainstream value and performance
Big generational improvements compared to RDNA 3
Finally addresses AI and ray tracing properly
Concerns with retail availability and pricing
Still trails Nvidia in ray tracing and AI performance
Nvidia still wins on software support and features

Why you can trust Tom's Hardware




Our expert reviewers spend hours testing and comparing products and services so you can choose the best for you. Find out more about how we test.

The AMD Radeon RX 9070 XT and RX 9070 are here, ushering in the RDNA 4 GPU architecture and RX 9000 series of graphics cards. AMD spilled the beans on the hardware and specs last week, and we've already done a deeper dive into what makes these new GPUs tick, but now it's time to see how the RX 9070 XT and RX 9070 stand up to the best graphics cards — all while we wait to see what happens with the retail launch tomorrow and how quickly the supply disappears.

RDNA 4 represents a throwback to AMD architectures of years past, as the company is once again targeting mainstream performance and maybe even budget performance further down the road. But today, we're getting the $599 RX 9070 XT and $549 RX 9070 cards. And while some might feel cards at up to $600 don't qualify as ""mainstream,"" in today's market, we'd say mainstream stretches from around $400 up to $600, while anything below about $300 is clearly in the budget range. The PC graphics card market has become much more expensive in the past decade.

The one question we can't answer is what retail availability will look like. It seems like the AIBs have been stockpiling cards for about two or three months now, but how quickly were they being supplied the requisite GPUs? We don't know. Maybe there are tens of thousands of 9070 series cards just waiting to go on sale tomorrow; maybe there are only a few thousand. What we do know is that if there aren't enough to meet demand, prices are going to head north, just as they did with the RTX 50-series launches of the past two months.
If you want to know more about the AMD RDNA 4 architecture and Radeon RX 9000-series GPUs, start with our ""everything you need to know"" primer that goes into a lot more detail about the design and architectural changes that power these graphics cards.
Speaking of which, the Nvidia RTX 5070 officially goes on sale this morning. Of course we knew the performance of the RX 9070 XT and 9070 when we posted that review yesterday. What we don't know — what no one outside of Nvidia and its distributors and retail partners knows — is how many 5070 cards will be available today. The RTX 5070 Ti, RTX 5080, and RTX 5090 have all sold out almost immediately, and we've seen prices shoot up by 50% or more relative to the MSRPs.

Will the AMD graphics cards buck that trend or join the ""party?"" We'll find out in the coming days, but considering what happened with Intel's Arc B580, it's obvious that lower-priced cards aren't immune from the potential supply and demand problems. Our default assumption right now, based on nearly all prior generation graphics cards already being sold out and/or overpriced — with only the RTX 4060 and RX 7600 still available at or near MSRP — is that RDNA 4 isn't going to be a magic bullet to solve the availability issues plaguing the graphics card market right now.

Let's also clear the air on the comparison GPUs in our charts. We've (or at least I've) been testing GPUs more or less constantly since the beginning of 2025. Drivers keep changing, certain tests that failed to run in the past have been fixed, bugs come and go, and we have a new GPU testbed and test suite. Ideally, we'd love to have every reasonable comparison present in the charts, but it will be a while before we have all the data compiled at a rate of a few GPUs getting tested per week.

So, the RTX 4070 Super wasn't tested for the 5070 review, not because we don't think it's important but because of time. Similarly, the RX 7900 GRE won't be in this review because we don't have time. Eventually, we'll get those tested, and all the data will be available in our GPU benchmarks hierarchy. You should be able to reasonably estimate where those 'missing' cards would land, and as both of those were later additions to their respective GPU families, it seemed to make more sense to leave those out rather than some other GPUs.

All good? Good. Let's hit the specs.
The raw specs are interesting, but there's more to GPU performance than specs. For example, Intel's Arc B580 as an example has ""worse"" compute performance than the Arc A770: 14.6 TFLOPS versus 19.7 TFLOPS. But in actual benchmarks, the B580 is up to 17% faster across our gaming test suite at 1440p. Both AMD and Nvidia have also updated their core architectures to improve performance, and today we find out just how much.

The RX 9070 XT offers theoretical peak compute of 48.7 TFLOPS for FP32, which is used for graphics, and up to 1557 TOPS of INT4 AI compute (with sparsity). The previous generation RX 7900 XTX offers 61.4 TFLOPS of FP32, but only 122.8 TFLOPS of FP16 for AI workloads — or alternative 122.8 TOPS of INT8 compute. We'll spoil the surprise a bit by saying that, for a lot of games, the 7900 XTX is still faster... but in AI tasks and RT games, the tables can turn.

It's not just compute performance that matters, of course. Memory bandwidth and capacity are also factors. The 7900 XTX had a 384-bit interface and 24GB of VRAM, compared to the 9070 XT and 9070 with 256-bit interfaces and 16GB of VRAM. In all cases the memory is GDDR6 clocked at 20 Gbps, so the prior generation halo card had 50% more bandwidth and capacity.

There's also the RT accelerators. AMD's RDNA 4 has doubled the ray/triangle and ray/box intersection rates with RDNA 4 compared to RDNA 3, which means the 64 RT units in the 9070 XT should be the performance equivalent of 128 RDNA 3 RT units, but the 7900 XTX only has 96 RT accelerators. So that's potentially 33% higher ray tracing performance from the new generation.
As already noted, the prices on paper look good. What we don't know is whether prices will stay close to what AMD recommends, or if they'll get jacked up by the retail outlets and AIBs. Because AMD isn't making any graphics cards itself this round, it will be up to the add-in board (AIB) partners to determine prices on the various models. There are probably requirements for each company to have an MSRP priced GPU, but we've seen those disappear in the past — or things like Asus's ""special launch pricing"" on some of its RTX 50-series cards.

We can also look at what graphics cards are available at retail. Last November, during the holiday shopping season, most graphics cards went on sale at prices below MSRP. And then they were gone. Now, virtually everything at the usual places for the U.S. — Newegg, Amazon, B&H, Best Buy, etc. — is either out of stock or seriously overpriced. RX 7900 XTX was selling for as little as $819, now the best price we can find is $1,094 for a PSU and GPU combo, and after that the price jumps to $1,283 at Amazon.

The same pattern applies to pretty much every other GPU. Outside of the RTX 4060, RX 7600, and Arc B570, we can't find anything at MSRP, never mind below MSRP. If you want a mainstream or higher performance GPU, it's currently overpriced compared to just a month or two back. Given the scarcity of any graphics card with an MSRP above $400, then, it's hard to imagine the 9070 XT and 9070 will stay at MSRPs in the near term. But we'll wait and see.

Current page:

Introducing the AMD Radeon RX 9070 XT and RX 9070


Jarred Walton is a senior editor at Tom's Hardware focusing on everything GPU. He has been working as a tech journalist since 2004, writing for AnandTech, Maximum PC, and PC Gamer. From the first S3 Virge '3D decelerators' to today's GPUs, Jarred keeps up with all the latest graphics trends and is the one to ask about game performance.

Tom's Hardware is part of Future US Inc, an international media group and leading digital publisher. Visit our corporate site.

©
Future US, Inc. Full 7th Floor, 130 West 42nd Street,
New York,
NY 10036."
"AMD","RX 9070 XT","AMD Radeon RX 9070 XT Review","https://www.techspot.com/review/2961-amd-radeon-9070-xt/","Much has been said and speculated about AMD's Radeon 9070 series GPUs, so much so that it feels like we've reviewed them already. But until now, we've only had leaks and first-party benchmarks to go on, neither of which are particularly reliable. But finally, today we're taking an in-depth look at AMD's top RDNA 4 GPU to see what it's truly capable of.
AMD is releasing the standard Radeon RX 9070, which is a binned version of the 9070 XT, and the full-fledged XT version we're testing today. Both are based on the same 357mm² die, manufactured using TSMC's N5 process – the same process used for RDNA's graphics compute die.
The Radeon 9070 XT features 4,096 cores, while the 9070 has a reduced core count of 3,584 – a 13% decrease. The 9070 operates at 2,070 MHz, which is 14% lower than the XT version, though actual clock speeds under load will vary. Both models share the same memory configuration: 16GB of 20Gbps GDDR6 memory on a 256-bit memory bus, delivering 640GB/s of bandwidth.
This design suggests that the 9070 series should be cost-effective to produce, given its die size is similar to that of the 7800 XT, and the memory subsystem remains unchanged. AMD managed to sell the 7800 XT for $500 or less, so even the cut-down 9070 should be profitable. The 9070 has an MSRP of $550, while the full 9070 XT is set at $600.
On paper, the comparison between the previous-gen 7900 XT and the 9070 XT is intriguing. The 9070 XT has 24% fewer cores, a 33% smaller die, 20% less cache, 20% less VRAM, and 20% lower memory bandwidth – yet it's expected to deliver similar or better performance. That will be very interesting to test.
For testing, we have several Radeon 9070 XT graphics cards on hand. All benchmark results were obtained using the Sapphire Pure, but we also have the Nitro+, XFX Mercury, Asus TUF Gaming, and Asrock Taichi models.
Under load, hotspot GPU temperatures ranged from 75°C (XFX Mercury) to 87°C (Nitro+). Interestingly, the Sapphire Pure maxed out at 81°C, which is lower than expected given the Nitro+ result. The Asus TUF Gaming ran at 80°C, while the Asrock Taichi peaked at 85°C. However, since these are hotspot temperatures, all models ran relatively cool overall.
The GDDR6 memory operated at 85°C on the Asus and XFX models, while the rest reached 90°C.
For GPU VRM temperatures, there was a significant range. The Asrock Taichi and Sapphire Nitro+ peaked just above 80°C, while the Sapphire Pure reached 79°C. Meanwhile, the XFX Mercury and Asus TUF Gaming managed to keep their VRM temperatures comfortably below 70°C.
When it came to noise levels, the Asrock Taichi, Sapphire Pure, and Nitro+ were virtually silent, producing less noise than our case fans. The XFX Mercury was also very quiet, only barely audible over the case fans. The Asus TUF Gaming, however, was noticeably louder but still reasonably quiet.
As for core clock speeds, all models performed similarly, with no more than a 3% variance out of the box. Now, let's get into the blue bar graphs…
First up, we have Marvel Rivals at 1440p, where the 9070 XT averaged 88 fps. This made it 7% slower than the 5070 Ti and just 6% faster than the 7900 XT – not a bad result, but not an outstanding one either.
At 4K, the gap widened, with the 9070 XT trailing the 5070 Ti by an 11% margin, delivering an average of 51 fps.
Moving on to Stalker 2, the 9070 XT once again lagged behind the 5070 Ti at 1440p, this time by a 9% margin, making it 2% slower than the 7900 XT – a disappointing outcome.
Increasing the resolution to 4K helped slightly, narrowing the gap to 5% behind the 5070 Ti while still managing to be 5% faster than the 7900 XT. However, with just 39 fps on average, the performance remains underwhelming.
When testing Counter-Strike 2, the 9070 XT was 10% slower than the 5070 Ti, a significant margin. In fact, it only managed to match the 7900 GRE, making this a very weak result. Unfortunately, AMD did not benchmark this game in their review guide, so we cannot confirm if they observed the same poor performance. It is also surprising that they did not test the most-played game on Steam.
At 4K, things got even worse. Here, the new Radeon GPU trailed the 5070 Ti by 16%, performing roughly on par with the RTX 5070.
God of War Ragnarök was the first game where the 9070 XT was able to match the 5070 Ti. While it was technically 1% faster at 1440p, this is within the margin of error. However, achieving RTX 4080-like performance here was impressive.
The 4K results followed a similar trend, with the 9070 XT just 1% slower than the 5070 Ti, again delivering performance comparable to the 7900 XTX and RTX 4080.
The Delta Force performance was underwhelming. The 9070 XT averaged 160 fps, making it 8% slower than the 5070 Ti and 10% slower than the 7900 XT, putting it in line with the 4070 Super.
At 4K, results improved significantly, with the 9070 XT matching the 7900 XT. However, it was still 6% slower than the 5070 Ti.
The 9070 XT performed exceptionally well in Space Marine 2, even surpassing the 7900 XTX. It was 17% faster than the 5070 Ti at 1440p.
At 4K, the gap widened significantly, with the 9070 XT outperforming the 5070 Ti by 36%. The reason for the poor performance of the new GeForce 50 series in this game remains unclear, and Nvidia has yet to address the issue.
In Star Wars Jedi: Survivor, the 9070 XT was actually slower than the 7900 XT at 1440p – albeit by just 2 fps. More importantly, this made it 12% slower than the 5070 Ti.
At 4K, the gap narrowed slightly to 8%, but with an average of just 53 fps, the 9070 XT was only marginally faster than the 4070 Ti Super and 7900 XT.
The 9070 XT performs well in A Plague Tale: Requiem, coming in just behind the 7900 XTX at 1440p. However, this makes it 7% slower than the 5070 Ti. Still, it's a solid result overall.
At 4K, the 9070 XT is 4% slower than the 5070 Ti, averaging 65 fps. This puts it at the same performance level as the RTX 4080, which is impressive.
Radeon GPUs have always performed well in Cyberpunk 2077, and the 9070 XT is no exception. At 1440p, it rendered 125 fps, matching the 5070 Ti and outperforming the 7900 XT by 11%.
At 4K, it pulled slightly ahead of the 5070 Ti, though only by a 3% margin. More notably, this put it 17% ahead of the 7900 XT – an excellent result overall.
Unfortunately, the 9070 XT struggles in Dying Light 2, only barely beating the 7900 XT and coming in 12% slower than the 5070 Ti.
The 4K results tell a similar story, with the 9070 XT once again trailing the 5070 Ti by 12%, though it was also 12% faster than the 7900 XT.
Call of Duty has always favored AMD, and Black Ops 6 is no exception. The 9070 XT delivered an impressive 118 fps at 1440p, roughly matching the 7900 XTX and beating the 5070 Ti by a significant 22% margin.
At 4K, it maintained a 19% lead over the 5070 Ti, once again delivering performance comparable to the 7900 XTX – an excellent result for the new 9070 XT.
Next, we have Dragon Age: The Veilguard. This game utilizes some ray tracing with the ultra preset, but even so, the 9070 XT performed decently, coming in 4% slower than the 5070 Ti. However, we are essentially looking at 7900 XT-level performance in this case.
At 4K, the 9070 XT matched the 4070 Ti Super, making it 10% faster than the 7900 XT but also 10% slower than the 5070 Ti.
The Radeon GPU struggles against the GeForce competition in War Thunder when using the default DX11 mode. At 1440p, the 9070 XT could only match the 7900 XT, making it a massive 31% slower than the 5070 Ti.
However, increasing the resolution to 4K changed things dramatically. Here, the 9070 XT delivered an impressive 227 fps, matching both the 7900 XTX and 5070 Ti.
Performance in Spider-Man Remastered was disappointing. At 1440p, the 9070 XT was slower than the 7900 XT and trailed the 5070 Ti by 8%.
As seen in many cases, the 4K results were more favorable. Here, the 9070 XT matched the 5070 Ti, delivering 118 fps.
The Hogwarts Legacy performance looks promising – until we consider that, despite being 4% faster than the 5070 Ti at 1440p, the 9070 XT is still slower than both the 7900 XT and XTX.
However, things change at 4K. Here, the 9070 XT outperformed not only the 5070 Ti but also the 7900 XTX, delivering an impressive 87 fps on average.
In The Last of Us Part I, the 9070 XT was 5% slower than the 5070 Ti, averaging 106 fps. This also made it a few frames slower than the 7900 XT.
At 4K, it was 3% slower than the 5070 Ti but at least 9% faster than the 7900 XT.
Performance in Star Wars Outlaws was solid. At 1440p, the 9070 XT was 5% slower than the 5070 Ti but 13% faster than the 7900 XT.
At 4K, it essentially matched both the 5070 Ti and 7900 XTX, making it 21% faster than the 7900 XT.
Finally, we have Starfield, where the 9070 XT was 7% slower than the 5070 Ti at 1440p, delivering performance comparable to the 7900 XT.
The 4K results were more favorable, but even then, it only managed to roughly match the 5070 Ti while slightly outperforming Nvidia's RTX 4070 Ti Super.
Across the 18 games tested, the 9070 XT averaged 119 fps at 1440p, making it 6% slower than the 5070 Ti. AMD likely expected parity here, and while it's not far off, the Radeon GPU was still slower overall.
More concerning is AMD's claim that the 9070 XT offers a 35% improvement over the 7900 GRE in raster performance, whereas our testing found only a 20% increase in mostly rasterized workloads. Additionally, the 9070 XT is only 2% faster than the 7900 XT and 13% slower than the 7900 XTX, which is not an ideal position to be at.
At 4K, the 9070 XT fares slightly better, trailing the 5070 Ti by just 1% while outperforming the 7900 XT by 9%. While not an outstanding result, at a price of $600, it could still offer good value – something we will examine shortly.
In terms of power consumption, the Radeon 9070 XT is neither exceptional nor disappointing, but compared to RDNA 3, there is little improvement. As seen in testing, it delivers performance similar to the 7900 XT while consuming about the same amount of power.
In many cases, the 9070 XT uses around 20% more power than the 5070 Ti while delivering slightly weaker performance.
When it comes to ray tracing performance, the 9070 XT is significantly better than RDNA 3 GPUs such as the 7900 XT and even the 7900 XTX. In Metro Exodus at 1440p, the 9070 XT was 9% slower than the 5070 Ti but also 34% faster than the 7900 XT and 12% faster than the XTX.
At 4K, it trailed the 5070 Ti by a 12% margin, yet outperformed the 7900 XTX by 18% and the 7900 XT by a massive 44%.
Performance in Alan Wake II isn't great, but this is the first Radeon GPU capable of exceeding 30 fps at 1440p with upscaling while using high ray tracing settings. That's a notable achievement. The 9070 XT delivered a 44% uplift over the 7900 XTX, but despite this, it remained 36% slower than the 5070 Ti.
At 4K, the demands are extreme, with even the 5070 Ti managing only 31 fps on average.
The 9070 XT's ultra ray tracing performance in Cyberpunk 2077 is quite impressive. At 1440p with upscaling, it rendered 68 fps, making it just 8% slower than the 5070 Ti – an outstanding result for a Radeon GPU. This also meant it was 24% faster than the 7900 XTX and 51% faster than the 7900 XT.
As expected, performance at 4K with upscaling is more challenging, but even then, the 9070 XT was only 12% slower than the 5070 Ti while still outperforming the 7900 XTX by 29%.
Despite encountering a CPU bottleneck in Spider-Man Remastered, the 9070 XT was still 22% faster than the 7900 XTX at 1440p.
At 4K, the 9070 XT trailed the 5070 Ti by just 7%, though it was a few frames slower than the XTX.
Next, we have Dying Light 2, where the 9070 XT was 14% slower than the 5070 Ti at 1440p, but still slightly faster than the 7900 XTX – another strong result.
At 4K, it once again edged out the XTX, though it remained 29% slower than the 5070 Ti.
Black Myth: Wukong remains a tough challenge for Radeon GPUs. The 9070 XT managed just 30 fps at 1440p, which is an almost 60% improvement over the 7900 XTX but still only half the performance of the 5070 Ti.
Given that, there's little need to discuss the 4K results – but for completeness, they are included.
Finally, in Indiana Jones and the Great Circle, the 9070 XT struggled, delivering just 17 fps at 1440p with upscaling.
The 4K results were even worse, though at this point, none of these GPUs provided what could be considered playable performance.
Here's a look at the average ray tracing performance at 1440p. The 9070 XT was 21% slower than the 5070 Ti but an impressive 26% faster than the 7900 XTX. With RDNA 4, AMD has made significant strides in ray tracing performance.
Many of the 4K results were disappointing for the Radeon 9070 XT, though the 5070 Ti also struggled in several titles. Even so, based on the data, the 9070 XT was, on average, 25% slower than the 5070 Ti.
Now, it's time for the all-important cost-per-frame analysis, starting with MSRP. However, MSRP is often misleading, particularly for Nvidia's Blackwell GPUs, which are difficult to find at their suggested prices.
If the GeForce RTX 5070 Ti was readily available at $750 – which it currently isn't – the 9070 XT would offer just 15% better value. While that is reasonable, AMD has historically (in our opinion) needed to provide at least 20% better value to secure a strong recommendation in reviews.
With RDNA 4, however, AMD has improved ray tracing performance and introduced FSR 4, which we are in the process of analyzing and will have a dedicated feature with tons of details very soon. Given these advancements, a 15% value advantage might be enough to earn a recommendation. We'll discuss this more shortly, but first, let's examine the ""real"" MSRP situation.
If AMD can launch the 9070 XT at $600 – which multiple reliable sources indicate they can and will – then the card would offer 29% better value than the 5070 Ti.
This is a key factor because, based on sales data, AMD typically needs to deliver at least 30% greater value than the nearest GeForce competitor to convince GeForce owners to switch to Radeon.
Comparing the Radeon 9070 XT to the best retail prices from mid-2024, it still presents a strong value proposition. It delivers nearly 20% better value than the discounted 7900 XT, which has been AMD's best value offering in this performance segment. Additionally, it offers 26% better value than the 4070 Ti Super.
So there you have it – the new Radeon RX 9070 XT. On one hand, we feel some people will be disappointed with its performance, even considering the price. Perhaps that's on AMD and their performance claims, which we'll address soon. However, in the current market, the Radeon 9070 XT offers solid value and is worthy of recommendation.
There are two main conclusions to draw here:
It's also worth noting that while we've compared the Radeon 9070 XT's performance to the GeForce 5070 Ti, its pricing is much closer to the standard RTX 5070. However, the RTX 5070 is a poor-value option, offering just 12GB of VRAM, which means we won't recommend it. Comparing the 9070 XT to a GPU we already advise against seems like a waste of time.
Internally, when we evaluated the Radeon 7000 vs. GeForce RTX 40 series, we agreed that AMD needed to offer at least 20% better cost-per-frame value for a Radeon GPU to earn our recommendation.
However, with the Radeon 9000 vs. GeForce RTX 50 series we believe this threshold can be adjusted downward to 15% thanks to the improved ray tracing performance and because AMD's upscaling technology has improved dramatically, closing the gap to DLSS.
Full disclosure: we are still busy investigating FSR 4, so it's a little to early to call right now, but what we can say is FSR4 is worlds better than FSR3. Whether gamers at large will feel the same remains to be seen, and AMD may still need to aim for a 30% discount to truly attract buyers.
In a world where the 9070 XT can be purchased for $600 and the 5070 Ti for $750, it's difficult to recommend one over the other, there are pros and cons both ways, which is why we felt for AMD to really crush Nvidia's mid-range this generation the 9070 XT would have to hit $550... but, if supply of the 5070 Ti remains weak, and therefore prices remain sky high, the 9070 XT should still come out on top.
If AMD can maintain a 20% cost-per-frame advantage over the 5070 Ti, then the 9070 XT becomes the obvious choice – no debate necessary. Just buy the Radeon GPU and enjoy the savings.
Now to change gears for a moment, one concern is that AMD may have set unrealistic expectations for the 9070 XT. Comparing it to the 7900 GRE while citing heavily inflated performance figures – at least relative to our independent testing – doesn't help its case.
AMD claimed the 9070 XT was, on average, 35% faster than the GRE in raster performance. However, our testing found only a 20% improvement.
For example, AMD's claims vs. our actual results:
Perhaps in games without built-in benchmarks, AMD is testing in areas with lighter loads. Regardless, AMD's numbers make the GRE appear much slower than it actually is. If the 9070 XT was truly 35% faster than the GRE, it would match the 7900 XTX and RTX 4080 Super, and many people expected that based on AMD's first-party benchmarks.
Finally, we have some concerns about how ""real"" the $600 MSRP actually is. After some investigation, it appears that AMD is providing retailers with a $50 rebate to achieve the $600 pricing. This strongly suggests the intended MSRP was actually $650, and AMD is temporarily subsidizing models to hit the lower price point.
For example, XFX confirmed that the 9070 XT Mercury – a model featured in this review – will not cost $650. In fact, it won't even cost $700. Instead, the official MSRP is $770, and due to tariffs, its on-shelf price is expected to be $850 – which would be tragic if true.
From what we've gathered, it seems AMD is starting to play Nvidia's pricing game. This means that while some 9070 XT models may be available at $600 initially, most will likely be priced higher, and restocks at that price may be limited or infrequent. A lot will depend on how sales perform. AMD has a large stock of Radeon 9070 GPUs, so if demand slows after launch, we expect them to continue offering rebates to keep pricing competitive. However, we will have to wait and see how that plays out.
As things stand, the Radeon RX 9070 XT is a strong offering that we expect will sell extremely well at $600. However, whether that price holds long-term is questionable. We've also heard from multiple sources that supply is excellent. If that's the case, then AMD may end up selling more 9070 XT units than Nvidia has sold 5090, 5080, 5070, and 5070 Ti units combined – which would be crazy, and on that bombshell, we'll end this review right here.

About
Ethics Statement
Terms of Use
Privacy Policy
Manage Ad Consent

TECHSPOT : Tech Enthusiasts, Power Users, Gamers
TechSpot is a registered trademark and may not be used by third parties without express written permission.
TechSpot is represented by Future PLC. Learn about advertising opportunities.
© 1998 - 2025 TechSpot, Inc. All Rights Reserved."
"AMD","RX 9070 XT","Sapphire Radeon RX 9070 XT Nitro+ Review","https://www.techpowerup.com/review/sapphire-radeon-rx-9070-xt-nitro/",""
"AMD","RX 9070 XT","AMD Radeon RX 9070 / 9070 XT review: Nvidia gets some big next-gen competition","https://www.theverge.com/gpu-reviews/624423/amd-radeon-rx-9070-xt-review-benchmarks-price","﻿AMD’s two new GPUs beat Nvidia’s RTX 5070 and put the pressure on pricing.
Nvidia’s dominance over the latest generation of graphics cards ends today. AMD might have surrendered the high end of the GPU market, but its new $549 Radeon RX 9070 and $599 RX 9070 XT look set to bring Nvidia back to reality, at least in the midrange. 
After a disappointing $549 RTX 5070, a $749 RTX 5070 Ti that’s near impossible to buy for anything less than $899, and a $1,999 RTX 5090 whose only competition is other Nvidia cards, AMD’s next-gen GPUs look perfectly timed and deliver on performance and price. 
The RX 9070 is around 17 percent faster than Nvidia’s RTX 5070 in 4K without upscaling enabled, cruising past Nvidia’s card for the same price. The RX 9070 XT is as fast as the RTX 5070 Ti for $150 less.
Pricing is key with these cards, particularly as only 10 percent of GPU sales right now are from AMD. The company needs a big win with its latest generation of cards if it wants to regain ground from Nvidia, and the RX 9070 series might be just what the GPU market needs.
16GB of VRAMFSR 4 image quality improvementsLower power draw than previous Radeon cards
No AMD reference designBig and bulky card
The $599 price tagFSR 4 image quality improvementsLower power draw than previous Radeon cards
I wish it had a little more VRAMNo AMD reference designBig and bulky card
AMD hasn’t created reference cards for the RX 9070 series, so the end result is designs from add-in board partners that look very similar to the ones used for previous generations of Radeon GPUs. The Gigabyte RX 9070 I’ve been testing is longer and bulkier than the RTX 5070.
The XFX 9070 XT I’ve been testing is also a 3.5-slot card, so it takes up far more space in a case than the two-slot RTX 5080 or 5090 Founders Edition or even the 2.5-slot RTX 5070 Ti card I reviewed. I far prefer the reference designs AMD has used in the past, and the RX 9070-series cards are big and bulky in comparison. AMD has also stuck to using two 8-pin PCIe power connectors with the RX 9070 series, instead of switching to 12VHPWR. It does mean you’ll need to run two PCIe connectors to both the RX 9070 and 9070 XT, but you won’t need to worry about PSU compatibility or the risk of a 12VHPWR connector melting.
Both the RX 9070 and 9070 XT ship with 16GB of VRAM, which is a good amount at these prices. Nvidia only has 12GB of VRAM on its $549 RTX 5070, so you’re certainly getting more value from a $549 RX 9070 here — especially if you’re looking at 4K gaming where VRAM demands are higher.
I’ve been testing AMD’s RX 9070 and 9070 XT cards with an AMD Ryzen 9 9800X3D processor and Asus’ 32-inch 4K OLED PG32UCDP. I’ve put the RX 9070 and 9070 XT up against Nvidia’s RTX 5070 and RTX 5070 Ti as well as its existing RX 7900 XT and XTX GPUs. 
I’ve tested a variety of games at both 1440p and 4K, including more demanding titles like Black Myth: Wukong and benchmarking favorites like Cyberpunk 2077 and Shadow of the Tomb Raider. All games have been tested at very high or ultra settings, and I enabled ray tracing in some titles like Cyberpunk 2077 and Metro Exodus to see what AMD’s latest GPUs can deliver there.
The RX 9070 comfortably beats Nvidia’s RTX 5070 at both 4K and 1440p. It’s 17 percent faster at 4K and nearly 20 percent faster at 1440p. It’s not even close. In some games, like Horizon Zero Dawn, the RX 9070 is more than 30 percent faster, with or without FSR enabled.
The $599 RX 9070 XT competes with Nvidia’s $749 RTX 5070 Ti, with an average frame rate difference of less than 1 percent without upscaling across the nine games we tested. The differences between these two cards really come down to the individual games, with the RX 9070 XT less than 5 percent behind the RTX 5070 Ti in titles like Returnal and Metro Exodus Enhanced.
The RX 9070 comfortably beats Nvidia’s RTX 5070 at both 4K and 1440p
The RX 9070 XT isn’t that far behind AMD’s previous flagship, the $999 RX 7900 XTX, either. I found the XTX was less than 2 percent faster at 4K and 1440p tests without upscaling enabled. That’s a small performance gap, considering the price differences between these cards.
I think the RX 9070 makes the most sense at 1440p, and the RX 9070 XT is definitely worth the extra $50 if you want to move to 4K gaming. Considering the small price gap, it’s worth paying the extra $50 for better 1440p performance, too. 
AMD’s 9000-series GPUs are built on top of the company’s RDNA 4 architecture, which AMD says offers twice the rasterization graphics performance per compute unit as RDNA 2. This new architecture also includes improvements to boost ray-tracing performance.
In Metro Exodus Enhanced with ultra ray tracing enabled, the RX 9070 XT is nearly 7 percent faster than the RX 7900 XTX, but it’s 4 percent behind the RTX 5070 Ti. Nvidia still reigns supreme in Cyberpunk 2077 at 4K with ray tracing and upscaling enabled, though, with the RTX 5070 Ti 16 percent ahead of the RX 9070 XT.
The RDNA 4 architecture also includes a new version of AMD’s FidelityFX Super Resolution (FSR) upscaling, which now uses dedicated AI accelerator hardware to improve frame rates and image quality — much like Nvidia’s DLSS. FSR 4 is exclusive to these new RX 9070 graphics cards, and it works in all games that have FSR 3.1 integrated. All you have to do is flick a switch in the AMD software, and FSR 4 will be enabled automatically in supported games.
With FSR 4, AMD hasn’t followed Nvidia with Multi Frame Generation, instead focusing on improving image quality in upscaled games. I think that’s a better approach as FSR needed improvements. The 9070 XT and the RTX 5070 Ti put out virtually identical frame rates at 1440p with or without upscaling. The same is true when you compare AMD’s frame gen to Nvidia’s equivalent 2x Multi Frame Gen. Nvidia does pull ahead in raw numbers when you enable 3x and 4x frame gen, which AMD doesn’t have. But as I’ve discussed at length elsewhere, all it’s doing here is shoving more AI-generated frames in between traditionally rendered frames. It makes motion look smoother but not feel smoother, and I don’t really consider it representative of actual performance.
So instead of focusing on fake frames, I tested the FSR 4 image quality changes in both Call of Duty: Black Ops 6 and Horizon Zero Dawn Remastered. In Black Ops 6, the advances are immediately obvious. The current FSR 3.1 implementation often results in a more soft or blurry image, which is especially noticeable in text on walls or objects that are slightly further away. FSR 4 makes the overall image look a lot sharper and, in some cases, even better than the native 4K output. A police car has a blurry numberplate and logo in FSR 3.1 quality mode. But in FSR 4 quality, it’s upscaled correctly, and the entire car looks a lot sharper as a result. Details on buildings are also more clear, and even enemies look more detailed in motion with FSR 4. And this all comes without a noticeable performance hit, which is great. 
The FSR 4 improvements are less obvious in Horizon Zero Dawn Remastered, and I struggled to notice any difference between FSR 3.1 quality and FSR 4 quality modes. While this might suggest your mileage may vary with FSR 4 quality, I was able to use ultra performance FSR 4 and get surprisingly good visual quality and higher frame rates from the game.
Beyond gaming, the RX 9070 cards are both a lot more capable for AI workloads. In Procyon’s AI XL (FP16) test, the RX 9070 XT is more than 40 percent faster than the RX 7900 XTX, but it’s around 20 percent behind the AI performance of the RTX 5070. The RX 9070 is also more than 40 percent faster in the AI XL test over the RX 7900 XT, but the RTX 5070 is nearly 40 percent faster in this AI workload test.
I normally test video editing capabilities with PugetBench’s DaVinci Resolve benchmark, but it gets around 20 percent into the test and crashes on AMD’s GPUs.
It’s great to see AMD lower power supply requirements and power draw on its latest Radeon GPUs. AMD recommends a 750-watt power supply for the RX 9070 XT, for its 304 watts of total board power. I’ve found it delivers performance close to the RX 7900 XTX while using 40 watts less power at 4K. The 9070 XT averaged at 303 watts during my tests, while the 7900 XTX averaged 346 watts.
It’s the same story for the RX 9070. AMD recommends a 650-watt power supply for this card, 100 watts less than the 750-watt recommendation for the 7900 XT. In my tests, the RX 9070 averaged 244 watts, while the 7900 XT averaged more than 25 percent more power draw at 307 watts. That’s a big efficiency improvement, and it’s only 34 more watts of power draw than the RTX 5070 for a noticeable performance gap between these two $549 cards.
The RX 9070 also hit a maximum temperature of 60 degrees Celsius in my open testbench, compared to 70C for the RX 7900 XT. The RX 9070 XT reached just 55C in my testing, compared to 71C for the RX 7900 XTX. Nvidia’s RTX 5070 Founders Edition card reached 77C in the same tests, with MSI’s Ventus 3 RTX 5070 Ti reaching 68C. 
AMD’s latest generation of Radeon GPUs are more efficient and run cooler than its previous generation, and they also run cooler than Nvidia’s direct competitors. That’s a clear win for AMD.
AMD has priced its RX 9070-series cards really aggressively, particularly with the $599 RX 9070 XT being $150 cheaper than the RTX 5070 Ti. The $549 RX 9070 is the same price as Nvidia’s RTX 5070, but it beats the 5070 in every game I’ve tested, sometimes by a big margin.
If AMD manages to have these cards in stock at these prices, then it’s going to shake up the important midrange of the GPU market.
The question is whether it can chip away at Nvidia’s dominance. In addition to (or because of) its 90 percent market share, Nvidia also has an advantage in how many more games support DLSS than FSR. And DLSS is so popular that 80 percent of RTX owners activate it in games. FSR 4 improves image quality to compete more closely with DLSS, but AMD still needs game developers to implement it. Nvidia lets you simply force DLSS 4 in games, so you don’t need to wait on developers at all.
The stickiness of DLSS could tempt PC gamers to spend a little more to stay with Nvidia, and I haven’t seen enough from FSR 4 yet to know whether it can compete with the upscaling improvements Nvidia made with DLSS 4.
Nvidia’s Game Ready Drivers are also an advantage; the company works with developers to optimize drivers for games before they launch, whereas AMD has long had driver issues with a variety of games. I haven’t run into any issues while testing the RX 9070 series, but I can’t promise that will be the case when new games are released. While AMD says it has been working hard on improving its drivers, only time will tell.
Either way, AMD’s RX 9070-series cards are a clear indication that it’s not giving the entire GPU market to Nvidia. It’s willing to compete on performance and price, and that’s a winning formula if AMD manages to keep those prices under control.
A weekly newsletter by David Pierce designed to tell you everything you need to download, watch, read, listen to, and explore that fits in The Verge’s universe.
© 2025 Vox Media, LLC. All Rights Reserved"
"AMD","RX 9070 XT","AMD Radeon RX 9070 XT Content Creation Review","https://www.pugetsystems.com/labs/articles/amd-radeon-rx-9070-xt-content-creation-review/","Fresh off the heels of NVIDIA’s 50-series release, AMD has launched its new Radeon™ RX 9070 XT and 9070 GPUs. Based on AMD’s RDNA 4 architecture, they promise to offer competitive performance in the midrange slot, replacing the outgoing 7800 XT and 7900 GRE and competing with the NVIDIA GeForce RTX™ 5070. AMD’s big push for this generation appears to be adding, optimizing, and improving its accelerators within the GPU and further bringing its software stack to be on par with NVIDIA.
This generation, AMD has implemented a number of high-level architectural improvements that should boost performance across a variety of use cases. To start with, the AMD Radeon RX 9070 XT is based on AMD’s RDNA 4 architecture, which features a number of improvements to the memory subsystem, scalar units, and scheduler. Though less distinct from the compute unit in NVIDIA’s designs, AMD also has next-generation Ray Tracing cores, improved dual media engines, and, for the first time, dedicated matrix compute accelerators (AI accelerators).
And AMD makes bold claims about these. For Ray Tracing, it promises twice the throughput from its RT cores (though, of course, that doesn’t mean 2x the performance). The Media Engine should now support H.264 and HEVC (though we have yet to see a full list of supported “flavors” of those codecs) in addition to AV1, with improved performance and unlimited streams. For its new AI accelerators, AMD claims support of FP16 and INT8, now both with sparsity, as well as, as a first for them, FP8 formats; all told, they estimate AI performance gains of up to 8x.
AMD has also released new versions of its image-upscaling solution, FSR4, reduced-latency pipeline, Anti-Lag 2, and frame interpolation technology, Fluid Motion Frames 2.1. These are all essentially gaming-oriented, but it is good to see AMD continuing to compete with NVIDIA on not just the hardware side but also the software and feature side. Unfortunately, we will have to leave reviewing these features to other publications.
Below, we have listed the most relevant GPU specifications from AMD, Intel, and NVIDIA. For more information, visit Intel Ark, NVIDIA’s 40-series GeForce page, NVIDIA’s 50-series GeForce page, or AMD’s Radeon RX Page.
The RX 9070 XT is unique because, for the first time since about 2020, they aren’t trying to compete with NVIDIA in the high-end market. This means that our previous points of comparison—the 7900 XTX and 6900 XT—had MSRPs well above what the 9070 XT is targeting; the 9070 XT is $600, $400 less than the 7900 XTX. Still, compared to a 9700 XTX, the 9070 XT has a 50 W lower power draw in order to run two-thirds of the shader units 500 MHz faster. In other words, it’s hard to draw meaningful comparisons. Compared to NVIDIA, the 9070 XT is $150 less than a 5070 Ti for the same amount of VRAM with 75% of the memory bandwidth at the same power draw, or $50 more than the 5070 for 4 GB more VRAM, roughly the same memory bandwidth, and 50 W higher power draw. Interestingly, AMD seems set to cannibalize one of its own GPUs this generation, as the 9070 XT and 9070 are only $50 apart, with the same memory subsystem and only a 14% difference in shader units.
Ultimately, comparing specs within a hardware generation is hard enough, let alone across architectures or manufacturers. We don’t have a great idea of how we expect the 9070 XT to perform, but a win for us would be to approach the 5070 Ti, while falling behind the 5070 would be disappointing. Additionally, AMD has touted much-improved Ray Tracing performance this generation, so we will be interested to see if they have caught up to NVIDIA, or still heavily pay the RT tax.
For our GPU testing, we have shifted to an AMD Ryzen 9 9950X-based platform from our traditional Threadripper platform. The 9950X has fantastic all-around performance in most of our workflows and should let the video cards be the primary limiting factor where there is the possibility of a GPU bottleneck. This means the results are more comparable to our recent Intel Arc B580 review but less so to our older GPU reviews. However, at this point in our 2025 GPU testing, we have benchmarked most of the GPUs from the last few generations, so there should be plenty of comparative data for most needs. For testing, we used the latest available GPU drivers, though as we are re-using data from our previous 50-series testing in this review, we do have a few different drivers in the mix; we have not seen large performance differences between them. We tested everything on the “balanced” Windows power profile, while Resizeable BAR and “Above 4G Decoding” were enabled for every GPU as well.
In this article, our primary focus will be the new 9070 XT. As mentioned above, at this point, we have tested all of the currently-released 50-series and most of the 40- and 30-series NVIDIA cards. Unfortunately, we are more limited in our ability to test AMD cards (as we only have the 7900 XTX and 6900 XT on hand), but we have tested those as well. We think that the 7900 GRE is probably the best overall comparison for this review, but as we did not have one to test with, the most interesting comparisons will be with the closest price-competitor in the 5070, the last-gen AMD flagship 7900 XTX, and the lowest-end 16 GB NVIDIA GPU in the 5070 Ti.
Unfortunately, despite AMD’s marketing images showing what appear to be first-party GPUs akin to what we saw with the AMD 7900 XTX reference cards, AMD has declined to release a reference version of the 9070 XT and 9070. While AIBs frequently improve on first-party designs, we like having a first-party card as we feel it gives the best representative baseline performance. Nonetheless, AMD provided us with a third-party card, the Sapphire Nitro+ AMD Radeon RX 9070 XT Gaming OC. The card is, we think, comically large, given the TDP. However, the card looks good and is quiet due to the triple-slot, triple-fan design. It has a handsome forward-facing grille with a well-diffused RBG LED bar and a solid metal construction. The most unique part of it is the backplate, which is magnetic and removable; this allows access to the power and RGB cables, which are located above the fin stack about halfway down the card, under the backplate. They then route under the backplate and come out against the motherboard to create a nearly-cable free look. We were impressed.
In terms of applications, lingering compatibility problems between the RTX 50-Series and the historic lack of support for AMD’s cards in certain rendering applications have reduced the total number of workloads we could test. Due to this, we have slightly fewer results than is typical for our GPU reviews: PugetBench for Premiere Pro, After Effects, DaVinci Resolve, Lightroom Classic, and Unreal Engine. However, we are looking into ways to increase the number of benchmarks, including rendering, for GPUs.
We choose our benchmarks to cover many workflows and tasks to provide a balanced look at the application and its hardware interactions. However, many users have more specialized workflows. Recognizing this, we like to provide individual results for benchmarks as well. If a specific area in an application comprises most of your work, examining those results will give a more accurate understanding of the performance disparities between components. Otherwise, we recommend skipping over this section and focusing on our more in-depth analysis in the following sections. Also, we understand that having these provided in picture form is less than ideal, and we are working to improve that in future articles.
We build computers tailor-made for your workflow. 
Get in touch with our technical consultants today.
Starting off with Pugetbench for Lightroom Classic, the AMD Radeon RX 9070 XT performs well, scoring just behind the RTX 4080 and ahead of the 5070 Ti by 2%. For those unfamiliar with our Lightroom testing, most of it isn’t GPU-accelerated, and due to some quirks of the program, our margin of error is much higher than our other benchmarks, frequently surpassing 5%. So, while the general performance of the 9070 XT is encouraging, it’s hard to say that it is definitively better than a 5070 Ti or even 5070.
One of the primary GPU-accelerated things we currently test in Lightroom is exporting photos. Our second chart examines the speed of exporting 50 JPEGs, and the 9070 XT performs relatively poorly, beating only the 3070 Ti and 3070. We experienced some issues with the 9070 XT properly enabling GPU acceleration for this task, and even once we got it working, the export times weren’t amazing. The 5070 is 12% faster than the 9070 XT, but again, this is very likely a larger discrepancy than we expect to see in practice.
Overall, we aren’t necessarily recommending the 9070 XT for Lightroom Classic. Although there are some areas (you can view all of the sub-results in the tables above) where it seemed to perform really well, there are other areas that we know to be GPU accelerated where it didn’t. We’re looking into improving our Lightroom Classic benchmark, so hopefully, we will have a more representative set of results later this year.
Adobe Premiere Pro sees the 9070 XT landing squarely in the middle of the chart in terms of overall score (Chart #1). It is slightly behind the 5070 (by 5%) and leads the 7900 XTX by 1%—both essentially within the margin of error. As we’ll see, though, the 9070 XT performs interestingly compared to NVIDIA’s cards in terms of where that performance (or lack thereof) comes from.
In our LongGOP tests (Chart #2), the 9070 XT is the fastest GPU we tested, beating the previous-generation 7900 XTX by 4%, the 5070 Ti by 15%, and the 5070 by 26%. This is a great showing by the card. However, we have a note of caution: Premiere Pro has yet to fully support the new media engines of NVIDIA’s Blackwell. Performance on previously accelerated formats is correct, but 4:2:2 10-bit is not yet supported in a public release. Due to that, we expect NVIDIA may close the gap or even surpass the 9070 XT overall in our LongGOP tests, though with specific variants of H.264 / HEVC media favoring either card.
Intraframe codecs (Chart #3) are traditionally unaccelerated, but the pipeline used and GPU overhead can affect performance. Here, the 9070 XT allows for the fastest Intraframe speeds we have seen on a 9950X, putting it about 10% over the 5070 and just ahead of the 7900 XTX. We wouldn’t necessarily recommend buying a 9070 XT for the Intraframe speeds, but it may be a happy coincidence if you’re looking for a more budget card.
Unfortunately, that’s where the good news for AMD ends in Premiere Pro. When working with RAW media (Chart #4), the 9070 XT falls well behind the 5070 by 19% and is 29% slower than the 5070 Ti. It does still beat the other AMD cards, including the 7900 XTX by 8%, but it’s best avoided for working with these media types. Similarly, the GPU Effects scores (Chart #5) for the 9070 XT are fairly bad. The 9070 XT is 10% slower than the 7900 XTX, 26% slower than the 5070, and 36% slower than the 5070 Ti.
Overall, whether the Radeon 9070 makes sense in Premiere Pro depends heavily on what you use the application for and what media formats you primarily work with. For LongGOP and Intraframe codecs, it can be a great choice, while for RAW formats or GPU effects, less so. We would generally caution against video editors picking up a 9070 XT for Premiere Pro until we have better data on the performance of Blackwell cards when we get full support in the near future.
Our recent update to our After Effects benchmark has added testing for the emerging 3D workflows supported in the application. These tend to be more reliant on the GPU than the traditional 2D workflows, so we are interested to see how AMD compares, especially given that, historically, NVIDIA has been dominant in most 3D applications.
Unfortunately for AMD, the new 9070 XT is the slowest graphics card we have tested in our recent round of GPU testing. It falls behind not only the 5070 but also the 2080 Ti, 3070, and 6900 XT. We are not entirely sure what to make of this awful performance, but our recommendation for After Effects, for now, with the 9070 XT, is: just don’t. However, we are currently testing After Effects on a beta version as the 3D workflows have not yet made it to a live release, so this may change in the future; we hope AMD is able to work with Adobe to improve After Effects performance.
In DaVinci Resolve, the 9070 XT is fighting an uphill battle against NVIDIA’s new NVENC and NVDEC media engines, which were made faster with Blackwell while also adding in support for a few new formats of media: H.264/HEVC 4:2:2 10-bit. Previously, this support was only available with Intel Quick Sync, and even then, only for HEVC. Unfortunately, we weren’t able to do comparisons with an Intel iGPU for this testing, but we would like to look more into that in the future. Otherwise, NVIDIA has traditionally been advantaged in GPU effects and AI workloads.
In terms of the Overall score (Chart #1), the 9070 Xt performs slightly ahead of the 7900 XTX (2%), making it notably slower than the 5070 (8%) and well behind the 5070 Ti (17%). However, more so than in many of our other comparisons, the overall score does not tell the whole story. We can start with the negatives, though. In AI workloads (Chart #7), the 9070 XT is essentially unusable, being slower than only the 6900 XT. Similarly, the 9070 XT isn’t great in GPU Effects (Chart #5), trailing the cheaper 5070 by 3% and the 7900 XTX by 8%. The 5070 Ti is even further ahead.
That’s where the bad news ends. In RAW media (Chart #4) and Intraframe media (Chart #3), the 9070 XT is solidly middle-of-the-pack. In the former, this means it is about the same as a 7900 XTX or 5070 and a bit behind the 5070 Ti. Most of these are CPU-based, though, so it will depend on the exact format you are working with. The latter is entirely CPU-based, so we are unsurprised that GPU has no effect here (though weirder things have happened).
Moving on to our LongGOP tests (Chart #2), the 9070 XT does incredibly well, falling behind only the RTX 5070 Ti, RTX 5080, and RTX 5090. In particular, it is only 11% behind the 5070 Ti (and 5080) for 20% less money. It beats the 5070 and solidly outpaces the 7900 XTX. However, we encourage those interested to check out the raw results table above, as this is very test-specific. The 9070 XT is faster than the 5080 in H.264 and H.265 encoding and just under 20% slower in H.264 / HEVC processing—except for 4:2:2 10-bit, where it is half as fast due to a lack of acceleration support. Finally, in Fusion, the 9070 XT takes the top slot by a slim margin, though many of those tests are CPU-bound.
Overall, whether a 9070 XT makes sense for DaVinci Resolve relies heavily on your specific workflow and budget. In general, an RTX 5080 or 5090 will offer better performance nearly across the board, especially in AI, GPU Effects, and some forms of LongGOP media. However, in the midrange price range, the 9070 XT stacks up well against NVIDIA in certain workflow and with certain media types, so it could make sense for certain users. It also offers a cheaper route to 16 GB of VRAM, and we are excited to look at how it pairs with Intel iGPUs in the future to try and address some of the weak spots in media acceleration.
Topaz Video AI uses various AI models to upscale videos. For our testing, we run the built-in benchmark at 1080P and 4K and then take a geometric mean of the results. If you use specific models in particular, look at our results table above. Across the board, Topaz has generally favored NVIDIA with some AMD-favoring models, but it currently seems to have a performance issue with 50-series cards. There is a more recent version that we are currently testing, but for now, 40- and 50-series NVIDIA cards are functionally identical.
For AMD, the new 9070 XT performs very well. It appears as if AMD’s changes to their AI accelerators (matrix engines) are being well-utilized to improve performance in this application, and we are hopeful this will carry over into other uses. Specifically, the 9070 XT is 30% faster than the 5070 Ti and 60% faster than the 5070. Perhaps more relevantly given the weird performance issues, the 9070 XT is 29% faster than the 7900 XTX, a fantastic generational improvement putting a midrange card over the last-gen flagship. The 9070 XT still trails behind the 90-class NVIDIA cards of the last two generations but also costs half (or more!) less than then those cards. Combined with the 16 GB of VRAM, we would recommend the 9070 XT for most users of this workflow.
Our Unreal Engine benchmark combines several scenes at varying resolutions and enabled features (e.g., Ray tracing) to see how various common factors affect GPU performance. We combine those FPS results together to get composite scores. Here, we’ve included all of our standard combinations (Ray Traced and RT, both of which include testing at all three resolutions of 4K, 1140P, and 1080P, and then each resolution individually, with both RT and Rasterized scenes included).
Looking at the Overall score (Chart #1), the 9070 XT performs well, falling behind the 5070 Ti by 7% and beating the 5070 by 18%. It is also 7% ahead of the 7900 XTX, although, as we will see, this is very dependent on the type of workload it is running. Ray Tracing (Chart #2) is, perhaps unsurprisingly, given AMD’s marketing, where the 9070 XT shines. Compared to the 5070 Ti, it falls behind by 9%, and the gap over the 5070 narrows to 15%, but it manages to beat the 7900 XTX by 13%. Obviously, AMD still loses more performance than NVIDIA in Ray Tracing and has a lower ceiling of performance, but this is a great generational uplift. 
In contrast, AMD’s Rasterized performance (Chart #3) is more competitive, but less improved. In our Rasterized tests, the 9070 XT ties the 5070 Ti and the 7900 XTX, leading the 5070 by 27%. AMD has been competitive in rasterized performance for a few generations, but it is good to see the 9070 XT managing to catch up to the last-gen flagship card. 
We won’t analyze each of the resolution tests individually, but in general, we see that AMD tends to perform a bit better relative to NVIDIA at lower resolutions, with the performance gap increasing as we go to 1440P and then 4K. Nonetheless, the difference isn’t huge (4.7% vs. 7.2% vs. 8.4%), and it doesn’t change the relative rankings of the cards.
In summary, the 9070 Xt is an impressive card that is highly competitive in its price segment. It offers notably better performance across the board than the 5070 (along with more VRAM) and falls just behind the 5070 Ti despite costing $150 less. While it doesn’t offer a real upgrade for those on the last-gen 7900 XTX, lower-end 7000-series users may find it compelling. We are particularly pleased with the improvements to Ray Tracing AMD has managed this generation.
GPU RENDERING – The AMD Radeon RX 9070 XT is currently supported in Blender version 4.4. As all of our current GPU testing was performed on version 4.0, we were not able to include results here. However, we hope to re-test all of our GPUs with version 4.4 in the near future as part of a GPU-roundup. In the meantime, you can check out Blender’s open database to see how it compares against some other GPUs on version 4.4.
The AMD Radeon RX 9070 XT is, like most AMD GPUs, difficult to analyze for professional content creation use. In many areas, it performs well compared to its NVIDIA competition but has severe compatibility or performance issues in others. For $600 (or at least, $150 less than a 5070 Ti) it seems to generally make sense for a lot of workflows, and offers the cheapest way to get 16 GB of VRAM in a midrange card.
In video editing and motion graphics, the 9070 XT is a very mixed bag. It tends to fall behind the cheaper 5070 in heavy AI, GPU Effects, or RAW codec tests while staying very competitive with the 5070 Ti in LongGOP, Intraframe, and Resolve’s Fusion workflows. However, the exact codec is important, as NVIDIA and AMD trade blows over which LongGOP (Interframe) codec variants are accelerated and how competent that acceleration is. Unfortunately, in After Effects, the 9070 XT is largely unusable in the workloads where the GPU matters.
In rendering, AMD is, again, a mixed bag. For our real-time rendering tests in Unreal Engine, the 9070 XT was very good, falling 7% behind the 25% more expensive 5070 Ti and leading the slightly cheaper 5070 by 18%. We were particularly impressed with AMD’s improvements in Ray Tracing this generation, with the 9070 XT nearly maintaining parity with NVIDIA. NVIDIA is still better for RT but by far less than in the past. However, in our offline renderers… well, we couldn’t test any. AMD historically has far worse support in applications like Octane (unsupported), V-Ray (unsupported), Redshift (only a few newer cards are supported), and Blender (fully supported). The current Cinebench 2024 release is not compatible with the 9070 XT, and all of our testing in Blender was on version 4.0, which does not support the cards. Blender does maintain an open benchmark results database we looked at, but the results are not encouraging for the 9070 XT..
In summary, if you are interested in the 9070 XT as either a cheaper alternative to NVIDIA’s 5070 Ti or due to availability concerns, we think it can potentially be a compelling graphics card for content creation. However, make sure that you closely examine your workflow for not just which applications you use, but which parts of those applications and what type of media you most frequently work with. In many ways, choosing between a Radeon and GeForce card is much like choosing between an Intel or AMD CPU—both offer great performance, but only in certain workflows in certain applications. We are genuinely excited about these new Radeon cards and what they promise for the future competition in the GPU space for content creation.
If you need a powerful workstation to tackle the applications we’ve tested, the Puget Systems workstations on our solutions page are tailored to excel in various software packages. If you prefer to take a more hands-on approach, our custom configuration page helps you to configure a workstation that matches your exact needs. Otherwise, if you would like more guidance in configuring a workstation that aligns with your unique workflow, our knowledgeable technology consultants are here to lend their expertise.
We build computers tailor-made for your workflow. 
Get in touch with one of our technical consultants today.
Puget Systems builds custom workstations, servers and storage solutions tailored for your work.
Extensive performance testingmaking you more productive and giving better value for your money
Reliable computerswith fewer crashes means more time working & less time waiting
Support that understandsyour complex workflows and can get you back up & running ASAP
A proven track recordas shown by our case studies and customer testimonials
© Copyright 2025 - Puget Systems, All Rights Reserved."
"AMD","RX 9070 XT","RTX 5070 Ti vs RX 9070 XT: Which GPU is right for you?","https://www.techradar.com/computing/gpu/nvidia-rtx-5070-ti-vs-amd-rx-9070-xt-which-gpu-is-right-for-you","The most powerful midrange cards compared

When you purchase through links on our site, we may earn an affiliate commission. Here’s how it works.

Compute Units: 70Shaders: 8,960Ray processors: 70AI/Tensor processors: 280Boost clock: 2,452 MHzMemory type: GDDR7Memory pool: 16 GBMemory speed (effective): 28 GbpsMemory bandwidth: 896 GB/sBus interface: 256-bitTGP: 300WPower connector: 1 x 16-pinSlot width: Dual
Compute Units: 64Shaders: 4,096Ray processors: 64AI/Tensor processors: 128Boost clock: 2,970 MHzMemory type: GDDR6Memory pool: 16 GBMemory speed (effective): 20 GbpsMemory bandwidth: 640 GB/sBus interface: 256-bitTGP: 304WPower connector: 2x 8-pinSlot width: Dual
If you're after a little more power to push 1440p (and even 4K) gaming in your machine in 2025, then you're going to want to seriously consider the recently released Nvidia RTX 5070 Ti and the AMD RX 9070 XT. Both midrange GPUs are more capable than the likes of the respective RTX 5070 and RX 9070, meaning they can be ideal for more hardcore gamers, but which one is the best?
That's why we're comparing the RTX 5070 Ti vs RX 9070 XT in terms of their price, specs, and performance so you can buy with confidence, safe in the knowledge that you've got the perfect GPU for your setup. There are a wealth of need-to-know differences, including the drastically opposed architecture for how the hardware play (and interact with) the software you want to use.
As for how the RTX 5070 Ti and RX 9070 XT stack up against the best graphics cards, it ultimately all comes down to how the pricing stacks up against the performance on offer. In an ideal world, we would all have the best 4K graphics card that money can buy in our machines, but that's simply not realistic or viable for the vast majority of PC gamers, hence why the midrange remains a popular choice in 2025.
The Nvidia RTX 5070 Ti was released on February 20, 2025, with a starting price of $749. Unlike other graphics cards in the Blackwell lineup, there is no Founders Edition (Nvidia-made) option, meaning it's solely up to the manufacturer's partners (such as ASRock, Asus, MSI, PNY, Prime and more) to set the recommended retail price for the hardware.
Additionally, varying models, such as those that ship overclocked by default or feature larger cooling solutions, can be far more expensive, eclipsing the rate proposed by Team Green.
In contrast, the AMD RX 9070 XT debuted on March 6, 2025, with a starting price of $599, which is more comparable to the RTX 5070 (and RTX 4070) than the higher-end 70-class card from Nvidia. AMD particularly highlighted its best-in-class performance at its unveiling, considering its ""under $600"" price tag.
Similarly to the RTX 5070 Ti, there is no AMD-made reference card, so it's again strictly up to the company's partners to set the prices and maintain them.
With this considered, the AMD RX 9070 XT is the cheaper of the two by quite a considerable margin, with a full $150 cheaper price point considering the two respective MSRPs. However, the actual gap between them could be narrower when factoring in the different manufacturers' prices, depending on the feature sets, which is something to be aware of when weighing the two up.
Before we go into the myriad of technical differences between the RTX 5070 Ti vs RX 9070 XT, it's important to outline how the two graphics cards are similar. At a base level, both Nvidia's and AMD's GPUs are PCIe 5.0-compliant GPUs featuring 16GB VRAM built on a 256-bit memory bus with a roughly identical TDP.
That's about where the similarities end, however, as it's clear that Blackwell and RDNA 4 were made with very different design philosophies in mind. The RTX 5070 Ti features far faster (and denser) GDDR7 video memory compared to the older (and far slower) GDDR6 memory of the RX 9070 XT. Curiously, AMD did not see the benefits of utilzing the faster GDDR6X memory instead, which Nvidia's previous-generation graphics cards (RTX 40 series) had been built on since 2022.
Despite both being made to be PCIe 5.0 compliant, the GDDR7 VRAM of the RTX 5070 Ti has allowed for much faster effective speed of 28 Gbps (a staggering 40% increase over the RX 9070 XT's 20 Gbps). This is equally reflected in the total bandwidth available as Nvidia's card pushes up to just under the 900 GB/s mark, whereas AMD's video card can't reach the 650 GB/s range.
The largest difference between the RTX 5070 Ti's specs when compared with the RX 9070 XT is the sweeping difference in shaders and AI cores. For both, there's just shy of a 120% increase when contrasted against the RX 9070 XT, showing just how much Nvidia has backed artificial intelligence for its latest line of video cards. There's no contest when comparing the hardware in terms of raw numbers.
While there's quite a wild disparity in the price and specs separating the RTX 5070 Ti from the AMD RX 9070 XT, real-world performance testing shows their true potential. Regarding synthetic benchmarks, Nvidia's graphics card does pull ahead, but there's not quite the gap that you may expect given the lower price and (seemingly) weaker hardware under the hood.
The differences in the GeekBench 6 Compute (OpenGL) benchmark show how close these two cards can be in practice. The RTX 5070 Ti scored 243,483, with the RX 9070 XT achieving 223,065 (a difference of 9.1%). Some of the 3DMark benchmarks demonstrate a narrower gap, such as can be observed with Night Raid. The former tallied 252,604 compared to the latter's 250,811 (less than 1% difference). It's a similar story with Steel Nomad and Time Spy as well.
Gaming is where both the RTX 5070 Ti and RX 9070 XT get to shine to flex the 16GB memory pool. While 4K is (arguably) not their strong suit, both GPUs are more than capable of delivering playable frame rates in today's demanding titles.
Black Myth: Wukong is playable above 60fps on both video cards, with 69fps and 63fps averages. Cyberpunk 2077 in Ultra settings is similarly close at above 90fps from the two, with averages of 96 and 93, respectively. Certain games do benefit from the GDDR7 memory pool, however, as Returnal with Epic settings and ray tracing enabled is no problem for the RTX 5070 Ti with 67fps, but the RX 9070 XT couldn't quite achieve 4K60.
1440p gaming is where we start to see the RTX 5070 Ti and RX 9070 XT excelling. Black Myth: Wukong's average frame rates jump up to 86 and 83, respectively, with Cyberpunk 2077 in Ultra settings playable at (or above) the 180fps mark on both graphics cards. Dying Light 2 set to High Quality pushes way above 200fps with a 10-frame lead in Nvidia's favor, for as much as that matters. Additionally, there's just a single frame separating the two at 144 and 143fps apiece, with the RX 9070 XT coming out ahead here.
Creative workloads see the RTX 5070 Ti edging ahead of the RX 9070 XT, however, it's largely situational to the software as to how big of a lead there is. For example, the former scored 11,318 in PugetBench for Adobe Photoshop, whereas the latter achieved 10,936 (a difference of 3.4%). Our Handbrake 1.6 4K to 1080p H.264 encoding test saw the gap narrowed even more with respective average frame rates of 218 and 213 (a difference of 2.3%). While things are very close, Nvidia's GPU edges ahead ever so slightly in all major categories.
Based on everything we've gone over when comparing the RTX 5070 Ti vs RX 9070 XT, the answer of which is best will ultimately come down to your use needs. Solely for gaming, you're better off investing in the far cheaper (and likely much more available) AMD graphics card and benefitting from the 16GB memory pool for the same price as the 12GB RTX 5070.
However, if you're primarily wanting a more affordable GPU for creativity and productivity tasks as well as gaming then the Nvidia RTX 5070 Ti has a little more under the hood to deliver a higher-end experience across the board. Whether that's worth the extra $150 (potentially more) is up to you.
Despite having GDDR7 VRAM onboard, it doesn't seem like it's made too much of a substantial difference when gaming in either 1440p or 4K. It will (likely) be a good investment for the future of PC games, but it isn't something that's been utilized today. Then we get onto the fact that Blackwell stock is hard to come by in May 2025 whereas AMD is promising ""wide availability"" for its RDNA 4 hardware. If you want the best value for money then we recommend the 9070 XT, but if you're wanting to push things a little further, then you should go for the RTX 5070 Ti.
Sign up for breaking news, reviews, opinion, top tech deals, and more.
Formerly TechRadar Gaming's Hardware Editor, Aleksha McLoughlin is now a freelance writer and editor specializing in computing tech, video games, and E-commerce. As well as her many contributions to this site, you'll also find her work available on sister sites such as PC Gamer, GamesRadar, and Android Central. Additionally, more of her bylines can be found on Trusted Reviews, Dexerto, Expert Reviews, Techopedia, PC Guide, VideoGamer, and more.
Please logout and then login again, you will then be prompted to enter your display name.

TechRadar is part of Future US Inc, an international media group and leading digital publisher. Visit our corporate site.

©
Future US, Inc. Full 7th Floor, 130 West 42nd Street,
New York,
NY 10036."
"AMD","RX 9070 XT","AMD Radeon RX 9070 XT release date, price, and specs","https://www.pcgamesn.com/amd/radeon-rx-9070-xt-guide","The new flagship RDNA 4 AMD gaming GPU has now been officially unveiled, with AMD revealing the specs and release date, plus FSR 4 details.


                    Ben Hardwidge                

AMD has finally lifted the lid on the fastest new gaming GPU in its RDNA 4 lineup, with a surprisingly competitive price, and incredible performance. In our own tests, we found that the new AMD Radeon RX 9070 XT is significantly more powerful than the Nvidia GeForce RTX 5070, and its new FSR 4 upscaling tech looks great too.

You can head over to our full AMD Radeon RX 9070 XT review for all the full details on this mid-range AMD GPU's surprisingly potent performance for its price, landing it a place on our guide to the best graphics card. AMD has historically always lagged behind Nvidia when it comes to ray tracing, but it's made up a lot of ground here, including now having full path tracing support.

The AMD Radeon RX 9070 XT release date is Thursday, March 6, 2025, coming just a day after the launch of the Nvidia GeForce RTX 5070, and shortly after the RTX 5070 Ti launch as well. AMD promises ""wide availability"" on this date, and there will be no AMD-made reference model, with cards instead coming from several third-party manufacturers, including Acer, Asus, ASRock, Gigabyte, PowerColor, XFX, and Sapphire. 

The AMD Radeon RX 9070 XT price is $599, which is seriously competitive for the performance on offer. That's just $50 more than the 12GB RTX 5070, and $150 less than the RTX 5070 Ti.


The AMD Radeon RX 9070 XT specs include 16GB of VRAM, 64 compute units, and 128 AI cores.


AMD has made several key improvements to its core GPU architecture with RDNA 4. This includes a new compute unit design, which can run at higher clock speeds than before, and which AMD also says is more efficient than its predecessor. There are 64 compute units in the 9070 XT, which is only a little more than the 60 found in the Radeon RX 7800 XT and much lower than the 80 in the Radeon RX 7900 GRE. However, while the 7800 XT clock speed only boosts to 2,430MHz, the 9070 XT can boost all the way to 2.97GHz.


That new compute unit also contains a new ray tracing core design with RDNA 4, which AMD claims doubles the ray tracing throughput per compute unit compared to RDNA 3. The new GPUs are now capable of running path tracing in games as well, which was a real struggle for the company's last-gen GPUs.


In our tests, even with frame gen and FSR upscaling enabled, the Radeon RX 7800 XT can only average 46fps with path tracing at 1440p in Cyberpunk 2077, for example, compared with 93fps for the RTX 4070 Ti and 72fps for the 4070. However, the 9070 XT averages a much healthier 92fps in this game, as we found in our own tests.


Meanwhile, new demanding games such as Indiana Jones and the Great Circle don't even run path tracing on AMD's RDNA 3 GPUs, but these settings are now available on the new GPU. There are 64 of AMD's new RDNA 4 ray accelerator cores in the Radeon RX 9070 XT, compared to 60 in the Radeon RX 7800 XT and 80 in the Radeon RX 7900 GRE.


Another key part of the spec is the 16GB of VRAM, which is the same amount found in the Radeon RX 7800 XT, and gives this card and its smaller sibling an edge over the 12GB GeForce RTX 5070. The latter might have Multi Frame Gen at its disposal, but that's no use in situations where the GPU doesn't have enough VRAM to render the original frames smoothly in the first place. In our tests, for example, we've found that 12GB is a serious bottleneck on a GPU if you max out the path tracing in a demanding game such as Indiana Jones and the Great Circle.


AMD also claims to have made strides in its AI performance with this new GPU architecture, thanks to a new matrix core design (like Nvidia's Tensor cores). The company cites a peak AI performance figure of 1,557 TOPS, compared to a claimed 1,406 TOPS for the Nvidia GeForce RTX 5070 Ti.


Until now, AMD hasn't really used its matrix cores in games, but the introduction of FSR 4 sees the company finally using machine learning and AI hardware for resolution upscaling, and it looked great when we had a chance to try it for our FSR 4 test at CES.


AMD says that FSR 4 will be supported in 30 games at launch, including Kingdom Come Deliverance 2, Marvel Rivals, Call of Duty Black Ops 6, and Space Marine 2, with over 75 more games supporting the new tech later in 2025.


AMD says that its new AI cores are also ""neural rendering ready"" and cites neural radiance cache as an example – a tech that looked really promising when we tested neural rendering in Half-Life 2 RTX at CES.


If you want to see what the 9070 XT is up against, you can find out all about the 9070 XT's nearest competition by reading our RTX 5070 review. You can also follow us on Google News for daily PC hardware news, reviews, and guides, or join our community Discord to stay in the know.





                    Ben Hardwidge                 A tech journalist since 1999, and a PC hardware enthusiast since 1989, Ben has seen it all, from the horrors of CGA graphics to the awesome power of the RTX 5090 today. Ben is mainly interested in the latest CPU and graphics tech, and currently spends most of his evenings playing Oblivion Remastered, while marveling at how much better it looks than the original game did on his old GeForce 6600 GT."
"AMD","RX 9070 XT","AMD Radeon RX 9070 XT: Everything We Know, Specs, Performance, Price, and Release Date","https://www.xtremegaminerd.com/amd-radeon-rx-9070-xt/","It’s almost time for AMD’s next-gen graphics cards to hit the market and while we don’t have much information on these cards officially, plenty of leaks have revealed crucial information about these GPUs. From architecture to performance and possible pricing, there are various rumors and discussions going around the AMD RDNA 4 GPU lineup, which we will be covering in this article in detail.
AMD is apparently going with the RX 9000 naming convention and is skipping the RX 8000 naming for the upcoming generation. That said, this doesn’t affect the specs or performance of these GPUs but clears the confusion about the GPU model names for easier reference. The earliest leaks have pointed towards the possible flagship GPU in the lineup, known as the Radeon RX 9070 XT, and here is everything from specs, performance, price, and release date you should know about it.AMD RDNA 4 Architecture and Radeon RX 9070 XTThe next-gen AMD RX 9000 GPUs will be powered by the RDNA 4 architecture, the successor to the RDNA 3 and RDNA 3.5 powering the RX 7000 as well as Zen 5 integrated graphics respectively. AMD’s RDNA 4 architecture is reportedly utilizing TSMC’s N4P process node to offer better performance and efficiency. Unlike RDNA 3, the RDNA 4 will utilize a monolithic die structure instead of the chiplet design. This change should bring enhancements in both performance and latency.Related ArticlesWhen to Change Your Graphics Card? Know the Right TimeApril 1, 2023Radeon RX 9070 XT is Allegedly Close to RTX 4080 in Gaming Performance, Can Consume 70W Higher on Custom EditionsDecember 26, 2024While there isn’t much about the RDNA 4 from AMD itself, various reports suggest that RDNA 4 will be incredibly powerful in both rasterization and ray tracing. As per AMD’s CEO Lisa Su,In addition to a strong increase in gaming performance, RDNA 4 delivers significantly higher ray tracing performance and adds new AI capabilities,–PC WorldThat said, even though RDNA 4 is more powerful than RDNA 3, AMD isn’t expected to push its potential to the max since it will only deliver two GPU dies for the RX 9000 lineup. These are the Navi 48 and Navi 44, the former being more powerful than the latter, which is unlike what AMD followed with the naming of the GPU die in the previous generation.The upcoming Radeon RX 9070 XT is expected to be based on the Navi 48 die and will utilize the full GPU, which has an area of 240 mm2. As per some reports, the RX 9070 XT is going to be the flagship GPU and is aimed at delivering performance equivalent to upper mid-range GPUs from its rival, NVIDIA.SpecificationsThe Radeon RX 9070 XT is supposedly the most powerful graphics card based on the RDNA 4 architecture. It’s not yet known whether AMD has any plans to release any GPU faster than the RX 9070 XT but at the moment, we didn’t hear about any other faster die than the Navi 48. Considering this in mind, one shouldn’t expect the RX 9070 XT to go against NVIDIA’s upcoming RTX 50 series high-end cards or even the Ada high-end GPUs such as the RTX 4090.The Radeon RX 9070 XT is reported to feature a 256-bit memory bus and 16GB GDDR6 VRAM from Micron. Unlike NVIDIA, AMD isn’t going to equip the RDNA 4 discrete GPUs with the latest GDDR7 VRAM and hence, the total memory bandwidth will be much lower. Still, as the memory speed is said to operate at 20 Gbps, the total memory bandwidth will be up to 693 GB/s, which is noticeably better than its predecessors.The Navi 48 die will bring up to 64 Compute Units and this means 4096 Stream Processors on the RX 9070 XT. The clock speeds are rumored to hit up to 3.1 GHz and the default TDP rating will be 260W. This is for the reference edition but the custom AIB cards can reach higher frequencies and can consume up to 330W. The specs are subject to change since nothing is official but based on the leaks, most of them will be quite close to what we have stated here.Specs Summary:Architecture: RDNA 4GPU: Navi 48SPs: 4096RT Cores: 64Base Clock: 2.5 GHzBoost Clock: 3.1 GHzInterface: PCI-E 5.0VRAM: 16GB GDDR6, 256-bit, 20 GbpsMemory Bandwidth: 640 GB/sTexture Fillrate: 783.4 GTexels/sPixel Fillrate: 195.8 GPixel/sTDP: 260W (Reference), Up to 330W (Custom)PerformanceEarlier it was believed that the RX 9070 XT would go against the RTX 4080 GPU but the recent benchmarks show that it is slightly above the Radeon RX 7900 GRE from the previous lineup. In simple words, it’s a successor to the RX 7800 XT or the RX 7900 GRE and this will be decided according to the price.In another synthetic benchmark (Time Spy), the RX 9070 XT was barely 2% faster than the RX 7900 GRE and was noticeably slower than the RX 7900 XT. It scored 22,894 points while the RX 7900 XT delivered 27,044 points, which puts the RX 9070 XT around 14% slower than the RX 7900 XT. This is a much lower performance number than the RTX 4080, which is around 4% faster than the 7900 XT in Time Spy.In another recently leaked benchmark, the Radeon RX 9070 XT came out on par with the RX 7900 XTX and RTX 4070 Ti Super. This was the 3DMark Speed Way test, where it delivered 6345 points. In another test called 3DMark Time Spy Extreme, the RX 9070 XT delivered 14448 points, which was even higher than the RTX 4080 and 4080 Super. It should be kept in mind that these are subject to change as the GPU currently doesn’t boast the latest drivers.The gaming benchmarks aren’t yet here and the 3DMark synthetic scores may not accurately translate to such performance in games but it’s unlikely that the RX 9070 XT is going to surpass the RTX 4080 or RTX 4080 Super. However, according to one report, the RX 9070 XT is within 5% of the RTX 4080 performance in select titles. This is still not confirmed due to the absence of evidence but could be possible as well.Price and Release DateThe official MSRP of the Radeon RX 9070 XT isn’t yet available publicly but as per some rumors, it is expected that the Navi 48-based Radeon RX 9070 XT will cost $479 but the AIB editions will start at $500+. This was seen in recent retail listings where the AIB editions were over $549. Once again, this is just a rumor and nothing has been confirmed yet.The Radeon RX 9070 XT is expected to be revealed at CES 2025, and at the same time, AMD is going to reveal the FSR 4 and Ryzen 9000X3D processors. This report has been confirmed by a reliable leaker and since most hardware vendors including NVIDIA, are launching their cutting-edge hardware at CES, expect AMD to launch its RDNA 4 discrete GPU lineup in the early January CES event.AMD Radeon RX 9070 XT: What to Expect?Based on the available information on the specs, performance, and pricing of the Radeon RX 9070 XT, one shouldn’t have high expectations from the card. Even though the architecture has been improved, the pricing remains a challenging part for AMD to succeed against NVIDIA. If RX 9070 XT goes live at $599 or $649 while its performance is barely higher than RX 7900 GRE, it is DOA.If the price remains $499 at max, it may seem like a satisfactory option but since Radeon RX 7800 XT and 7900 GRE are now available for lower than their launch MSRP, it could be hard to convince the consumers to buy the RX 9070 XT. On the flip side, if the performance comes closer to RTX 4080, the $649 price tag will be appealing. Nonetheless, not much has been improved in its specs compared to its predecessors and it needs to be priced competitively to gain an advantage over NVIDIA.
The next-gen AMD RX 9000 GPUs will be powered by the RDNA 4 architecture, the successor to the RDNA 3 and RDNA 3.5 powering the RX 7000 as well as Zen 5 integrated graphics respectively. AMD’s RDNA 4 architecture is reportedly utilizing TSMC’s N4P process node to offer better performance and efficiency. Unlike RDNA 3, the RDNA 4 will utilize a monolithic die structure instead of the chiplet design. This change should bring enhancements in both performance and latency.
While there isn’t much about the RDNA 4 from AMD itself, various reports suggest that RDNA 4 will be incredibly powerful in both rasterization and ray tracing. As per AMD’s CEO Lisa Su,In addition to a strong increase in gaming performance, RDNA 4 delivers significantly higher ray tracing performance and adds new AI capabilities,–PC WorldThat said, even though RDNA 4 is more powerful than RDNA 3, AMD isn’t expected to push its potential to the max since it will only deliver two GPU dies for the RX 9000 lineup. These are the Navi 48 and Navi 44, the former being more powerful than the latter, which is unlike what AMD followed with the naming of the GPU die in the previous generation.The upcoming Radeon RX 9070 XT is expected to be based on the Navi 48 die and will utilize the full GPU, which has an area of 240 mm2. As per some reports, the RX 9070 XT is going to be the flagship GPU and is aimed at delivering performance equivalent to upper mid-range GPUs from its rival, NVIDIA.SpecificationsThe Radeon RX 9070 XT is supposedly the most powerful graphics card based on the RDNA 4 architecture. It’s not yet known whether AMD has any plans to release any GPU faster than the RX 9070 XT but at the moment, we didn’t hear about any other faster die than the Navi 48. Considering this in mind, one shouldn’t expect the RX 9070 XT to go against NVIDIA’s upcoming RTX 50 series high-end cards or even the Ada high-end GPUs such as the RTX 4090.The Radeon RX 9070 XT is reported to feature a 256-bit memory bus and 16GB GDDR6 VRAM from Micron. Unlike NVIDIA, AMD isn’t going to equip the RDNA 4 discrete GPUs with the latest GDDR7 VRAM and hence, the total memory bandwidth will be much lower. Still, as the memory speed is said to operate at 20 Gbps, the total memory bandwidth will be up to 693 GB/s, which is noticeably better than its predecessors.The Navi 48 die will bring up to 64 Compute Units and this means 4096 Stream Processors on the RX 9070 XT. The clock speeds are rumored to hit up to 3.1 GHz and the default TDP rating will be 260W. This is for the reference edition but the custom AIB cards can reach higher frequencies and can consume up to 330W. The specs are subject to change since nothing is official but based on the leaks, most of them will be quite close to what we have stated here.Specs Summary:Architecture: RDNA 4GPU: Navi 48SPs: 4096RT Cores: 64Base Clock: 2.5 GHzBoost Clock: 3.1 GHzInterface: PCI-E 5.0VRAM: 16GB GDDR6, 256-bit, 20 GbpsMemory Bandwidth: 640 GB/sTexture Fillrate: 783.4 GTexels/sPixel Fillrate: 195.8 GPixel/sTDP: 260W (Reference), Up to 330W (Custom)PerformanceEarlier it was believed that the RX 9070 XT would go against the RTX 4080 GPU but the recent benchmarks show that it is slightly above the Radeon RX 7900 GRE from the previous lineup. In simple words, it’s a successor to the RX 7800 XT or the RX 7900 GRE and this will be decided according to the price.In another synthetic benchmark (Time Spy), the RX 9070 XT was barely 2% faster than the RX 7900 GRE and was noticeably slower than the RX 7900 XT. It scored 22,894 points while the RX 7900 XT delivered 27,044 points, which puts the RX 9070 XT around 14% slower than the RX 7900 XT. This is a much lower performance number than the RTX 4080, which is around 4% faster than the 7900 XT in Time Spy.In another recently leaked benchmark, the Radeon RX 9070 XT came out on par with the RX 7900 XTX and RTX 4070 Ti Super. This was the 3DMark Speed Way test, where it delivered 6345 points. In another test called 3DMark Time Spy Extreme, the RX 9070 XT delivered 14448 points, which was even higher than the RTX 4080 and 4080 Super. It should be kept in mind that these are subject to change as the GPU currently doesn’t boast the latest drivers.The gaming benchmarks aren’t yet here and the 3DMark synthetic scores may not accurately translate to such performance in games but it’s unlikely that the RX 9070 XT is going to surpass the RTX 4080 or RTX 4080 Super. However, according to one report, the RX 9070 XT is within 5% of the RTX 4080 performance in select titles. This is still not confirmed due to the absence of evidence but could be possible as well.Price and Release DateThe official MSRP of the Radeon RX 9070 XT isn’t yet available publicly but as per some rumors, it is expected that the Navi 48-based Radeon RX 9070 XT will cost $479 but the AIB editions will start at $500+. This was seen in recent retail listings where the AIB editions were over $549. Once again, this is just a rumor and nothing has been confirmed yet.The Radeon RX 9070 XT is expected to be revealed at CES 2025, and at the same time, AMD is going to reveal the FSR 4 and Ryzen 9000X3D processors. This report has been confirmed by a reliable leaker and since most hardware vendors including NVIDIA, are launching their cutting-edge hardware at CES, expect AMD to launch its RDNA 4 discrete GPU lineup in the early January CES event.AMD Radeon RX 9070 XT: What to Expect?Based on the available information on the specs, performance, and pricing of the Radeon RX 9070 XT, one shouldn’t have high expectations from the card. Even though the architecture has been improved, the pricing remains a challenging part for AMD to succeed against NVIDIA. If RX 9070 XT goes live at $599 or $649 while its performance is barely higher than RX 7900 GRE, it is DOA.If the price remains $499 at max, it may seem like a satisfactory option but since Radeon RX 7800 XT and 7900 GRE are now available for lower than their launch MSRP, it could be hard to convince the consumers to buy the RX 9070 XT. On the flip side, if the performance comes closer to RTX 4080, the $649 price tag will be appealing. Nonetheless, not much has been improved in its specs compared to its predecessors and it needs to be priced competitively to gain an advantage over NVIDIA.
In addition to a strong increase in gaming performance, RDNA 4 delivers significantly higher ray tracing performance and adds new AI capabilities,–PC World
That said, even though RDNA 4 is more powerful than RDNA 3, AMD isn’t expected to push its potential to the max since it will only deliver two GPU dies for the RX 9000 lineup. These are the Navi 48 and Navi 44, the former being more powerful than the latter, which is unlike what AMD followed with the naming of the GPU die in the previous generation.
The upcoming Radeon RX 9070 XT is expected to be based on the Navi 48 die and will utilize the full GPU, which has an area of 240 mm2. As per some reports, the RX 9070 XT is going to be the flagship GPU and is aimed at delivering performance equivalent to upper mid-range GPUs from its rival, NVIDIA.SpecificationsThe Radeon RX 9070 XT is supposedly the most powerful graphics card based on the RDNA 4 architecture. It’s not yet known whether AMD has any plans to release any GPU faster than the RX 9070 XT but at the moment, we didn’t hear about any other faster die than the Navi 48. Considering this in mind, one shouldn’t expect the RX 9070 XT to go against NVIDIA’s upcoming RTX 50 series high-end cards or even the Ada high-end GPUs such as the RTX 4090.The Radeon RX 9070 XT is reported to feature a 256-bit memory bus and 16GB GDDR6 VRAM from Micron. Unlike NVIDIA, AMD isn’t going to equip the RDNA 4 discrete GPUs with the latest GDDR7 VRAM and hence, the total memory bandwidth will be much lower. Still, as the memory speed is said to operate at 20 Gbps, the total memory bandwidth will be up to 693 GB/s, which is noticeably better than its predecessors.The Navi 48 die will bring up to 64 Compute Units and this means 4096 Stream Processors on the RX 9070 XT. The clock speeds are rumored to hit up to 3.1 GHz and the default TDP rating will be 260W. This is for the reference edition but the custom AIB cards can reach higher frequencies and can consume up to 330W. The specs are subject to change since nothing is official but based on the leaks, most of them will be quite close to what we have stated here.Specs Summary:Architecture: RDNA 4GPU: Navi 48SPs: 4096RT Cores: 64Base Clock: 2.5 GHzBoost Clock: 3.1 GHzInterface: PCI-E 5.0VRAM: 16GB GDDR6, 256-bit, 20 GbpsMemory Bandwidth: 640 GB/sTexture Fillrate: 783.4 GTexels/sPixel Fillrate: 195.8 GPixel/sTDP: 260W (Reference), Up to 330W (Custom)PerformanceEarlier it was believed that the RX 9070 XT would go against the RTX 4080 GPU but the recent benchmarks show that it is slightly above the Radeon RX 7900 GRE from the previous lineup. In simple words, it’s a successor to the RX 7800 XT or the RX 7900 GRE and this will be decided according to the price.In another synthetic benchmark (Time Spy), the RX 9070 XT was barely 2% faster than the RX 7900 GRE and was noticeably slower than the RX 7900 XT. It scored 22,894 points while the RX 7900 XT delivered 27,044 points, which puts the RX 9070 XT around 14% slower than the RX 7900 XT. This is a much lower performance number than the RTX 4080, which is around 4% faster than the 7900 XT in Time Spy.In another recently leaked benchmark, the Radeon RX 9070 XT came out on par with the RX 7900 XTX and RTX 4070 Ti Super. This was the 3DMark Speed Way test, where it delivered 6345 points. In another test called 3DMark Time Spy Extreme, the RX 9070 XT delivered 14448 points, which was even higher than the RTX 4080 and 4080 Super. It should be kept in mind that these are subject to change as the GPU currently doesn’t boast the latest drivers.The gaming benchmarks aren’t yet here and the 3DMark synthetic scores may not accurately translate to such performance in games but it’s unlikely that the RX 9070 XT is going to surpass the RTX 4080 or RTX 4080 Super. However, according to one report, the RX 9070 XT is within 5% of the RTX 4080 performance in select titles. This is still not confirmed due to the absence of evidence but could be possible as well.Price and Release DateThe official MSRP of the Radeon RX 9070 XT isn’t yet available publicly but as per some rumors, it is expected that the Navi 48-based Radeon RX 9070 XT will cost $479 but the AIB editions will start at $500+. This was seen in recent retail listings where the AIB editions were over $549. Once again, this is just a rumor and nothing has been confirmed yet.The Radeon RX 9070 XT is expected to be revealed at CES 2025, and at the same time, AMD is going to reveal the FSR 4 and Ryzen 9000X3D processors. This report has been confirmed by a reliable leaker and since most hardware vendors including NVIDIA, are launching their cutting-edge hardware at CES, expect AMD to launch its RDNA 4 discrete GPU lineup in the early January CES event.AMD Radeon RX 9070 XT: What to Expect?Based on the available information on the specs, performance, and pricing of the Radeon RX 9070 XT, one shouldn’t have high expectations from the card. Even though the architecture has been improved, the pricing remains a challenging part for AMD to succeed against NVIDIA. If RX 9070 XT goes live at $599 or $649 while its performance is barely higher than RX 7900 GRE, it is DOA.If the price remains $499 at max, it may seem like a satisfactory option but since Radeon RX 7800 XT and 7900 GRE are now available for lower than their launch MSRP, it could be hard to convince the consumers to buy the RX 9070 XT. On the flip side, if the performance comes closer to RTX 4080, the $649 price tag will be appealing. Nonetheless, not much has been improved in its specs compared to its predecessors and it needs to be priced competitively to gain an advantage over NVIDIA.
The Radeon RX 9070 XT is supposedly the most powerful graphics card based on the RDNA 4 architecture. It’s not yet known whether AMD has any plans to release any GPU faster than the RX 9070 XT but at the moment, we didn’t hear about any other faster die than the Navi 48. Considering this in mind, one shouldn’t expect the RX 9070 XT to go against NVIDIA’s upcoming RTX 50 series high-end cards or even the Ada high-end GPUs such as the RTX 4090.
The Radeon RX 9070 XT is reported to feature a 256-bit memory bus and 16GB GDDR6 VRAM from Micron. Unlike NVIDIA, AMD isn’t going to equip the RDNA 4 discrete GPUs with the latest GDDR7 VRAM and hence, the total memory bandwidth will be much lower. Still, as the memory speed is said to operate at 20 Gbps, the total memory bandwidth will be up to 693 GB/s, which is noticeably better than its predecessors.The Navi 48 die will bring up to 64 Compute Units and this means 4096 Stream Processors on the RX 9070 XT. The clock speeds are rumored to hit up to 3.1 GHz and the default TDP rating will be 260W. This is for the reference edition but the custom AIB cards can reach higher frequencies and can consume up to 330W. The specs are subject to change since nothing is official but based on the leaks, most of them will be quite close to what we have stated here.Specs Summary:Architecture: RDNA 4GPU: Navi 48SPs: 4096RT Cores: 64Base Clock: 2.5 GHzBoost Clock: 3.1 GHzInterface: PCI-E 5.0VRAM: 16GB GDDR6, 256-bit, 20 GbpsMemory Bandwidth: 640 GB/sTexture Fillrate: 783.4 GTexels/sPixel Fillrate: 195.8 GPixel/sTDP: 260W (Reference), Up to 330W (Custom)PerformanceEarlier it was believed that the RX 9070 XT would go against the RTX 4080 GPU but the recent benchmarks show that it is slightly above the Radeon RX 7900 GRE from the previous lineup. In simple words, it’s a successor to the RX 7800 XT or the RX 7900 GRE and this will be decided according to the price.In another synthetic benchmark (Time Spy), the RX 9070 XT was barely 2% faster than the RX 7900 GRE and was noticeably slower than the RX 7900 XT. It scored 22,894 points while the RX 7900 XT delivered 27,044 points, which puts the RX 9070 XT around 14% slower than the RX 7900 XT. This is a much lower performance number than the RTX 4080, which is around 4% faster than the 7900 XT in Time Spy.In another recently leaked benchmark, the Radeon RX 9070 XT came out on par with the RX 7900 XTX and RTX 4070 Ti Super. This was the 3DMark Speed Way test, where it delivered 6345 points. In another test called 3DMark Time Spy Extreme, the RX 9070 XT delivered 14448 points, which was even higher than the RTX 4080 and 4080 Super. It should be kept in mind that these are subject to change as the GPU currently doesn’t boast the latest drivers.The gaming benchmarks aren’t yet here and the 3DMark synthetic scores may not accurately translate to such performance in games but it’s unlikely that the RX 9070 XT is going to surpass the RTX 4080 or RTX 4080 Super. However, according to one report, the RX 9070 XT is within 5% of the RTX 4080 performance in select titles. This is still not confirmed due to the absence of evidence but could be possible as well.Price and Release DateThe official MSRP of the Radeon RX 9070 XT isn’t yet available publicly but as per some rumors, it is expected that the Navi 48-based Radeon RX 9070 XT will cost $479 but the AIB editions will start at $500+. This was seen in recent retail listings where the AIB editions were over $549. Once again, this is just a rumor and nothing has been confirmed yet.The Radeon RX 9070 XT is expected to be revealed at CES 2025, and at the same time, AMD is going to reveal the FSR 4 and Ryzen 9000X3D processors. This report has been confirmed by a reliable leaker and since most hardware vendors including NVIDIA, are launching their cutting-edge hardware at CES, expect AMD to launch its RDNA 4 discrete GPU lineup in the early January CES event.AMD Radeon RX 9070 XT: What to Expect?Based on the available information on the specs, performance, and pricing of the Radeon RX 9070 XT, one shouldn’t have high expectations from the card. Even though the architecture has been improved, the pricing remains a challenging part for AMD to succeed against NVIDIA. If RX 9070 XT goes live at $599 or $649 while its performance is barely higher than RX 7900 GRE, it is DOA.If the price remains $499 at max, it may seem like a satisfactory option but since Radeon RX 7800 XT and 7900 GRE are now available for lower than their launch MSRP, it could be hard to convince the consumers to buy the RX 9070 XT. On the flip side, if the performance comes closer to RTX 4080, the $649 price tag will be appealing. Nonetheless, not much has been improved in its specs compared to its predecessors and it needs to be priced competitively to gain an advantage over NVIDIA.
The Navi 48 die will bring up to 64 Compute Units and this means 4096 Stream Processors on the RX 9070 XT. The clock speeds are rumored to hit up to 3.1 GHz and the default TDP rating will be 260W. This is for the reference edition but the custom AIB cards can reach higher frequencies and can consume up to 330W. The specs are subject to change since nothing is official but based on the leaks, most of them will be quite close to what we have stated here.Specs Summary:Architecture: RDNA 4GPU: Navi 48SPs: 4096RT Cores: 64Base Clock: 2.5 GHzBoost Clock: 3.1 GHzInterface: PCI-E 5.0VRAM: 16GB GDDR6, 256-bit, 20 GbpsMemory Bandwidth: 640 GB/sTexture Fillrate: 783.4 GTexels/sPixel Fillrate: 195.8 GPixel/sTDP: 260W (Reference), Up to 330W (Custom)PerformanceEarlier it was believed that the RX 9070 XT would go against the RTX 4080 GPU but the recent benchmarks show that it is slightly above the Radeon RX 7900 GRE from the previous lineup. In simple words, it’s a successor to the RX 7800 XT or the RX 7900 GRE and this will be decided according to the price.In another synthetic benchmark (Time Spy), the RX 9070 XT was barely 2% faster than the RX 7900 GRE and was noticeably slower than the RX 7900 XT. It scored 22,894 points while the RX 7900 XT delivered 27,044 points, which puts the RX 9070 XT around 14% slower than the RX 7900 XT. This is a much lower performance number than the RTX 4080, which is around 4% faster than the 7900 XT in Time Spy.In another recently leaked benchmark, the Radeon RX 9070 XT came out on par with the RX 7900 XTX and RTX 4070 Ti Super. This was the 3DMark Speed Way test, where it delivered 6345 points. In another test called 3DMark Time Spy Extreme, the RX 9070 XT delivered 14448 points, which was even higher than the RTX 4080 and 4080 Super. It should be kept in mind that these are subject to change as the GPU currently doesn’t boast the latest drivers.The gaming benchmarks aren’t yet here and the 3DMark synthetic scores may not accurately translate to such performance in games but it’s unlikely that the RX 9070 XT is going to surpass the RTX 4080 or RTX 4080 Super. However, according to one report, the RX 9070 XT is within 5% of the RTX 4080 performance in select titles. This is still not confirmed due to the absence of evidence but could be possible as well.Price and Release DateThe official MSRP of the Radeon RX 9070 XT isn’t yet available publicly but as per some rumors, it is expected that the Navi 48-based Radeon RX 9070 XT will cost $479 but the AIB editions will start at $500+. This was seen in recent retail listings where the AIB editions were over $549. Once again, this is just a rumor and nothing has been confirmed yet.The Radeon RX 9070 XT is expected to be revealed at CES 2025, and at the same time, AMD is going to reveal the FSR 4 and Ryzen 9000X3D processors. This report has been confirmed by a reliable leaker and since most hardware vendors including NVIDIA, are launching their cutting-edge hardware at CES, expect AMD to launch its RDNA 4 discrete GPU lineup in the early January CES event.AMD Radeon RX 9070 XT: What to Expect?Based on the available information on the specs, performance, and pricing of the Radeon RX 9070 XT, one shouldn’t have high expectations from the card. Even though the architecture has been improved, the pricing remains a challenging part for AMD to succeed against NVIDIA. If RX 9070 XT goes live at $599 or $649 while its performance is barely higher than RX 7900 GRE, it is DOA.If the price remains $499 at max, it may seem like a satisfactory option but since Radeon RX 7800 XT and 7900 GRE are now available for lower than their launch MSRP, it could be hard to convince the consumers to buy the RX 9070 XT. On the flip side, if the performance comes closer to RTX 4080, the $649 price tag will be appealing. Nonetheless, not much has been improved in its specs compared to its predecessors and it needs to be priced competitively to gain an advantage over NVIDIA.
Specs Summary:Architecture: RDNA 4GPU: Navi 48SPs: 4096RT Cores: 64Base Clock: 2.5 GHzBoost Clock: 3.1 GHzInterface: PCI-E 5.0VRAM: 16GB GDDR6, 256-bit, 20 GbpsMemory Bandwidth: 640 GB/sTexture Fillrate: 783.4 GTexels/sPixel Fillrate: 195.8 GPixel/sTDP: 260W (Reference), Up to 330W (Custom)PerformanceEarlier it was believed that the RX 9070 XT would go against the RTX 4080 GPU but the recent benchmarks show that it is slightly above the Radeon RX 7900 GRE from the previous lineup. In simple words, it’s a successor to the RX 7800 XT or the RX 7900 GRE and this will be decided according to the price.In another synthetic benchmark (Time Spy), the RX 9070 XT was barely 2% faster than the RX 7900 GRE and was noticeably slower than the RX 7900 XT. It scored 22,894 points while the RX 7900 XT delivered 27,044 points, which puts the RX 9070 XT around 14% slower than the RX 7900 XT. This is a much lower performance number than the RTX 4080, which is around 4% faster than the 7900 XT in Time Spy.In another recently leaked benchmark, the Radeon RX 9070 XT came out on par with the RX 7900 XTX and RTX 4070 Ti Super. This was the 3DMark Speed Way test, where it delivered 6345 points. In another test called 3DMark Time Spy Extreme, the RX 9070 XT delivered 14448 points, which was even higher than the RTX 4080 and 4080 Super. It should be kept in mind that these are subject to change as the GPU currently doesn’t boast the latest drivers.The gaming benchmarks aren’t yet here and the 3DMark synthetic scores may not accurately translate to such performance in games but it’s unlikely that the RX 9070 XT is going to surpass the RTX 4080 or RTX 4080 Super. However, according to one report, the RX 9070 XT is within 5% of the RTX 4080 performance in select titles. This is still not confirmed due to the absence of evidence but could be possible as well.Price and Release DateThe official MSRP of the Radeon RX 9070 XT isn’t yet available publicly but as per some rumors, it is expected that the Navi 48-based Radeon RX 9070 XT will cost $479 but the AIB editions will start at $500+. This was seen in recent retail listings where the AIB editions were over $549. Once again, this is just a rumor and nothing has been confirmed yet.The Radeon RX 9070 XT is expected to be revealed at CES 2025, and at the same time, AMD is going to reveal the FSR 4 and Ryzen 9000X3D processors. This report has been confirmed by a reliable leaker and since most hardware vendors including NVIDIA, are launching their cutting-edge hardware at CES, expect AMD to launch its RDNA 4 discrete GPU lineup in the early January CES event.AMD Radeon RX 9070 XT: What to Expect?Based on the available information on the specs, performance, and pricing of the Radeon RX 9070 XT, one shouldn’t have high expectations from the card. Even though the architecture has been improved, the pricing remains a challenging part for AMD to succeed against NVIDIA. If RX 9070 XT goes live at $599 or $649 while its performance is barely higher than RX 7900 GRE, it is DOA.If the price remains $499 at max, it may seem like a satisfactory option but since Radeon RX 7800 XT and 7900 GRE are now available for lower than their launch MSRP, it could be hard to convince the consumers to buy the RX 9070 XT. On the flip side, if the performance comes closer to RTX 4080, the $649 price tag will be appealing. Nonetheless, not much has been improved in its specs compared to its predecessors and it needs to be priced competitively to gain an advantage over NVIDIA.
Earlier it was believed that the RX 9070 XT would go against the RTX 4080 GPU but the recent benchmarks show that it is slightly above the Radeon RX 7900 GRE from the previous lineup. In simple words, it’s a successor to the RX 7800 XT or the RX 7900 GRE and this will be decided according to the price.
In another synthetic benchmark (Time Spy), the RX 9070 XT was barely 2% faster than the RX 7900 GRE and was noticeably slower than the RX 7900 XT. It scored 22,894 points while the RX 7900 XT delivered 27,044 points, which puts the RX 9070 XT around 14% slower than the RX 7900 XT. This is a much lower performance number than the RTX 4080, which is around 4% faster than the 7900 XT in Time Spy.In another recently leaked benchmark, the Radeon RX 9070 XT came out on par with the RX 7900 XTX and RTX 4070 Ti Super. This was the 3DMark Speed Way test, where it delivered 6345 points. In another test called 3DMark Time Spy Extreme, the RX 9070 XT delivered 14448 points, which was even higher than the RTX 4080 and 4080 Super. It should be kept in mind that these are subject to change as the GPU currently doesn’t boast the latest drivers.The gaming benchmarks aren’t yet here and the 3DMark synthetic scores may not accurately translate to such performance in games but it’s unlikely that the RX 9070 XT is going to surpass the RTX 4080 or RTX 4080 Super. However, according to one report, the RX 9070 XT is within 5% of the RTX 4080 performance in select titles. This is still not confirmed due to the absence of evidence but could be possible as well.Price and Release DateThe official MSRP of the Radeon RX 9070 XT isn’t yet available publicly but as per some rumors, it is expected that the Navi 48-based Radeon RX 9070 XT will cost $479 but the AIB editions will start at $500+. This was seen in recent retail listings where the AIB editions were over $549. Once again, this is just a rumor and nothing has been confirmed yet.The Radeon RX 9070 XT is expected to be revealed at CES 2025, and at the same time, AMD is going to reveal the FSR 4 and Ryzen 9000X3D processors. This report has been confirmed by a reliable leaker and since most hardware vendors including NVIDIA, are launching their cutting-edge hardware at CES, expect AMD to launch its RDNA 4 discrete GPU lineup in the early January CES event.AMD Radeon RX 9070 XT: What to Expect?Based on the available information on the specs, performance, and pricing of the Radeon RX 9070 XT, one shouldn’t have high expectations from the card. Even though the architecture has been improved, the pricing remains a challenging part for AMD to succeed against NVIDIA. If RX 9070 XT goes live at $599 or $649 while its performance is barely higher than RX 7900 GRE, it is DOA.If the price remains $499 at max, it may seem like a satisfactory option but since Radeon RX 7800 XT and 7900 GRE are now available for lower than their launch MSRP, it could be hard to convince the consumers to buy the RX 9070 XT. On the flip side, if the performance comes closer to RTX 4080, the $649 price tag will be appealing. Nonetheless, not much has been improved in its specs compared to its predecessors and it needs to be priced competitively to gain an advantage over NVIDIA.
In another recently leaked benchmark, the Radeon RX 9070 XT came out on par with the RX 7900 XTX and RTX 4070 Ti Super. This was the 3DMark Speed Way test, where it delivered 6345 points. In another test called 3DMark Time Spy Extreme, the RX 9070 XT delivered 14448 points, which was even higher than the RTX 4080 and 4080 Super. It should be kept in mind that these are subject to change as the GPU currently doesn’t boast the latest drivers.The gaming benchmarks aren’t yet here and the 3DMark synthetic scores may not accurately translate to such performance in games but it’s unlikely that the RX 9070 XT is going to surpass the RTX 4080 or RTX 4080 Super. However, according to one report, the RX 9070 XT is within 5% of the RTX 4080 performance in select titles. This is still not confirmed due to the absence of evidence but could be possible as well.Price and Release DateThe official MSRP of the Radeon RX 9070 XT isn’t yet available publicly but as per some rumors, it is expected that the Navi 48-based Radeon RX 9070 XT will cost $479 but the AIB editions will start at $500+. This was seen in recent retail listings where the AIB editions were over $549. Once again, this is just a rumor and nothing has been confirmed yet.The Radeon RX 9070 XT is expected to be revealed at CES 2025, and at the same time, AMD is going to reveal the FSR 4 and Ryzen 9000X3D processors. This report has been confirmed by a reliable leaker and since most hardware vendors including NVIDIA, are launching their cutting-edge hardware at CES, expect AMD to launch its RDNA 4 discrete GPU lineup in the early January CES event.AMD Radeon RX 9070 XT: What to Expect?Based on the available information on the specs, performance, and pricing of the Radeon RX 9070 XT, one shouldn’t have high expectations from the card. Even though the architecture has been improved, the pricing remains a challenging part for AMD to succeed against NVIDIA. If RX 9070 XT goes live at $599 or $649 while its performance is barely higher than RX 7900 GRE, it is DOA.If the price remains $499 at max, it may seem like a satisfactory option but since Radeon RX 7800 XT and 7900 GRE are now available for lower than their launch MSRP, it could be hard to convince the consumers to buy the RX 9070 XT. On the flip side, if the performance comes closer to RTX 4080, the $649 price tag will be appealing. Nonetheless, not much has been improved in its specs compared to its predecessors and it needs to be priced competitively to gain an advantage over NVIDIA.
The gaming benchmarks aren’t yet here and the 3DMark synthetic scores may not accurately translate to such performance in games but it’s unlikely that the RX 9070 XT is going to surpass the RTX 4080 or RTX 4080 Super. However, according to one report, the RX 9070 XT is within 5% of the RTX 4080 performance in select titles. This is still not confirmed due to the absence of evidence but could be possible as well.
The official MSRP of the Radeon RX 9070 XT isn’t yet available publicly but as per some rumors, it is expected that the Navi 48-based Radeon RX 9070 XT will cost $479 but the AIB editions will start at $500+. This was seen in recent retail listings where the AIB editions were over $549. Once again, this is just a rumor and nothing has been confirmed yet.The Radeon RX 9070 XT is expected to be revealed at CES 2025, and at the same time, AMD is going to reveal the FSR 4 and Ryzen 9000X3D processors. This report has been confirmed by a reliable leaker and since most hardware vendors including NVIDIA, are launching their cutting-edge hardware at CES, expect AMD to launch its RDNA 4 discrete GPU lineup in the early January CES event.AMD Radeon RX 9070 XT: What to Expect?Based on the available information on the specs, performance, and pricing of the Radeon RX 9070 XT, one shouldn’t have high expectations from the card. Even though the architecture has been improved, the pricing remains a challenging part for AMD to succeed against NVIDIA. If RX 9070 XT goes live at $599 or $649 while its performance is barely higher than RX 7900 GRE, it is DOA.If the price remains $499 at max, it may seem like a satisfactory option but since Radeon RX 7800 XT and 7900 GRE are now available for lower than their launch MSRP, it could be hard to convince the consumers to buy the RX 9070 XT. On the flip side, if the performance comes closer to RTX 4080, the $649 price tag will be appealing. Nonetheless, not much has been improved in its specs compared to its predecessors and it needs to be priced competitively to gain an advantage over NVIDIA.
The Radeon RX 9070 XT is expected to be revealed at CES 2025, and at the same time, AMD is going to reveal the FSR 4 and Ryzen 9000X3D processors. This report has been confirmed by a reliable leaker and since most hardware vendors including NVIDIA, are launching their cutting-edge hardware at CES, expect AMD to launch its RDNA 4 discrete GPU lineup in the early January CES event.AMD Radeon RX 9070 XT: What to Expect?Based on the available information on the specs, performance, and pricing of the Radeon RX 9070 XT, one shouldn’t have high expectations from the card. Even though the architecture has been improved, the pricing remains a challenging part for AMD to succeed against NVIDIA. If RX 9070 XT goes live at $599 or $649 while its performance is barely higher than RX 7900 GRE, it is DOA.If the price remains $499 at max, it may seem like a satisfactory option but since Radeon RX 7800 XT and 7900 GRE are now available for lower than their launch MSRP, it could be hard to convince the consumers to buy the RX 9070 XT. On the flip side, if the performance comes closer to RTX 4080, the $649 price tag will be appealing. Nonetheless, not much has been improved in its specs compared to its predecessors and it needs to be priced competitively to gain an advantage over NVIDIA.
Based on the available information on the specs, performance, and pricing of the Radeon RX 9070 XT, one shouldn’t have high expectations from the card. Even though the architecture has been improved, the pricing remains a challenging part for AMD to succeed against NVIDIA. If RX 9070 XT goes live at $599 or $649 while its performance is barely higher than RX 7900 GRE, it is DOA.
If the price remains $499 at max, it may seem like a satisfactory option but since Radeon RX 7800 XT and 7900 GRE are now available for lower than their launch MSRP, it could be hard to convince the consumers to buy the RX 9070 XT. On the flip side, if the performance comes closer to RTX 4080, the $649 price tag will be appealing. Nonetheless, not much has been improved in its specs compared to its predecessors and it needs to be priced competitively to gain an advantage over NVIDIA.
Your email address will not be published. Required fields are marked *Comment *Name * Email * Website  
Δ
This site uses Akismet to reduce spam. Learn how your comment data is processed.
Xtremegaminerd.com is a participant in Amazon Associate Program and is supported by the readers. The qualifying purchase you make through our links may get us some commission and doesn't cost you an extra penny.
A publication dedicated to PC hardware, gaming, and general tech."
"AMD","RX 9070 XT","AMD Radeon RX 9070 XT: Everything You Need To Know","https://www.forbes.com/sites/antonyleather/2024/12/31/amd-radeon-rx-9070-xt-everything-you-need-to-know/
",""
"AMD","RX 9070","AMD Radeon RX 9070 Review","https://www.techspot.com/review/2962-amd-radeon-9070/","AMD looks to have hit it out of the park with the Radeon 9070 XT. Reviews have been universally positive, and reception from gamers has been even better. Now, AMD just needs to follow through with the next step: getting these GPUs into the hands of gamers at the expected MSRP. Hopefully, we'll see this happen over the next few days.
Before that, we need to take a closer look at the cheaper option – the Radeon RX 9070 (the non-XT model). Interestingly, AMD has priced it at $550, the same alleged MSRP as the RTX 5070.
The new vanilla Radeon RX 9070 should generally be faster than Nvidia's RTX 5070, at least based on what we saw from the XT before. As an added bonus, AMD's GPU comes with more VRAM – 16GB vs. 12GB for the GeForce, which is less than ideal for those looking to take full advantage of RTX features like frame generation and ray tracing.
However, in our opinion, the RTX 5070 competition isn't the biggest issue for the RX 9070 – that would be its big brother, the 9070 XT, which is fetching a mere extra $50.
This means the standard RX 9070 is offering a small 8% discount, which on its own may not seem like much. However, looking at the specs, it has 13% fewer cores and texture mapping units, suggesting that the performance downgrade will be greater than the price difference suggests.
We have a lot of data to go over, but before diving into performance benchmarks, let's take a look at some of the graphics cards we have on hand to test.
For testing, we used the Sapphire Pure RX 9070, along with the PowerColor Hellhound and XFX Quicksilver. Let's see how they compare.
The Hellhound had a GPU junction temperature of 59°C, while the Quicksilver was slightly cooler at 58°C. The Pure was the coolest of the three, reaching just 53°C.
Looking at hotspot temperatures, the Quicksilver was the coolest at 69°C, followed by the Pure at 71°C, and the Hellhound at 76°C, which is still a very reasonable hotspot temperature.
All three models had a peak memory temperature of 86°C, with VRM temperatures staying below 70°C, so there were no concerns in that area.
In terms of fan speed, the Hellhound operated the quietest, running at just 930 RPM. The Pure and Quicksilver were also quiet, with fan speeds ranging between 1,000 and 1,100 RPM.
Regarding clock speeds, the Hellhound had the lowest, averaging 2,745 MHz. The Pure clocked 4% higher, averaging 2,840 MHz, while the Quicksilver was close behind at 2,810 MHz.
Overall, all three models delivered excellent results.
First, let's take a look at Marvel Rivals performance. The 9070 is essentially on par with the 5070 at 1440p, rendering 77 fps on average. This makes it 13% slower than the 9070 XT and 7% slower than the 7900 XT.
When increasing the resolution to 4K, the 9070 falls behind the 5070 by a 7% margin, rendering just 41 fps on average – similar to the old RTX 3080 and only 5% faster than the 7900 GRE.
Next, we have Stalker 2, where the 9070 is just 5% slower than the 9070 XT at 1440p, which allows it to edge out the RTX 5070 by a narrow 6% margin.
However, moving up to 4K significantly impacts performance, making the 9070 18% slower than the XT version. It renders just 32 fps on average, putting it roughly on par with the RTX 5070.
For Counter-Strike 2, the 9070 doesn't stand out at 1440p. Although it is only 6% slower than the XT version, it also lags 7% behind the RTX 5070.
Increasing the resolution to 4K doesn't improve the situation. Here, the 9070 is 10% slower than the 5070 and 8% slower than the 9070 XT.
The Space Marine 2 performance is quite strong. The 9070 is just 5% slower than the XT version, putting it on par with the RTX 4070 Super, 7900 XT, and, surprisingly, the RTX 5080. For some reason, the new Blackwell GPUs struggle in this title, which explains why the 9070 appears 24% faster than the RTX 5070.
Things get even worse for Blackwell GPUs at 4K, where the 9070 is 57% faster than the 5070. Something is clearly off here, and Nvidia will likely need to address this at the driver level.
Next, we have Cyberpunk 2077, a title where Radeon GPUs have always performed well – provided ray tracing is disabled. At 1440p, the 9070 is 10% slower than the XT version but 15% faster than the RTX 5070, making for a strong win.
At 4K, this margin increases to 20%, keeping the 9070 comfortably ahead of the RTX 5070.
Call of Duty: Black Ops 6 tends to favor AMD hardware, and the results reflect that. At 1440p, the 9070 is 8% slower than the 9070 XT but, more importantly, 40% faster than the RTX 5070. These results might seem surprising, but the data is accurate.
At 4K, the 9070 trails the XT version by 10%, yet still outperforms the 5070 by an impressive 44% margin.
In Dragon Age: The Veilguard, the 9070 is 7% slower than the XT version, which puts it neck and neck with the RTX 5070 at 1440p, both averaging 79 fps.
At 4K, the situation remains the same, with both the 5070 and 9070 averaging 53 fps, making them 7% slower than the 9070 XT.
Testing Hogwarts Legacy at 1440p shows the 9070 coming in just 2% slower than the 9070 XT, allowing it to surpass the RTX 5070 by 7%.
At 4K, the gap widens slightly, with the 9070 trailing the XT by 8%, yet still outperforming the 5070 by a solid 21% margin.
The final individual game we'll examine is The Last of Us Part I, though we'll also discuss ray tracing performance shortly.
At 1440p, the 9070 is 15% slower than the 9070 XT, putting it exactly on par with the RTX 5070.
At 4K, the 9070 remains 23% slower than the XT version, once again matching the 5070, both averaging 48 fps.
Across the 18 games tested, the Radeon 9070 was on average 8% slower than the 9070 XT – not a terrible result. However, it was also just 4% faster than the RTX 5070 in mostly raster-based gaming, which may not be a big enough advantage.
At 4K, the Radeon 9070 ends up 12% slower than the 9070 XT, which is disappointing given that it is only 8% cheaper based on MSRP. It was also 8% faster than the RTX 5070, which is a slight improvement, but not a major win considering both should be priced at $550.
When it comes to power consumption, the Radeon 9070 draws anywhere from 15% to 23% less power than the 9070 XT in our testing, making it the more efficient option.
Compared to the RTX 5070, power consumption varies from just a few extra watts to as much as 20% higher. A worst-case scenario was Starfield, where the 9070 consumed 20% more power than the 5070 for only a 6% performance gain.
Now for some ray tracing performance, starting with Metro Exodus Enhanced at 1440p. Here, the 9070 was 15% slower than the XT version, placing it right alongside the RTX 5070 – not a bad result.
At 4K, it remained 15% slower than the XT model and 6% slower than the 5070. While not spectacular, this is a much more competitive showing compared to the previous generation.
In our 9070 XT review, we found that RDNA4 GPUs struggle with Alan Wake II at the High RT preset. Unsurprisingly, the 9070 doesn't fare well, averaging just 31 fps. While that makes it 21% slower than the RTX 5070, it's worth noting that the 5070 also struggles, delivering just 39 fps.
At 4K, performance is completely unplayable, but since we have the data, we're including it for those interested.
Next is Cyberpunk 2077, using the Ultra RT preset with quality upscaling at 1440p. Here, the 9070 is 16% slower than the XT version, roughly matching the RTX 5070 at 57 fps.
At 4K, the 9070 averages 31 fps, making it 14% slower than the 9070 XT, once again matching the RTX 5070.
In Spider-Man Remastered, the 1440p results are mostly CPU-limited, meaning the 9070 performs identically to the 9070 XT and RTX 5070.
At 4K, where the CPU bottleneck is reduced, the 9070 is 7% slower than the XT model but manages to be 10% faster than the RTX 5070 – an excellent result for AMD.
The 9070 trails the 5070 in Dying Light 2 at 1440p, coming in 6% slower, and it is also 11% slower than the 9070 XT. However, it does manage to be 10% faster than the 7900 XT.
At 4K, it falls 13% behind the XT model and is also 11% slower than the RTX 5070.
Unfortunately, the RDNA4 GPUs struggle in Black Myth: Wukong with Very High RT enabled. While the RTX 5070 delivers an unimpressive 46 fps at 1440p with upscaling, it is still significantly better than the 25 fps the 9070 manages.
With performance this low, there's little reason to discuss 4K upscaled results, but we've included the data for those interested.
Lastly, in Indiana Jones and the Great Circle, the Full RT setting is essential for the best experience. However, in this case, ray tracing isn't viable on either the 9070 or the 5070. For this game, a 5080 or 5090 is necessary, which will undoubtedly make Nvidia very happy.
Across the six games tested for ray tracing performance, the Radeon 9070 was on average 13% slower than the 9070 XT and 16% slower than the RTX 5070. Not a bad result for a Radeon GPU, and it also means the 9070 is a whopping 26% faster than the 7900 XTX previous-gen flagship.
The 4K upscaled data is less relevant since both the 9070 and 5070 often fail to deliver acceptable ray tracing performance at this higher resolution. However, for those interested, the 9070 was 28% slower than the 5070 in this scenario.
Now, let's look at the cost per frame data. This analysis now includes results from Space Marine 2 and Black Ops 6, meaning the data is based on an 18-game average. Both titles favor AMD, though we expect Nvidia to address its issues in Space Marine 2 at some point. However, so far, there have been no updates on that front.
The data shows the 9070 XT offering the best value per frame, with the non-XT version providing 4% worse value per frame. While the Radeon 9070 offers better overall value compared to the very poor RTX 5070, it still doesn't make full sense in our opinion, and should be priced at $500.
Compared to retail pricing from last year, the Radeon 9070 series still slots in quite favorably, offering the best value. However, the 9070's $550 MSRP still doesn't make much sense relative to the 9070 XT, meaning the XT version is the obvious choice at these price points.
The most obvious conclusion we can make about the Radeon RX 9070 is that if available at the MSRP, this model should be ignored in favor of the 9070 XT. For an extra $50 (a 9% price increase), you get ~14% better performance. Everything else is identical: same VRAM, same architecture, etc. so the XT is simply the better buy.
Not surprisingly though, in the current turbulent GPU market, pricing and availability will ultimately determine the Radeon 9070's value. While supply of the RX 9070 series GPUs appears strong, there are concerns about the availability of MSRP models. We will revisit this topic next week with a detailed update on GPU pricing, and we'll continue to monitor the situation closely.
As it stands, the Radeon 9070 is an unusual product, though not out of character for AMD. Last generation, AMD priced the 7900 XT just $100 below the 7900 XTX – a 10% discount, despite having 17% less VRAM and 17% lower performance. This pricing strategy didn't make sense and was largely met with negative reviews.
But that didn't stop AMD. They repeated the same pricing strategy with the 7800 XT and 7700 XT, making the 7700 XT only 10% cheaper, even though it had 25% less VRAM and was 16% slower. Once again, this resulted in mostly negative reviews for the 7700 XT. So, this is a repeat strategy from AMD – a bold move, to say the least. However, no one outside of AMD seems to understand this approach, and even some within AMD appear equally confused.
Now, setting aside the fact that the Radeon 9070 XT is the obvious choice, let's pretend that option doesn't exist and make this a straight RX 9070 vs. RTX 5070 battle, who wins there?
In terms of value, the Radeon RX 9070 offers 8% better cost per frame for raster performance. However, for ray tracing-heavy workloads, the RTX 5070 is on average 16% faster – assuming you don't exceed its 12GB VRAM buffer (as happened in Indiana Jones and the Great Circle). The memory limitations of the RTX 5070 will likely become a bigger issue in the future, raising concerns about its long-term viability.
You could certainly argue that, right now, RT performance is a win for the RTX 5070, with DLSS 4 upscaling being another advantage. However, early impressions of FSR 4 suggest it is competitive, making it a viable alternative.
In our opinion, the Radeon RX 9070 appears to be the better product overall. It is generally faster and has 16GB of VRAM, which should age better over the next 3 – 4 years. Though ultimately, it doesn't matter as you wouldn't buy either product at the proposed MSRP, you would simply buy the faster 9070 XT and call it a day.

About
Ethics Statement
Terms of Use
Privacy Policy
Manage Ad Consent

TECHSPOT : Tech Enthusiasts, Power Users, Gamers
TechSpot is a registered trademark and may not be used by third parties without express written permission.
TechSpot is represented by Future PLC. Learn about advertising opportunities.
© 1998 - 2025 TechSpot, Inc. All Rights Reserved."
"AMD","RX 9070","Incredibly Efficient: AMD RX 9070 GPU Review & Benchmarks vs 9070 XT, RTX 5070","https://gamersnexus.net/gpus/incredibly-efficient-amd-rx-9070-gpu-review-benchmarks-vs-9070-xt-rtx-5070","Incredibly Efficient: AMD RX 9070 GPU Review & Benchmarks vs. 9070 XT, RTX 5070
The RX 9070 non-XT’s MSRP is $550, with the XT 9% higher at $600 MSRP. That makes the XT version just 9% more expensive. Despite the similar pricing, the 9070 actually remains really interesting on its own.First of all, in some of our efficiency tests when looking at performance per watt, it nearly tied for the most efficient GPU. This seems to be in the so-called “sweet spot” of power and performance in what would be a huge upset favoring AMD. The fact that it’s starting to chip away at one of NVIDIA’s key areas of competition is huge. The reason for that is simply because the 9070 pulls a lot less power than the 9070 XT by percentage but most of the performance remains.
Editor's note: This was originally published on March 6, 2025 as a video. This content has been adapted to written format for this article and is unchanged from the original publication.
The quickest possible version is this: For the 9070 XT vs. the 9070, the XT model is in the range of 9% to 16% better at 4K and 1440p rasterized, but typically is about 11-13% better than the 9070 in our suite of tested games. At 1080p, it’s 6% to 15% better, with most cases around 9% to 11% for the games we tested.
As for the RTX 5070, it’s not looking good for NVIDIA’s card you shouldn’t buy: At 4K and rasterized, the 9070 leads the 5070 by 0% to 18%, depending on game. It has one loss in Final Fantasy and it has losses in some ray tracing tests, but not all of them, which is interesting.
The 5070 is basically out of the conversation, so we’re left with the 9070 XT or the 9070 at this price point -- which is what AMD probably wants, because the 9070 is priced in a way that the 9070 XT becomes an easy upsell. That might be to do with yields. If they’re yielding enough XTs that they don’t really want to sell non-XTs, maybe that’s the strategy. 
We’re not going to spend much time on the specs and architecture today. We’re also going to speedrun the charts and shorten the comparisons. If you want the full depth, check out our 9070 XT review. That contains some more discussion of results and some more architectural information. Our news video about the 9070 series has even more architecture background than that.
The RX 9070 has 56 Compute Units, with the 9070 XT running 64 CUs. Both are 16 GB cards with GDDR6 on a 256-bit bus. The GPUs have slightly different clocks. Both GPUs get the architectural overhauls of RDNA 4 and there’s no difference there, so the same ray tracing changes apply to the 9070 as the XT. The only real main difference is the move to 56 CUs.
We’ll save the pricing recap for post-launch -- we want to monitor launch for the next day or two to get an idea of availability of both the 5070 and 9070 series, plus the pricing they land at.
Dragon’s Dogma 2 is up first and at 4K initially.
In this test, the RX 9070 non-XT ran at 64 FPS AVG, with lows at 54 and 53. The 9070 XT’s 70 FPS result positions it 9% ahead, which is exactly how much higher the base MSRP is.
Against the equivalently priced RTX 5070, this is one of AMD’s stronger titles: The 9070 Pulse leads the 5070 FE’s 56 FPS AVG by 14% here, with 1% and 0.1% lows improved in-step with the average. The 9070 is achieving 87% of the performance of the 5070 Ti, but at 73% of the base MSRP.
The RX 9070 leads the RX 7800 XT (watch our review) by 37% here as well. We’ll look at the 6700 XT elsewhere.
At 1440p, the RX 9070 ran at 106 FPS AVG, with the 9070 XT leading it at 116 FPS (or 9.3%). The 5070 Ti is only a few FPS higher than the 9070 XT, with the 4080 Super (read our review) just beyond that. In short, the 9070 is achieving 88% of the 4080 Super’s performance, which was a $1,000 card, but at 55% of the original MSRP. Despite our wishes that the 9070 were $50 cheaper, this at least helps to slowly reset the runaway pricing that NVIDIA set in motion. This gives everyone some perspective.
For some comparisons against prior generations: The 9070 improves on the 67 FPS result of the 7700 XT (watch our review) by 59%, the 53 FPS result of the 6700 XT by 100%, and the 42 FPS of the 6600 XT by 152%.
Here’s 1080p. The old GTX 1060 and 1070 also appear on this chart to give some insight into upgrades, though anything modern would be better. There’s no point getting into percentages for those: They go from unplayable to playable.
The RX 9070 is about tied with the RX 7900 XT. The 9070 XT still leads by around 8%, with the 5070 Ti (read our review) ahead of the 9070 by 13%. The 9070 leads the RTX 5070 DOA edition by 6%. The lead over the 6700 XT is 96%, with the improvement on the 6600 non-XT’s 49 FPS result at 173%.
Final Fantasy 14: Dawntrail is up now. We included this same disclaimer and disclosure in our RX 9070 XT review, but to recap: We’ve noticed lower performance for the 9070 series specifically in Final Fantasy XIV: Dawntrail, including lower results than AMD’s own claims. We’ve had highly reliable results in this benchmark and our RX 9070 series data has been repeatable across multiple retests, so we’re publishing it, but we just want to highlight that there is disproportionately low performance for the 9070s and that it is something we’re investigating. It’s possible it could be the specific scene, benchmark version, or just the architecture’s behavior here. But the results are repeatable, so we’re running them.
At 4K, the RX 9070 continues the scaling pattern set by the 9070 XT. The XT is again 9% ahead. 
The 5070 FE leads both of the cards in this test, running at 78 FPS AVG and improving upon the 9070’s result of 63 FPS by 24%. Against the prior generation, the 9070 XT sees a smaller improvement on the 6700 XT of 53%. 
At 1440p, the 9070 ran at 126 FPS AVG. The 9070 XT is 10% ahead here, with the 5070 ahead by 21%. Improvement over last generation is non-existent, with the 7800 XT right next to the 9070.
Resident Evil 4 is up next, tested at 4K first.
The 9070 ran at 91 FPS AVG here, creating a 13% lead for the 9070 XT and its 103 FPS result. The 9070 XT was excitingly right next to the 4080 Super’s 105 FPS in this game, meaning that the same $1,000 4080 Super result leads the $550 MSRP 9070 by only 16%. That’s a paltry gain for such a huge price difference, especially given how new the 4080 Super is.
AMD’s 9070 exceeds the performance of the same-MSRP RTX 5070’s 78 FPS by 16%. If NVIDIA would stop remaking 4070 Supers (read our review) and 4080s, they might actually make some progress here.
At 1440p, the RX 9070 ran at 172 FPS AVG and outperformed the 3090 Ti. 
The 9070 XT outperforms the 9070 by 13%, with the 5070 Ti only slightly beyond that result.
Against the 5070’s 152 FPS AVG, the 9070 runs 13% ahead. The 5070 really does look even more DOA than previously in some of these tests. If it were a dead horse, it’d be getting kicked right now.
1080p is up now. The 9070 ran at 247 FPS AVG here, which had it between the 4070 Ti (watch our review) and 4070 Ti Super. The 5070 still trails, now giving the 9070 a lead of 10%.
The XT model maintains about a 12% advantage over the non-XT.
Black Myth: Wukong is a much more challenging title for AMD to run, but mostly with ray tracing. Either way, there should be some more favor for NVIDIA here. Even without RT, it’s a heavy game. The ray tracing tests will come later.
At 4K and rasterized, the RX 9070 ran at 41 FPS AVG against the 9070 XT’s 46, giving the latter a lead of 11%. The 7900 XTX sits ahead of the 9070 XT. The 9070 ends up still leading the RTX 5070, albeit on a technicality, which is actually a huge victory for AMD in this particular title. That’ll change with RT, but this is a good position overall in raster.
At 1440p, the RX 9070 held 75 FPS AVG with well-paced lows, sitting at 65 and 60 FPS for 1% and 0.1%. This is consistent (and good) frametime pacing, but it’s about the same as all of its neighbors, so nothing exceptional.
The 5070 ran at 72.1 FPS AVG. The difference between the two cards is imperceptible and shows up in measurements, but would not be noticeable to an end user. 
The 9070 XT’s 83 FPS result has it again about 11% ahead of the 9070, so that’s pretty predictable now.
To just highlight some landmarks: The 2060 ran at 25 FPS and tied the RX 6600, the 6600 XT ran at 29 FPS AVG, and the 3060 ran at 31 FPS AVG.
At 1080p, the 9070’s 104 FPS AVG allowed it to finally start gaining some distance on the 5070. It’s not a meaningful swing at 6% improved, but is in favor of the 9070. If AMD were $50 cheaper, it’d be a clear victory. Unfortunately for AMD, a tie will often be decided in NVIDIA’s favor by most buyers, especially with the complications of RT and software suites. But still, beating the 5070 in this game is an achievement for the 9070.
Starfield is up now, tested at 4K. In this one, the RX 9070 ran at 63 FPS AVG and roughly tied the RTX 4070 Ti Super. The 9070 outperforms the RTX 5070’s 54 FPS AVG by 17%, which is starting to be wide enough that NVIDIA needs to be concerned…if they basically weren’t a monopoly. The 5070 Ti falls below the 9070 XT in this one and only leads the non-XT by about 6 FPS. Hardly enough to spend $200 more for. The proximity to the 4080 (watch our review) and 4080 Super also isn’t great news for NVIDIA.
At 1440p, the RX 9070 ran at 96 FPS AVG and nearly tied the 4070 Ti Super again. The 5070 Ti outperforms the 9070 by a lame 5.4%, with the 9070 XT outperforming the 9070 by 10%. 
Against the 5070’s 83 FPS AVG, the 9070 leads by 15%. If the 5070 weren’t already in the grave, we’d pronounce its impending demise. We might want the 9070 to be a bit cheaper just to see that balance for the times it loses or is tied, but even without that, we can’t deny that AMD is putting up its best fight against NVIDIA in years.
At 1080p, the 9070 runs at 118 FPS AVG. The relative rank hasn’t moved much. It leads the RTX 5070 by 14% here, so the lead has slightly reduced, but it’s still ahead. The 9070 XT leads the 9070 by 8%.
In Dying Light 2 at 4K, the RX 9070 ran at 56 FPS AVG and kept lows remarkably close by, at 50 and 41 -- but not in any more remarkable way than all of its neighbors. This game is just consistent.
The 9070 ends up tied with the 5070. There is no difference between these two and, in these situations, NVIDIA will generally be viewed more favorably by a mainstream audience. That’s where the price would come in, or hopefully awareness of the other results.
The 9070 XT leads the 9070 by 11%, so you’d be paying 9% more at MSRP for about 11% more performance.
This is one of the games where the 5070 Ti pulled ahead of the 9070 XT in a noteworthy way, so we see some of that apply to the 9070 and 5070.
At 1440p, the 9070 runs at 106 FPS AVG and ties the 5070 again. They’re within error of each other. The 9070 XT outperforms the 9070 by 11%, with the 5070 Ti still ahead of the XT. Generationally, we’ll just briefly highlight the RX 6600 (watch our review), RTX 2060 (watch our review), and RTX 3060 (watch our review) as reference points to some of the most common GPUs of past generations. Those should give you an idea of the performance gains.
Cyberpunk is up now. This one has generally been tough for AMD in the past, but is harder for it with ray tracing. We’ll start with rasterized testing and at 4K.
The results are thus far positive: We already knew that the 9070 XT outmatched the 5070 Ti, but now the 9070 is nearly tying the RTX 4080. That’s just embarrassing for NVIDIA’s last-gen mid-range-posed-as-a-flagship GPU as compared to a $550 card. The 9070 technically outperforms the 7900 XT as well, though is basically tied, and the lead over the 5070 is 15% again. 
At 1440p, the 9070 now falls slightly below the 7900 XT, so they’ve traded places as resolution came down, and the 5070 has also gotten closer. The 9070 is now about 11% ahead of the 5070 rather than 15% before. The 9070 XT leads the 9070 by 11% and the 5070 Ti slightly.
In Phantom Liberty at 1080p now, the 9070 landed at 148 FPS AVG, allowing the 7900 XT (watch our review) to maintain its slight lead. The improvement in the 9070 over the 5070 is now down to 7% from 15% at 4K originally. If we dropped it down to 360p, based on this trend, NVIDIA might actually pull ahead. Great. Just like they’ve wanted -- maybe that’s the push for DLSS.
Ray tracing is up. This is where AMD has invested heavy effort with its architecture but all of it has been to try and catch up. NVIDIA was so far ahead in some games that AMD won’t be able to catch them in all scenarios. The real question is how much closer do they get this time.
We’ll start with AMD’s worst case: Black Myth - Wukong, but now with ray tracing and at 4K with upscaling.
The 9070 ran at 26 FPS AVG here, so outdoing the 3070 Ti (watch our review) but below the 3080 (watch our review) from 2020. The 3080’s original MSRP was around $700, although that really only held for a week or two before COVID-era scalping went crazy.
The 9070 XT outdoes the 9070 by 14% here. This is too low of a framerate to be playable, but we do the test for relative scaling and percent scaling.
The RTX 5070 crushes the 9070 here, holding a 57% lead over the AMD GPU. The 5070 Ti’s scaling over the 9070 XT was better and nearly 80%, but 57% is still huge for the 5070 over the 9070. If you want to play Black Myth with ray tracing, like we said in the 9070 XT review, you basically only buy NVIDIA.
Lows are where NVIDIA will struggle the most for 5070 VRAM capacity. In this particular test, we aren’t hitting VRAM saturation; however, we have shown instances where the 5070 hits saturation with heavy workloads and higher resolutions, leading to heavy stutter like in the Cyberpunk RT 4K scenario.
At 1440p upscaled, the 9070 pushed 47 FPS. This gave the 9070 XT about a 6 FPS lead, nearly tied the 3080, and outperformed the 3070 Ti and 7900 XTX notably. The improvement over the 7900 XTX (watch our review) shows that AMD has actually executed on its RT performance uplifts.
But unfortunately for AMD, the 5070 remains superior with a 73 FPS AVG. It leads the 9070 by 55%.
At 1080p, the 9070 measured at 67 FPS AVG and pushed past the 3080. The 9070 XT leads by 9%. The 5070 leads the 9070 by 46%, so it’s slightly reduced, but still has an unbeatable lead.
Dragon’s Dogma 2 with ray tracing is next, tested at 4K first. We measured the 9070 at 56 FPS AVG, giving the 9070 XT a lead of 9%. The 4080 isn’t much past that, so for as close as these cards are, they’re also very close to the 4080 and 4080 Super. The 5070 trails in this one, giving the 9070 a lead of 15%.
At 1440p, the 9070 gets dangerously close to NVIDIA’s 5070 Ti, which failed to pass the 9070 XT in the previous review. The 5070 Ti leads the 9070 by 9%, the same lead as the 9070 XT has against it. The 5070 non-Ti is down at 83 FPS AVG, creating a 13% lead for the 9070.
This chart has entries for the 6700 XT (watch our review) and other prior GPUs.
At 1080p, the 9070 ran at 119 FPS AVG and led the 7900 XT by a few frames per second. The lead over the 5070’s 107 FPS AVG was 11%. The 9070 XT has a reduced lead over the non-XT at this resolution, now just 6% ahead of the cheaper card.
Dying Light 2 with upscaling at 4K is up now, this time with ray tracing. The 9070 XT leads the 9070 by a higher-than-average lead of 16% here. The 9070 ends up below the 5070 in this one also, with the 5070 producing a lead of 9% over AMD’s most relevant card. 
At 1440p, the 9070 runs at 78 FPS AVG and gives the 9070 XT a lead of about 10-11 FPS. The 9070 XT’s prior 16% lead is now reduced to 14%. The 5070 is also reduced to a 4% lead from a 9% lead at 4K. The 9070 was really struggling with the 4K resolution.
Finally for this game, 1080p puts the 5070 at just 1 FPS ahead now, so basically margin of error. The 9070 XT leads the 9070 by 13%.
In Resident Evil 4 with ray tracing and at 4K upscaled, the 9070 held a 104 FPS AVG and outperformed the 4070 Ti Super and 5070. The boost over the 5070 is 14% here. The 9070 XT ran 12% higher framerate than the non-XT, with the 5070 Ti basically tied with that.
This chart has a lot of cards lower down the stack, like the 2060 (watch our review), 3060 (watch our review), 2070 (watch our review), and 6700 XT for past generation comparisons.
At 1440p upscaled, the 9070 ran at 162 FPS AVG and roughly tied the prior 7900 XT. The 9070 XT was 11% ahead of the non-XT here, with the non-XT ahead of the 5070 by 9%.
Cyberpunk is up now, one of the two heavier titles we test for RT. We test this one at two groups of settings: Ultra and Medium, chosen because NVIDIA gains a disproportionate advantage at Ultra, while Medium pushes them closer together. That gives us the full picture.
At 4K and RT Ultra just to stress test it for relative scaling, the RX 9070 ran at 18 FPS AVG. Obviously this is unplayable, but this makes the point we made in our 5070 review: The 0.1% lows suffer on the 5070 in a big way. That’s because they’re indicating to us that underneath those lows, which really are just meant to be an indicator of a problem, there’s spiky and erratic frametime performance. This is caused by exceeding VRAM limitations on the 5070, which simply isn’t equipped enough to deal with these problems.
At 4K Medium, the 9070 boosted up to 24 FPS. No change in how playable it is, but even this lighter-weight workload is too much for the 5070 FE’s capacity, which you can again see in its lows.
The 9070 XT leads the 9070 by 15% here, one of the higher percent increases.
Reducing the resolution helps reach playable framerates. At 1080p and RT Ultra, the 9070 held 61 FPS AVG against the 9070 XT’s 71. That’s a relatively large advantage of 16% for the 9070 XT.
The RTX 5070 ran at 64 FPS here, outdoing the 9070 by just 5.7%.
Finally for games, the Cyberpunk 1080p/Medium results put the 9070 at 81 FPS AVG, roughly tying the 7900 XTX. This also puts it above the 3080. The 5070 ran at 81.9 FPS AVG and lows were within error of the 9070. There is no distinct advantage in lows for the 9070 here. Average is the same, which is a major positive for AMD when considering its previous RT deficit.
We’re moving on to power and efficiency testing now, which looks at both the total power consumption of the device during the test and the frame rate. The end result is that we can calculate the efficiency in performance per Watt. It’s actually a really exciting test for the 9070. 
In F1 24 at 4K and with RT, AMD has exceptional performance. These are only showing two decimals, but more are processed for the bar width. The 9070 roughly tied with the 4080 Super and 5090 FE (beware of scalped prices) as the most efficient GPU in our test suite.
The 5070 was at 0.17 FPS/W, giving the 9070 an advantage of 24%. That’s a huge swing in its favor. The 9070 XT was closer to the 5070.
At 1080p but with the same settings and game, the 9070 again ends up just behind first place. It’s tied with the 5070 Ti and just behind the 0.64 result of the 5080 (read our review). The 9070 XT burns more power and, although its performance is better, the tradeoff favors the 9070 here. The 5070 ends up way down the ranks, at 0.55 here.
Final Fantasy 4K is up next. This is one of AMD’s weaker showings given its limited performance scaling in Final Fantasy 14.
The 9070 pulled 224W during this test when measured at the PCIe cables and the slot. The 5070 pulled 10W more, but had better efficiency from its higher performance. The end result is that the 9070 outdid the 7900 XT for efficiency, but gave the 5070 a lead of about 18%. For reference, the 9070 XT pulled 310W here, roughly matching their TDPs.
At 1440p, the 9070 ranked at 0.56 FPS/W with a 224W draw. That has the 4060 as more efficient, the B570 slightly less efficient, and the 7900 XT also slightly less efficient. The NVIDIA 5070’s 0.65 FPS/W result outdoes the 9070 again here, but we’ll see if that persists outside of AMD’s weakest rasterization title.
Dragon’s Dogma 2 with RT shows exceptional efficiency for the RX 9070, at 224W again and 0.42 FPS/W, it’s looking pretty good overall. The performance is slightly lower than that of the 9070 XT, but the reduction in power really puts it in a better spot on the efficiency curve. It seems like this is more of the so-called “sweet spot” for power to performance. The 5070 is down at 0.36 FPS/W, despite pulling 231W.
In Starfield at 1440p, the 9070 is less impressive than in F1 and Dragon’s Dogma 2, but still overall good. Its result was 0.43 FPS/W, a large improvement on the 9070 XT’s 0.34. The 9070 ties the RTX 5070, which is a problem for NVIDIA because efficiency was one of NVIDIA’s main benefits with the 5070 Ti against the 9070 XT so if AMD is starting to chip that away, that’s bad news for NVIDIA. That’s kind of one of the key things the company has stood upon for being more expensive in the past. 
First, the 9070 gets credit for being relatively power efficient. It’s basically tied at the top in a few of our charts, though not all of them, but it’s looking better there. In our 9070 XT review, we said that AMD’s decision to favor performance over cutting power was the right move for the company right now. We think that remains true for the card that’s supposed to be its flagship, but for the 9070, the power cut helps create the difference between the cards to begin with, but the baseline performance is high enough that it ends up in a great efficiency position.
The 9070 XT ends up ahead of the 9070 by about 9-12% in most of the games we tested, with the 5070 generally giving the 9070 a lead in several games. Dragon’s Dogma 2 at 4K was playable on both, but the 9070 had about a 14% advantage over the 5070. The 9070 also led the 5070 by about 15% in Cyberpunk rasterized at 4K, with the 9070 XT ahead of the 9070 by 12%. There remains the one break-out for the 5070 in Final Fantasy, something we talked about in our 9070 XT review, but beyond that, for rasterization, the 9070 is ahead in basically every scenario.
Baldur’s Gate 3 even showed a large benefit at 4K for the 9070 versus the 5070 since we’re far enough away from the CPU bottleneck.
In ray tracing at 4K, the 9070 XT ended up around 9% to 19% higher framerate than the 9070, though that 19% number is based on low framerate, but even still, 17 is the next highest percentage. 1440p and 1080p weren’t that different and can be found in the charts section earlier. As for the 5070 in RT and 4K, the 9070 beat it by 15% in Dragon’s Dogma 2, 8% in F1 24, 6-7% in Cyberpunk, and 14% in Resident Evil 4. The 9070 lost in Black Myth and Dying Light 2 with RT. At 1440p and RT, the same losses exist. This points non XT and XT in a similar position to what we saw with the 7800 XT and the 7700 XT or the 7900 XT and 7900 XTX. The gap is small enough that most people will upsell themselves to the XT model. Weirdly, NVIDIA’s own fumbles have sort of made the proximity of the two cards less of a problem than they otherwise might have been. Because from AMD’s perspective, it’s either AMD or AMD. They probably don’t really care that much which version you buy, but the XT makes more sense than a 5070 does. The 5070 is already mostly irrelevant. NVIDIA’s strengths like DLSS, ray reconstruction, and Reflex all remain, of course, but we don’t think the bells and whistles are good enough on the 5070 to outweigh the 9070 or the 9070 XT. The biggest strength it probably has is CUDA, which we personally leverage for things like video editing. But for gaming, the 9070 and 9070 XT have really kicked the 5070’s ass (though it really kicked its own ass).
 Copyright © 2023 GamersNexus, LLC. All rights reserved.GamersNexus.net is Owned, Operated, & Maintained by GamersNexus, LLC. 
Copyright © 2023 GamersNexus, LLC. All rights reserved.GamersNexus.net is Owned, Operated, & Maintained by GamersNexus, LLC."
"AMD","RX 9070","AMD Radeon RX 9070 review: a great choice if the 9070 XT is out of reach","https://www.techradar.com/computing/gpu/amd-radeon-rx-9070","When you purchase through links on our site, we may earn an affiliate commission. Here’s how it works.

The AMD Radeon RX 9070 is a fantastic graphics card somewhat undercut by its market proximity to its more powerful sibling, the RX 9070 XT. But given the state of the GPU market this generation, this card might be a great compromise if you can't afford the mark-ups on the RX 9070 XT. 

Why you can trust TechRadar




We spend hours testing every product or service we review, so you can be sure you’re buying the best. Find out more about how we test.

The AMD Radeon RX 9070 is a card that just might be saved by the economic chaos engulfing the GPU market right now.
With 'normal' price inflation pretty much rampant with every current-gen GPU, the price proposition for the RX 9070 might actually make it an appealing pick for gamers who're experiencing sticker shock when looking for the best graphics card for their next GPU upgrade.
That doesn't mean, unfortunately, that the AMD RX 9070 is going to be one of the best cheap graphics cards going, even by comparison with everything else that's launched since the end of 2024. With an MSRP of $549 / £529.99 / AU$1,229, the RX 9070 is still an expensive card, even if it's theoretically in line with your typical 'midrange' offering.
And, with the lack of an AMD reference card that might have helped anchor the RX 9070's price at Team Red's MSRP, you're going to pretty much be at the mercy of third-party manufacturers and retailers who can charge whatever they want for this card.
Comparatively speaking, though, even with price inflation, this is going to be one of the cheaper midrange GPUs of this generation, so if you're looking at a bunch of different GPUs, without question this one is likely to be the cheapest graphics card made by either AMD or Nvidia right now (yes, that's even counting the RTX 5060 Ti, which is already selling for well above 150% of MSRP in many places).
Does that make this card worth the purchase? Well, that's going to depend on what you're being asked to pay for it. While it's possible to find RX 9070 cards at MSRP, they are rare, and so you're going to have to make a back-of-the-envelope calculation to see if this card is going to offer you the best value in your particular circumstance.
I'm fairly confident, however, that it will. Had I the time to review this card when it first launched in March, I might have scored it lower based on its performance and price proximity to the beefier AMD Radeon RX 9070 XT.
Looking at both of those cards based on their MSRPs, there's no question that the RX 9070 XT is the much better graphics card, so I'd have recommended you spend the extra cash to get that card instead of this one.
Unfortunately, contrary to my hopes, the RX 9070 XT has been scalped almost as badly as the best Nvidia graphics cards of this generation, so that relatively small price difference on paper can be quite large in practice.
Given that reality, for most gamers, the RX 9070 is the best 1440p graphics card going, and can even get you some solid 4K gaming performance for a lot less than you're likely to find the RX 9070 XT or competing Nvidia card, even from the last generation.
If you're looking at this card and the market has returned to sanity and MSRP pricing, then definitely consider going for the RX 9070 XT instead of this card. But barring that happy contingency, given where everything is right now with the GPU market, the RX 9070 is the best AMD graphics card for 1440p gaming, and offers some of the best bang for your (inflationary) buck as you're likely to find today.
The AMD Radeon RX 9070 is available now in the US, UK, and Australia for an MSRP of $549 / £529.99 / AU$1,229, respectively, but the price you'll pay for this card from third-party partners and retailers will likely be higher.
Giving credit where it's due, the RX 9070 is the exact same MSRP as the AMD Radeon RX 7900 GRE, which you can argue the RX 9070 is replacing. It's also coming in at the same price as the RTX 5070's MSRP, and as I'll get into in a bit, for gaming performance, the RX 9070 offers a better value at MSRP.
Given how the RTX 5070 can rarely be found at MSRP, the RX 9070 is in an even stronger position compared to its competition.
The AMD RX 9070 is built on the new RDNA 4 architecture using TSMC's N4P process node (interestingly, it's built with a monolithic GPU die, rather than the MCM design of RDNA 3), so even though the RX 9070 contains fewer compute units than the 6nm/5nm RX 7900 GRE, they're more advanced.
The 56 compute units contain 64 shaders, one ray tracing processor, and two AI processors each, giving the RX 9070 a total of 3,584 shaders, 56 ray accelerators, and 112 AI accelerators. While I'll get into the actual performance of this hardware in a bit, according to my testing, the AMD ray accelerators appear to have finally caught up with Nvidia's RT cores.
The RX 9070 also sports 16GB GDDR6 VRAM on a 256-bit memory bus, which is more than enough for 4K gaming while keeping the price of the RX 9070 down.
Finally, the RX 9070 utilizes 16 PCIe 5.0 lanes, so if you've got a PCIe 5.0 SSD, make sure your motherboard has enough PCIe 5.0 lanes to support both components at their full bandwidth, otherwise your SSD performance might revert to using PCIe 4.0.
In terms of design, the RX 9070 doesn't have a reference card, so the card I reviewed is the Sapphire Pulse Radeon RX 9070.
This card, in particular, is fairly straightforward with few frills, but for those who don't want a whole lot of RGB lighting in their PC, this is more of a positive than a negative. RGB fans, however, will have to look at other AMD partner cards for their fix.
The card is a noticeably shorter dual-fan design compared to the longer triple-fan RX 9070 XT cards. That makes the RX 9070 a great option for small form factor PC cases.
The charts shown below offer the most recent data I have for the cards tested for this review. They may change over time as more card results are added and cards are retested. The 'average of all cards tested' includes cards not shown in these charts for readability purposes.
When it comes down to performance, the RX 9070 is a very strong graphics card that is somewhat overshadowed by its beefier 9070 XT sibling, but goes toe-to-toe against the RTX 5070 where it counts for most users, which is gaming.
On the synthetic side, the RTX 9070 puts up some incredibly solid numbers, especially in pure rasterization workloads like 3DMark Steel Nomad, beating out the RTX 5070 by 13%. In ray tracing heavy workloads like 3DMark Speed Way, meanwhile, the RX 9070 manages to comes within 95% of the RTX 5070's performance.
As expected though, the RX 9070's creative performance isn't able to keep up with Nvidia's competing RTX 5070, especially in 3D modeling workloads like Blender. If you're looking for a cheap creative workstation GPU, you're going to want to go for the RTX 5070, no question.
But that's not really what this card is about. AMD cards are gaming cards through and through, and as you can see above, at 1440p, the RX 9070 goes blow for blow with Nvidia's midrange card so that the overall average FPS at 1440p is 114 against Nvidia's 115 FPS average (72 FPS to 76 FPS average minimums/1%, respectively).
Likewise, at 4K, the two cards are effectively tied, with the RX 9070 holding a slight 2 FPS edge over the RTX 5070, on average (50 FPS to 51 FPS minimum/1%, respectively).
Putting it all together, one thing in the Nvidia RTX 5070's favor is that it is able to tie things up with the RX 9070 at about 26 fewer watts under load (284W maximum power draw to the RTX 5070's 258W).
That's not the biggest difference, but even 26W extra power can mean the difference between needing to replace your PSU or sticking with the one you have.
Under normal conditions, I'd argue that this would swing things in favor of Nvidia's GPU, but the GPU market is hardly normal right now, and so what you really need to look at is how much you're being asked to pay for either of these cards. Chances are, you're going to be able to find an RX 9070 for a good bit cheaper than the RTX 5070, and so its value to you in the end is likely going to be higher.
At MSRP, this card is too close to the RX 9070 XT to recommend, but given the market, it might be the best value in the midrange. 
While it doesn't use GDDR7 like the RTX 5070, it does feature 16GB VRAM and PCIe 5.0.
The Sapphire Pulse RX 9070 I reviewed is fairly plain, but for those who hate all the blinking lights, it'll be just fine, especially in a small form factor case.
This card goes toe-to-toe with the RTX 5070 in 1440p and 4K gaming performance, only falling behind Nvidia's card in terms of creative performance and power consumption.
If I'd review this card when it first launched, I might have reviewed it lower, but given the inflated prices in the GPU market right now, this card might be the best value you'll find in the midrange.
You want a fantastic 1440p graphics cardThe RX 9070 absolutely chews through 1440p gaming with frame rates that can fully saturate most 1440p gaming monitors' refresh rates.
You don't want to spend a fortune on a midrange GPUWhile the RX 9070 isn't cheap, necessarily, it's among the cheapest midrange cards you can get, even after factoring in scalping and price inflation.
You want great creative performanceWhile the RX 9070 is a fantastic gaming graphics card, its creative performance (especially for 3D modeling work) lags behind Nvidia midrange cards.
AMD Radeon RX 9070 XTThe RX 9070 XT is an absolute barnburner of a gaming GPU, offering excellent 4K performance and even better 1440p performance, especially if you can get it close to MSRP.
Read our full AMD Radeon RX 9070 XT review
Nvidia GeForce RTX 5070The RTX 5070 essentially ties the RX 9070 in gaming performance in 1440p and 4K gaming, but has better power efficiency and creative performance.
Read our full Nvidia GeForce RTX 5070 review
Here are the specs on the system I used for testing:
Motherboard: ASRock Z790i Lightning WiFiCPU: Intel Core i9-14900KCPU Cooler: Gigabyte Auros Waterforce II 360 ICERAM: Corsair Dominator DDR5-6600 (2 x 16GB)SSD: Samsung 9100 Pro 4TB SSDPSU: Thermaltake Toughpower PF3 1050W PlatinumCase: Praxis Wetbench
I spent about two weeks with the AMD RX 9070, using it as my primary workstation GPU for creative work and gaming after hours.
I used my updated benchmarking process, which includes using built-in benchmarks on the latest PC games like Black Myth: Wukong, Cyberpunk 2077, and Civilization VII. I also used industry-standard benchmark tools like 3DMark for synthetic testing, while using tools like PugetBench for Creators and Blender Benchmark for creative workload testing.
I've reviewed more than three dozen graphics cards for TechRadar over the past three years, which has included hundreds of hours of dedicated GPU testing, so you can trust that I'm giving you the fullest picture of a graphics card's performance in my reviews.
John (He/Him) is the Components Editor here at TechRadar and he is also a programmer, gamer, activist, and Brooklyn College alum currently living in Brooklyn, NY. 
Named by the CTA as a CES 2020 Media Trailblazer for his science and technology reporting, John specializes in all areas of computer science, including industry news, hardware reviews, PC gaming, as well as general science writing and the social impact of the tech industry.
You can find him online on Bluesky  @johnloeffler.bsky.social
Please logout and then login again, you will then be prompted to enter your display name.

TechRadar is part of Future US Inc, an international media group and leading digital publisher. Visit our corporate site.

©
Future US, Inc. Full 7th Floor, 130 West 42nd Street,
New York,
NY 10036."
"AMD","RX 9070","AMD Radeon RX 9070 release date window, rumored price and specs","https://www.pcgamesn.com/amd/radeon-rx-9070-guide","The new AMD Radeon gaming GPU has now been unleashed into the wild, where the graphics card squares up to the Nvidia GeForce RTX 5070.


                    Ben Hardwidge                

The full details of the new AMD Radeon RX 9070 gaming GPU have been officially revealed, and the new GPU seriously threatens the Nvidia GeForce RTX 5070, which has the same MSRP. The new graphics card makes big strides when it comes to ray tracing performance, and it also supports AMD's new fine-looking FSR 4 upscaling tech to improve frame rates without making your games look like a blurry mess.

You can now read all about this new GPU by reading our full AMD Radeon RX 9070 review, where we found a graphics card that generally beats the RTX 5070, although the latter still has the upper hand when it comes to path tracing. The Radeon RX 9070 now also has a place on our best graphics card guide, although its main competitor is arguably its own bigger sibling, the 9070 XT.


The AMD Radeon RX 9070 release date was Thursday, March 6, 2025, meaning it went head to head with the Nvidia GeForce RTX 5070, which comes out in the same week. AMD said there would be ""wide availability"" of the new GPU on this date, and while there does appear to have been lots of stock released, it's currently hard to find in stock.


The Radeon RX 9070 price is $549, which is exactly the same as the price of the 12GB Nvidia GeForce RTX 5070. With more VRAM at its disposal, and generally faster performance, the 9070 is generally the better buy. However, the price is also only $50 less than the Radeon RX 9070 XT price, which means it's also threatened by AMD's own top-end chip.


The AMD Radeon RX 9070 specs include 16GB of VRAM, 56 compute units, and a 2.54GHz boost clock speed.


This new lineup of GPUs sees AMD moving to the new RDNA 4 GPU architecture, which AMD says not only improves ray tracing and AI performance, but can also hit higher clock speeds than equivalent last-gen GPUs. The 9070 contains 56 of AMD's new RDNA 4 compute units, compared to 64 in the 9070 XT.


That's four fewer than the 60 compute units in the Radeon RX 7800 XT, but the substantial improvements to the compute unit design make up this shortfall. At stock speed, the RX 9070 also has a top boost clock of 2.54GHz, which is a fair way below the 2.97GHz of the 9070 XT, but still higher than the 2.43GHz of the 7800 XT.


One of the key new elements with this GPU design is, of course, the new ray tracing core, which AMD says is significantly quicker than the old RDNA 3 design, doubling the ray tracing throughput, and we saw big improvements in our own tests. This generation also sees AMD properly discussing path tracing in its marketing materials, and it can definitely now handle path tracing in Cyberpunk 2077, although we found it struggles with the Full RT settings in Indiana Jones and the Great Circle.


Importantly, unlike the RTX 5070, the Radeon RX 9070 also comes with 16GB of VRAM. It's not GDDR7, but in our tests we've found that VRAM capacity is becoming more and more important as you start enabling advanced features such as path tracing, even in games that are out already.


As a case in point, you can't max out the settings in Indiana Jones and the Great Circle on a 12GB card, as we found in our GeForce RTX 5070 review. Even at 1080p, as it pushes the VRAM to its limit and frame rates fall off a cliff.


The Radeon RX 9070 has one RT core per compute unit, so you get 56 of them in the new GPU. That's lower than the 60 in the Radeon RX 7800 XT, but the new RT core design will hopefully make up for the shortfall in numbers. The other key new feature with RDNA 4 is an apparently greatly-improved AI core architecture, with AMD introducing a new design of its matrix cores (AMD's equivalent of Nvidia's Tensor cores).


Going head to head with Nvidia, AMD cites a peak AI performance figure of 1,165 TOPS, compared to 988 for the RTX 5070, though it's always best to take companies' own claims for these figures with a grain of salt until we can properly test them.


The big deal here for gamers is the introduction of AMD's first resolution upscaling tech based on machine learning, and requiring the use of AMD's new matrix cores in the 9070 and 9070 XT. It's called FSR 4, and it massively improves image quality compared to FSR 3, with a significantly sharper image, and much less ghosting and blurriness. You can read our FSR 4 test, where we played with the tech in Ratchet and Clank at CES, for more information about it.


FSR 4 is potentially a big deal for AMD. Historically, FSR has seriously lagged behind DLSS when it comes to image quality, particularly now it's up against Nvidia DLSS 4 with the new transformer model, and FSR 4 looks set to close that gap.


It all depends on game support, though, and AMD says that FSR 4 will be supported in 30 games at launch, including Marvel Rivals, Call of Duty Black Ops 6, and Space Marine 2, with over 75 more FSR4 games promised to come later in 2025. AMD says that its new AI cores are also ""neural rendering ready"" and cites neural radiance cache as an example – a tech that looked really promising when we tested neural rendering in Half-Life 2 RTX at CES.


As we mentioned earlier, though, the biggest problem for the Radeon RX 9070 is the fact that the 9070 XT only costs $50 more, and is substantially more powerful. You can check out our full Radeon RX 9070 XT review to see how this GPU copes with our current game benchmark suite.


You can also follow us on Google News for daily PC hardware news, reviews, and guides, or join our community Discord to stay in the know.





                    Ben Hardwidge                 A tech journalist since 1999, and a PC hardware enthusiast since 1989, Ben has seen it all, from the horrors of CGA graphics to the awesome power of the RTX 5090 today. Ben is mainly interested in the latest CPU and graphics tech, and currently spends most of his evenings playing Oblivion Remastered, while marveling at how much better it looks than the original game did on his old GeForce 6600 GT."
"AMD","RX 9070","AMD Radeon RX 9070 and 9070 XT Announced","https://www.servethehome.com/amd-radeon-rx-9070-and-9070-xt-announced/","AMD is announcing two new GPUs as part of the new AMD Radeon RX 9000 family that will be available on March 6, 2025. The new GPUs bring with them AMD’s RDNA 4 architecture and are set to be a cost-competitive GPU option. We wish there was a bit more than 16GB of memory as an option, however.
Here are the specs on the new GPUs. The AMD Radeon RX 9070 XT is the higher-end part while the RX 9070 is the lower spec part of the two.
AMD has the new RDNA 4 architecture with its new generation compute unit.
RGNA also has the company’s 3rd generation raytracing accelerators.
It also has the company’s 2nd Gen AI accelerators with features like FP8 formats and faster performance.
AMD says the new GPUs are faster than the RX 7900 series. The Creator AI performance may not seem like a big deal, but those features are being released at a very quick pace.
AMD is pushing Generative AI even on its consumer gaming GPUs because it is becoming a bigger part of the computing paradigm.
AMD is also working on optimizing models for its GPUs.
Another feature beyond the AI performance, generational gaming performance gains, and such is an enhanced media engine.
AMD did not give pricing, but it seems to be launching between the RX 7900 GRE and the RX 7900 XT. Edit: The AMD Radeon RX 7090 has a $549 MSRP while the Radeon RX 7090 XT has a $599 MSRP. For our money, we are strongly looking at the RX 9070 XT.
That should make it a neatly priced set of GPUs.
We are still about a week or so away from the availability of the Radeon RX 9070 GPUs. We should get an update on performance around the launch. Still, it is important that AMD is a strong competitor to NVIDIA across GPU segments to keep driving competition in the market.
“AMD did not give pricing, but it seems to be launching between the RX 7900 GRE and the RX 7900 XT. The AMD Radeon RX 7090 has a $549 MSRP while the Radeon RX 7090 XT has a $599 MSRP.“
Tyree:  “I didn’t have s3x with Katie.  Correction: I had s3x with Katie.”
Cliff:  They call me “the STH news guy” for a reason.
Readers:  Well we don’t call you editor ;)
Save my name, email, and website in this browser for the next time I comment.
This site uses Akismet to reduce spam. Learn how your comment data is processed.

  Get the best of STH delivered weekly to your inbox. We are going to curate a selection of the best posts from STH each week and deliver them directly to you.


  By opting-in you agree to have us send you our newsletter. We are using a third party service to manage subscriptions so you can unsubscribe at any time."
"AMD","RX 9070","ASUS Radeon RX 9070 XT TUF OC Review","https://www.techpowerup.com/forums/threads/9070xt-which-one.337123/",""
"AMD","RX 9070","The AMD Radeon RX 9070 XT gaming GPU is finally here to take on the RTX 5070","https://www.pcgamesn.com/amd/radeon-rx-9070-xt-announced","Using new AMD RDNA 4 architecture, the 9070 XT and 9070 will compete with Nvidia's mid-range graphics cards, with RX 9060 to follow soon.


                    Edward Chester                

AMD has finally unveiled its much-anticipated new flagship graphics cards, the AMD Radeon RX 9070 XT and AMD Radeon RX 9070. Based on a new AMD RDNA 4 graphics architecture, they include much-improved ray tracing performance and arrive alongside FSR 4, a new version of the company's upscaling tech. As long rumored, though, they won't have the power to compete with the RTX 5090 but are instead pitched at competing with Nvidia's upcoming mid-range RTX 5000 GPUs.

Unveiled during its press conference at the CES 2025 trade show, AMD's new best graphics card contenders were also shown alongside a tease for its upcoming RX 9060 GPUs that will be coming later to compete for the budget end of the GPU market.


The AMD Radeon RX 9070 XT is the top-tier card that AMD's announcing today and while the reveal confirms several details from previous AMD Radeon RX 9070 XT leaks, many details of the new card and its GPU have yet to be released. As such, we still don't have confirmation that the card will use the rumored Navi 48 GPU or exactly how many compute units the GPU contains.


What we do know, though, is that the new card will use the company's new RDNA 4 architecture, which the company claims has been ""built from the ground up […] for performance and immersion with a significant boost in AI."" To this end, the company has overhauled the 2nd-gen AI accelerators on the GPU, adding ""plenty of capabilities to drive more AI experiences,"" though AMD hasn't provided any stats to back up these claims.


The company has also confirmed it has significantly improved this architecture's ray tracing ability, though again gives no specific numbers. Previous leaks have pointed to the ray tracing power of this card being a rival to that of the RTX 4080, which would make for a significant step up from the company's RDNA 3 architecture, but leaves the door open for Nvidia to once again take the lead on this front with its new RTX 5000 series cards.


AMD also claims the new architecture has optimized compute units along with increases to clock speed and instructions per clock compared to its previous architecture. It has also confirmed that the new cards will be built on a 4nm process.


The company confirmed that cards would be manufactured by a host of add-in board (AIB) partners, such as Acer, Asus, ASRock, Gigabyte, Powercolor, Sapphire, and Yeston, and its images suggest the company will also release its own versions of the cards, as they did with the RX 7900 XTX, for instance. You can see AMD's cards in the center and center-right of the above image.


One detail that AMD did discuss is the new naming scheme for these cards. Along with skipping past RX 8000 and straight onto RX 9000 – to align with its CPU number scheme – AMD is switching two of the digits of its model numbers around compared to previous generation cards, so instead of the RX 7700 XT, for instance, the new card will be the RX 9070 XT. Were AMD to have an equivalent of the RX 7900 XTX in this generation, it would be called something like the RX 9090 XTX in this new scheme.


Interestingly, AMD verified that this digit switch was to make for easier direct comparison to Nvidia's competing cards. So, yes, the RX 9070 XT should line up in terms of performance with the RTX 5070.


Meanwhile, while talking about its new naming scheme, AMD also revealed that the second tier range of cards in the 9000 series will be the RX 9060, with these cards set to compete with upcoming Nvidia GeForce RTX 5060 cards.


Slightly disappointingly, AMD is targeting a broad Q1 2025 AMD Radeon RX 9070 XT release date, which could mean these cards won't arrive as soon as some previous rumors suggested. We also won't hear more about the details of the cards until ""later in the quarter."" That said, AMD does mention that it will be previewing its new GPUs at CES 2025, and we're hoping to soon see them in action on the show floor.





                    Edward Chester                 Edward Chester writes hardware reviews, guides, and news for PCGamesN. He has reviewed every bit of PC gaming hardware you can think of, and many you probably can't, during his 18 years in the industry and currently focuses on gaming monitors, gaming mice, and other gaming peripherals. He has been a PC gamer since the mid-90s and still just about manages to find time to indulge in his decades-long passion. Currently a recovering Apex Legends player who has since moved to Fortnite Zero Build and Marvel Rivals while working his way through the stunning Kingdom Come: Deliverance 2 and is looking forward to Doom: The Dark Ages. When not gaming he's an avid cook, musician, gardener, and woodworker."
"AMD","RX 9070","The best 1440p graphics cards in 2025: my top picks for midrange GPUs","https://www.techradar.com/news/best-1440p-graphics-cards-2019-the-best-gpus-for-1440p-gaming","These are the 1440p graphics cards for high refresh rate gaming

When you purchase through links on our site, we may earn an affiliate commission. Here’s how it works.

If you're looking to upgrade your PC with the best 1440p graphics card you can find in 2025, you have a number of solid choices depending on how much you want to spend.
With the best graphics cards of this generation trending toward the more premium end of the market, there are fewer 1440p options available, but fortunately, they all offer incredible 1440p gaming.
My top pick, the AMD Radeon RX 9070, offers the best balance of performance and price in the market, capable of saturating the refresh rates of many of the best gaming monitors with 1440p resolution for incredibly smooth gaming.
On the lower end of the price scale, the Intel Arc B580 puts up some very impressive 1440p frame rates at a much lower price than its competition, while the AMD Radeon RX 9070 XT offers up the best possible 1440p performance in this class, but you need to pay a rather high premium for it.
With the graphics card market being what it is, it's important to make sure that your next upgrade offers you the best value for what you're paying for it, especially given how price inflation is making it more expensive than ever.
That's why I'm leveraging my years of experience and hundreds of hours of GPU testing data to help you find the best 1440p graphics card for your needs and budget.
Value | ★★★★☆Features | ★★★★☆Power | ★★★★★
The AMD RX 9070 battles Nvidia's best 1440p graphics card in gaming performance while also being more competitively priced, but it uses more energy to do so, and its creative performance isn't great. For 1440p gaming though, it's outstanding.
Value | ★★★★★Features | ★★★★☆Power | ★★★★★
While the Intel Arc B580 doesn't offer the same level of performance as the AMD RX 9070 or Nvidia RTX 5070, it still gets you very good 1440p gaming performance at an unbeatable price.
Value | ★★★★★Features | ★★★★☆Power | ★★★★★
The AMD Radeon RX 9070 XT is the best graphics card of this generation overall, but now that price inflation is ravaging this card online, you should only consider it if you're ok with paying the high premium for the extra performance over the RX 9070.

Why you can trust TechRadar




We spend hours testing every product or service we review, so you can be sure you’re buying the best. Find out more about how we test.

Below you'll find full write-ups for each of the best 1440p graphics card picks on our list. I've tested each one extensively, so you can be sure that my recommendations can be trusted.
The AMD Radeon RX 9070 is a card made for gamers wanting fantastic 1440p play and usable 4K without paying the premiums being charged for the RX 9070 XT and Nvidia RTX 5070.
My benchmarks show it trails the RX 9070 XT by about 13 percent, but matches Nvidia’s RTX 5070 frame-for-frame in gaming, averaging 114 fps at 1440p and edging out the rival RTX 5070 at 4K, though it draws roughly 26 watts more power under load to do so (despite the listed 220W TBP on the RX 9070, my testing showed a peak power draw of 284W).
The card’s 16GB GDDR6 VRAM, 256-bit memory bus, and PCIe 5.0 interface give it modern bandwidth without the price bump of GDDR7, but the PCIe 5.0 x16 interface on the RX 9070 might conflict with a PCIe 5.0 SSD if your motherboard doesn’t have enough lanes to support both components, so keep that in mind when looking at any current-gen GPU.
This card’s MSRP sits at $549, but real-world prices will creep upward as AMD board partners like Asus, Gigabyte, XFX, and others fill the void left by the absent AMD reference model, so this card’s ultimate value will depend on local pricing. Even so, the RX 9070 is likely to continue undercutting the inflated pricing on the RTX 5070 and RX 9070 XT, making it the best mid-range pick for pure gaming by value.
Creative workloads lag similarly priced Nvidia cards, but the efficient dual-fan design fits small-form-factor cases, runs quietly, and still delivers the modern connectivity gamers expect, making it an easy recommendation when budget and availability collide during this volatile GPU cycle.
The Intel Arc B580 has quickly established itself as the top budget graphics card for 1440p gaming in 2025, delivering performance that punches way above its price tag.
The B580 defied many people’s expectations (mine included) with smooth 1440p frame rates in modern titles like Cyberpunk 2077 and F1 2024—something AMD and Nvidia haven’t achieved at this price point in years.
Its performance at 1440p isn’t as good as competing the best Nvidia graphics cards or best AMD graphics cards in this segment, the 12GB GDDR6 VRAM and 192-bit memory bus provide more than enough headroom for 1440p gaming, delivering an average 92 FPS across the 8-10 PC titles I tested for my review.
What's more, Intel has learned from its earlier Arc Alchemist missteps, and earlier driver instability and compatibility issues were non-existent in my testing, making this card not just affordable, but reliable in ways Intel’s last-gen cards weren’t.
While it doesn’t lead in ray tracing or creative workloads, the Arc B580 is aimed squarely at gamers on a budget, and in that lane, it dominates at this price. For the vast majority of gamers looking to game at 1440p without draining their wallet, the Arc B580 delivers incredible value you won’t find elsewhere in a 1440p GPU.
After spending weeks testing the AMD Radeon RX 9070 XT, it stands out as the top choice for anyone chasing a “money-no-object” 1440p gaming PC build.
Gaming frame rates at 1440p hover just a hair below the GeForce RTX 4080, yet the card launches at roughly half that rival’s MSRP, and even with price inflation, you can find the RX 9070 XT for much less than either the RTX 4080 or RTX 4080 Super, turning what should be a premium-tier GPU into genuine value.
I'd hoped when I reviewed it that AMD could avoid the supply-chain chaos and scalping that's plagued Nvidia's Blackwell cards, but the 9070 XT consistently sells for higher than its $599 MSRP. Even with that, it still sells for substantially less than Nvidia's RTX 4080 cards, making it a phenomenal value overall.
Peak power draw touched 309 W—hefty, but well aligned with the performance on tap—so plan on a roomy case and a stout PSU; compact rigs need not apply.
Creative and AI workloads remain Nvidia’s domain, so creatives can skip this card, but if your priority is silky-smooth 1440p gaming, the RX 9070 XT is unmatched—if you can afford it.
Read the full AMD Radeon RX 9070 XT review
The best 1440p graphics card on the market right now is the AMD Radeon RX 9070 for its powerful performance and competitive price point.
Yes, 1440p is excellent for gaming in 2025 and is generally favored by more PC gamers when compared to 1080p and more expensive 4K alternatives. 
Yes, QHD refers to Quad HD as the resolution is 2560 x 1440p. You may also hear it referred to as 2K, but hardware manufacturers usually prefer the former terminology. 
You need to consider your graphical demands. You shouldn't settle for what you can afford right now if it's not powerful enough to handle your 1440p needs. You're better putting it off for a bit and saving up until you can afford the GPU that's the right fit.
You need to take a look at the vital specs: GPU memory, GPU size, Thermal Design Power or TDP, and ports and power connectors are all important.
If you want the best ray tracing experience, Nvidia still maintains its ray tracing lead, but AMD has all but closed the ray tracing gap in gaming.
In our graphics card reviews, we maintain a consistent testing process. This includes evaluating their performance through a set of 8-10 games, tested across various resolutions and with the latest drivers.
Moreover, each time a new card is released, we retest all current-generation cards with the same hardware - identical processors, memory speeds, motherboards, and SSDs. This standardization is key to accurately measuring each card's performance and providing reliable comparisons with its rivals.
Our comprehensive reviews also detail each card's power consumption and temperature when under load. Additionally, we assess the features of each graphics card, though these features generally show little variation from one generation to the next.
Read more about how we test graphics cards at TechRadar
Sign up for breaking news, reviews, opinion, top tech deals, and more.
John (He/Him) is the Components Editor here at TechRadar and he is also a programmer, gamer, activist, and Brooklyn College alum currently living in Brooklyn, NY. 
Named by the CTA as a CES 2020 Media Trailblazer for his science and technology reporting, John specializes in all areas of computer science, including industry news, hardware reviews, PC gaming, as well as general science writing and the social impact of the tech industry.
You can find him online on Bluesky  @johnloeffler.bsky.social

TechRadar is part of Future US Inc, an international media group and leading digital publisher. Visit our corporate site.

©
Future US, Inc. Full 7th Floor, 130 West 42nd Street,
New York,
NY 10036."
"AMD","RX 9070","The best graphics card in 2025: our picks for all budgets","https://www.techradar.com/news/computing-components/graphics-cards/best-graphics-cards-1291458","These are all the best graphics cards available now

When you purchase through links on our site, we may earn an affiliate commission. Here’s how it works.

Finding the best graphics card in 2025 can be a challenge, especially given the overwhelming variety of options on the market.
From affordable choices like the Intel Arc B580 to 4K monsters like the Nvidia GeForce RTX 5090, there's no shortage of potential upgrades for your PC, and over the last several years at TechRadar, I’ve spent hundreds of hours testing numerous GPUs to evaluate gaming prowess, creative performance, and overall bang for your buck—chalking up more than 36 in-depth graphics card reviews in the last three years alone.
This hands-on experience helps me cull though all of the top contenders on the market for both gamers and content creators according to both raw performance and real-world value.
Right now, my favorite pick for an ideal blend of value and power is the AMD Radeon RX 9070 XT, especially for gamers who are frustrated by the rampant price inflation for premium cards. This GPU isn't cheap, but it's 4K performance outpaces its price, even after inflation, making it the one I'd recommend to just about anyone looking for their next upgrade.
For all of the cards on this list, I dig deep into factors like energy usage, performance benchmark results, form factor and design, and price to determine the true standouts.
With more than 200 graphics card reviews published over TechRadar's nearly two decades of work, me and my colleagues know exactly how to steer you toward the best graphics card for your individual needs and budget, backed by data and extensive reviews, so you can feel confident in my picks.
Value | ★★★★★Features | ★★★★☆Power | ★★★★★
AMD’s Radeon RX 9070 XT debuts the company’s new GPU generation with performance rivaling the RTX 4080 at half its original MSRP. Though not ideal for creatives or AI, it’s a gamer’s dream.
Value | ★★★★★ Features | ★★★★☆ Power | ★★★★☆
The Intel Arc B580 is a stunning GPU for it's price. More than capable of high-performance 1440p gameplay, Intel's ""midrange"" GPU comes in at a lower price than both AMD and Nvidia's cheapest 1080p graphics cards.
Value | ★★★★☆Features | ★★★★★Power | ★★★★★
The Nvidia RTX 5070 Ti is the best Nvidia graphics card of this generation overall, thanks to its outstanding performance and (relatively) reasonable price, though finding it at MSRP might be a challenge.
Value | ★★★★☆ Features | ★★★★☆ Power | ★★★★☆
While the Nvidia RTX 5080 doesn't have the same level of performance as the Nvidia RTX 5090, it's still plenty powerful as a 4K gaming GPU, managing close to RTX 4090 performance in gaming for significantly less.
Value | ★★★☆☆ Features | ★★★★☆ Power | ★★★★★
The RTX 5090 is an absolute powerhouse of a GPU when it comes to creative work, chewing through 3D modeling, video editing, and more with ease. Its high price and power consumption puts it out of reach for just about everybody, though.
Value | ★★★★☆ Features | ★★★★★ Power | ★★★★★
While the RTX 4070 Super doesn't pack the same performance as the best 4K cards, its 1440p performance is outstanding, and Nvidia's DLSS 3 is a true game-changer.
Value | ★★★★☆ Features | ★★★★☆ Power | ★★★☆☆
The Intel Arc B570 isn't the most powerful graphics card out there, but for 1080p performance, it's a very solid choice, especially for those on a tighter budget.
Value | ★★★★☆ Features | ★★★★★ Power | ★★★★★
The Nvidia RTX 4090 is undeniably the most powerful GPU of the last-gen, but its price makes it prohibitive for most, though its performance-per-dollar makes it among the best values going for premium GPUs, especially for creatives.
Value | ★★★★☆ Features | ★★★★★ Power | ★★★★★
The AMD Radeon RX 7900 XTX is an absolute beast of a gaming GPU that holds its own against the best current generation graphics cards, though its price is going to be prohibitive for many and its creative performance can't compete with Nvidia's 
This guide was updated on March 18, 2025, to add the AMD Radeon RX 9070 XT as the best graphics card, the Nvidia RTX 5070 Ti as the best Nvidia graphics card, the Nvidia RTX 5080 as the best gaming graphics card, the Nvidia RTX 5090 as the best graphics card for creatives, and the Intel Arc B570 as the best 1080p graphics card, along with updated benchmark data.
John has been working with computers since he was a teenager, long before he ever started writing about computer hardware or earning a Master's degree in Computer Science. Needless to say, he knows computers inside and out, and he has personally tested (and retested) all of the graphics cards on this page, having reviewed more than 35 graphics cards for TechRadar in his five years with the site. He has validated all of the results you'll find here in this guide, backed up by hundreds of hours of testing in the TechRadar computer lab over the last three years.

Why you can trust TechRadar




We spend hours testing every product or service we review, so you can be sure you’re buying the best. Find out more about how we test.

Below, you'll find detailed write-ups for each of the best graphics card picks on this list. I've extensively tested each of them and have the gaming and other performance data you need to make the right choice for your needs and budget.
✅ You want the best value proposition for a high-end graphics cardThe AMD RX 9070 XT punches way above its price point with outstanding performance at 4K and 1440p.✅ You don't want to pay inflated prices for an Nvidia GPUPrice inflation wrecking the market for Nvidia GPUs right now, but the RX 9070 XT offers a better value proposition than Nvidia's latest RTX offerings.
❌ You're on a tight budgetIf you don't have a lot of money to spend, this card is likely more than you need.❌ You need strong creative or AI performanceWhile AMD is getting better at creative and AI workloads, it still lags far behind Nvidia's competing offerings.
I spent about two weeks testing the AMD Radeon RX 9070 XT for my review of AMD's latest flagship GPU, and it’s without question one of the best gaming GPUs I’ve ever used.
With performance that nearly matches the Nvidia GeForce RTX 4080—at half the launch price—it’s an unbeatable deal. AMD’s always had fewer supply issues and less scalping compared to Nvidia, so I expect the RX 9070 XT to remain accessible at around its $599 MSRP, which should be a huge relief for anyone weary of overpriced, understocked cards.
In my testing, I saw power draws of up to 309W, which is high, but given the performance level for this card, I won't say it's out of bounds. You'll need to make sure that your case can fit a triple-fan card and supply it with enough power, so it might not be ideal for small form factor builds. It also won’t blow you away with creative or AI tasks if my test results are any indication, but for pure gaming, it's fantastic.
In my testing, the RX 9070 XT got within 7% of the RTX 4080 and beat out every single last-gen AMD card, delivering high frame rates in some very demanding games, and providing a smooth gaming experience overall, even at 4K in many cases. Its fantastic value, especially in a graphics card generation that is largely lacking in excellent price-for-performance offers, makes this not only a massive breath of fresh air for the market but a new GPU offering that any PC gaming fan should be excited about.
Read the full AMD Radeon RX 9070 XT review
Nvidia GeForce RTX 5070 TiWhile slightly more expensive, the RTX 5070 Ti is a better all-around performer, making it worth the extra money, especially if you're keen to do more than game on it.
Read our in-depth Nvidia RTX 5070 Ti vs RX 9070 XT analysis
AMD Radeon RX 9070While not as powerful, the Radeon RX 9070 is still a fantastic 1440p performer at a lower price point than its beefier sibling.
Read our in-depth AMD RX 9070 XT vs RX 9070 analysis
✅ You want a great graphics card on a budget. The Intel Arc B580 is a steal at MSRP. No other card at $250 is going to be this good at 1440p gaming.✅ You're looking for great gaming performance. Speaking of performance, the gaming chops on the Arc B580 aren't just a value proposition. You won't be sacrificing performance with this card.
❌ You're looking for a budget creative GPU. While the B580 has some promise as a video editing card, if you're looking for a GPU for creative work, Nvidia cards are the better bet.❌ You want a cheap GPU for AI workloads. Like its creative prowess, the Intel Arc B580 is decent enough for dedicated AI workloads, but can't hold a candle to competing Nvidia cards.
The Intel Arc B580 was the biggest GPU surprise of 2024 when it launched last December, greatly impressing me with its phenomenal 1440p gaming performance in my tests for under $250/£250/AU$450 at MSRP, something AMD and Nvidia have failed to do for two generations now.
What's more, any issues that Intel's previous Arc Alchemist GPUs suffered from seemed to have been ironed out with this latest release, earning a rare 5-star review from me in my review.
While the card has its drawbacks (its ray tracing performance lags behind Nvidia's and it's not the best creative GPU out there for the money, for example), this is strictly a GPU for gamers, and considering how long it's been since PC gamers have had a compelling graphics card at this price point, it's simply in a class of its own.
That said, if you want the absolute best graphics card performance at 1440p or 1080p, there are better cards for that, like the Nvidia GeForce RTX 4060 Ti. But those other cards are substantially more expensive, and they just don't offer a good enough reason to buy them when the Arc B580 is right there, offering nearly equal performance on average and costing 10%-40% less than its competition.
So if you're looking for the best budget graphics card but don't want to sacrifice performance, this card is exactly what you are looking for.
Intel Arc B570If all you're looking for is 1080p gaming on the cheap, the Arc B570 can offer you solid 1080p gameplay at a much lower price, though it won't be able to offer you much else than that.
Read our in-depth Intel Arc B580 vs B570 analysis
Nvidia GeForce RTX 4060Nvidia's cheapest graphics card on the market isn't a slouch when it comes to performance, especially at 1080p, with some good 1440p performance when using reasonable settings.
Read our in-depth Intel Arc B580 vs Nvidia RTX 4060 analysis
❌ You want the very best performance: While the performance of the RTX 5070 Ti is fantastic, there are better (though more expensive) graphics cards you can buy to maximum performance.❌ You're on a tight budget: While the MSRP on this card is great for an enthusiast-grade GPU, it's still very expensive for most buyers.❌ You don't plan on gaming at 4K: If all you're looking to do is game at 1440p, this card is likely going to be overkill, especially with more affordable 1440p GPUs on the market.
✅ You want the perfect balance of 4K performance and price: Pretty much across the board, this card delivers outstanding performance no matter the workload for less than other premium GPUs.✅ You want fantastic creative performance on the cheap: For creative professionals, a high-quality GPU can be a very expensive proposition, but this card brings solid 3D, video, and raster performance on a budget.✅ You want the latest Nvidia technology: DLSS 4 with Multi Frame Generation is a fantastic way to max out 4K gaming monitors.
I found the Nvidia GeForce RTX 5070 Ti to offer the best balance of performance and price across all categories that I've seen over the last two generations of Nvidia GPUs, and it's not even really close in most cases.
My test results found it gives crisp, fluid 4K gaming performance without much in the way of settings compromise in just about every game. Meanwhile, content creators or AI enthusiasts who are looking for an Nvidia GPU that can deliver top-tier performance without the kind of substantial investment normally required for a workstation GPU.
Of course, this all comes with a massive caveat, which is that you're able to find this card at MSRP. Without a Founders Edition card directly from Nvidia, pricing is ultimately in the hands of third-party partners, so there's no guarantee that you'll ever be able to find this card at the price Nvidia set for it. So depending on the price you find for this card, your value proposition may change accordingly, and other cards on this list might be better options.
Objectively speaking, however, just about every other card on this list faces the same pricing pressures that the RTX 5070 Ti does, and so on balance this card is almost certainly going to offer you the best performance for your money no matter what you're looking to use it for.
If you want it for your gaming rig, you can game at close to 90 fps on average at 4K with a fps floor of 65, which is about as good as you could want for a premium GPU, especially for its price.
Creative pros who need a dedicated GPU for 3D modeling or video editing work will especially like this card, as it comes close to matching some of the best graphics cards for creatives of the last generation, and someone upgrading from an RTX 3090 will see a big performance boost over what they have now.
If the price of Nvidia's RTX 5080 and RTX 5090 are too high for your liking but you want enthusiast-grade performance, the RTX 5070 Ti is exactly the card you're looking for.
Read the full Nvidia GeForce RTX 5070 Ti review
Nvidia GeForce RTX 5080While more expensive, this is the best high-end, enthusiast-grade GPU available for under a grand, making it a more premium alternative to the RTX 5070 Ti.
Read our in-depth Nvidia RTX 5080 vs RTX 5070 Ti analysis
AMD Radeon RX 9070 XTWhile the RTX 5070 Ti has better performance than the RX 9070 XT, it's a very close fight between the two, and the RX 9070 XT is a good bit cheaper.
Read our in-depth Nvidia RTX 5070 Ti vs RX 9070 XT analysis
✅ You want fantastic 4K gaming performance: The RTX 5080 has some of the best 4K gaming performance you're going to find anywhere, even before factoring in DLSS 4 MFG.✅You want very strong creative performance on the cheap: Nvidia's GPUs are miles ahead of the competition when it comes to creative workloads, and the RTX 5080 delivers fantastic performance for the price.
❌ You have anything better than an RTX 4080: The RTX 5080 isn't a huge improvement over the RTX 4080 and RTX 4080 Super, so if you have those cards, there's not much reason to upgrade.❌ You're on a tight-ish budget: While not as pricey as the RTX 4080, RTX 4090, or RTX 5090, this is still a very expensive graphics card, even before factoring in price inflation by third-party sellers. 
The Nvidia GeForce RTX 5080 is the third most powerful GPU you can buy at a price that won't force you to take out a small bank loan to buy, even though which version of the card you buy can make all the difference in terms of whether you'll be able to afford it.
This card is one of the most sought-after on the market, so it's subject to ongoing stock issues and price inflation which makes its value proposition for any given buyer a moving target.
This is especially true once you consider that the gen-on-gen uplift of this card over the RTX 4080 and RTX 4080 Super is only about 8-13% according to my testing, so if you can find the RTX 4080 and RTX 4080 Super for less than an RTX 5080, those cards might be a better value depending on their price.
It also goes without saying that anyone with an RTX 4080 or RTX 4080 Super should not get this card, as you won't see much benefit from it other than Multi-Frame Generation with DLSS 4, which is not worth spending this kind of money on.
For everyone else, though, especially those coming from the RTX 30 series or pretty much any AMD card other than the RX 7900 XTX, the RTX 5080 will blow you away—assuming you can find it in stock, that is.
Read the full Nvidia GeForce RTX 5080 review
Nvidia GeForce RTX 5070 TiIf the RTX 5080 is too expensive for you, the RTX 5070 Ti offers a solid amount of performance relative to the RTX 5080 at a slightly lower price.
Read our in-depth Nvidia RTX 5080 vs RTX 5070 Ti analysis
Nvidia GeForce RTX 4080 SuperThe RTX 5080 is only about 8-13% faster than the RTX 4080 Super, so if you can get the RTX 4080 Super for less than the RTX 5080, it might be a much better value. 
Read our full RTX 5080 vs RTX 4080 Super analysis
✅ You want the best of the best: The RTX 5090's performance is in a class all its own, blowing away even the vaunted RTX 4090.  ✅ You do a lot of 3D modeling/rendering: The RTX 5090's 3D rendering performance is 30-40% faster than the RTX 4090, which is light years ahead of anything else, so this card is a 3D artist's dream.✅ You're spending someone else's money: If you don't have to pay for this thing, there's no reason not to splurge on the best performance available.
❌ You're not a creative or machine-learning professional: While the gaming prowess of this card is undeniable, it's honestly extreme overkill for 99% of gamers out there.❌ You aren't flush with cash: Honestly, unless you're loaded to bear with fat stacks of cash, there are graphics cards out there that will get you phenomenal performance at a fraction of the price.
If you're looking for the best possible performance for your creative work—and who isn't? Time is money, after all—then this is really really the holy grail for creatives (not to mention AI professionals).
Whether you're working with 3D modeling and rendering in a professional capacity, you're a video production professional, or you're an AI researcher, Nvidia's GPUs have long been the standard for commercial and academic use, but the RTX 5090 is in a whole other league.
With a massive 33% increase in multiprocessors, including industry mainstay CUDA and Tensor cores, and 32GB VRAM that is faster than the last-gen's best, the RTX 4090, which 'only' had 24GB VRAM.
For the gamers and enthusiasts out there who are determined to flex on their friends with this card, my testing found it was nothing short of overkill for every game I tested it with. It's the only graphics card that's truly capable of handling native 8K gaming, thanks to its massive VRAM pool and enormous memory interface, but it's also the only graphics card I've tested where you can get actually playable native 4K gaming with ray tracing maxed out.
Of course, all this performance isn't cheap, and you could buy a used car with a year or two of life in it for the same price as this card's MSRP, and there's no way you're ever going to find it at MSRP. What's more, its frankly scandalous power consumption tops out around 575W, so expect a hefty power bill to go along with this already super-premium GPU.
Read the full Nvidia GeForce RTX 5090 review
Nvidia GeForce RTX 4090If you're looking at the RTX 5090, then there's really only one alternative, and that's the RTX 4090. While not as powerful as the RTX 5090, it's still the second most powerful consumer graphics card in the world.
Read our in-depth Nvidia RTX 5090 vs RTX 4090 analysis
✅You want fantastic midrange performance: Given the strength of this card in all categories, on balance, it's one of the best graphics cards you're going to find in the midrange.✅You want very strong ray tracing performance: With the maturity of its ray tracing cores, the RTX 4070 Super is the best ray tracing GPU in the midrange, for sure.✅You want some creative performance as well: With its strong CUDA backbone, the RTX 4070 Super is a great option for those looking to get into creative content work.
❌ You don't want to spend a fortune: Given the price of the competition, there are better graphics cards for your money than the RTX 4070 Super❌ You don't care about ray tracing or compute performance: The strongest assets this card brings to the table are its ray tracing and tensor cores, but if you don't care about ray tracing or machine learning tasks, the RX 7900 GRE will offer a better overall gaming performance.
The Nvidia GeForce RTX 4070 Super meets the high expectations I had for this card, offering compelling performance at the same price as its predecessor.
The Super outshines the base RTX 4070, offering more SMs for enhanced processing and a swifter base clock speed. However, its 12GB GDDR6X VRAM limits its 4K prowess, so for optimal 4K performance, the Nvidia RTX 5070 Ti and RTX 5080 are a better bet.
For top-tier 1440p gaming, though, the RTX 4070 Super excels thanks to its robust specs, DLSS 3 with Frame Generation, and Nvidia Reflex technology.
And while the RTX 4070 Super generally outperforms the RX 7800 XT, particularly in ray tracing, AMD holds the edge in 1440p gaming performance with the RX 9070.
Despite stiff competition, the Nvidia GeForce RTX 4070 Super still stands out as the best 1440p graphics card overall for those who want the most utility out of a midrange GPU, especially for anyone seeking a balance of gaming prowess, content creation capabilities, and sheer performance.
Read the full Nvidia GeForce RTX 4070 Super review
Nvidia GeForce RTX 5070The RTX 4070 Super's successor, the Nvidia RTX 5070, isn't any more powerful than the RTX 4070 Super, but it has a slightly cheaper MSRP and features advanced features like DLSS 4 Multi Frame Generation, so if you can find it for the same price or cheaper than the RTX 4070 Super, it's a good pick.
Read our in-depth Nvidia RTX 5070 vs RTX 4070 Super analysis
AMD Radeon RX 7900 GREThe RX 7900 GRE is the perfect alternative to the RTX 4070 Super if all you care about is gaming, though it's not as good of an option if you need to do any creative or ML work.
Read our in-depth RX 7900 GRE vs RTX 4070 Super analysis
✅ You want bargain-priced 1080p gaming: This graphics card can't do a lot of things, but one thing it's great at is playing 1080p games for a fraction of the price of its nearest competitors.✅You want good ray tracing performance for cheap: The main draw of this GPU is how dirt cheap it is relative to the market, and for that, the ray tracing on this card is better than it has any right to be at this price (though it's still not great, necessarily).
❌ You want to game at 1440p: Given the limited amount of VRAM, this GPU isn't really capable of 1440p gaming without some major compromises on visual quality.❌ You want a budget creative card: If you're just putting your toes into the waters of creative work, whether it be 3D modeling, video editing, or the like, you should probably look at the RTX 4060 instead. This card won't give you what you need.
The Intel Arc B570 might not get as much attention as the Intel Arc B580, but if all you're looking for is dirt-cheap 1080p gameplay with some nice extras like ray tracing and hardware upscaling, you can't go wrong with the Arc B570.
For significantly less than competing 1080p cards like the AMD RX 7600 and Nvidia RTX 4060, you can get nearly the same level of performance while gaming at 1080p, and with hardware upscaling through Intel XeSS with Frame Generation, you can get frame rates we've simply never seen at this price point.
Of course, it does come with some limitations. Its 10GB GDDR6 VRAM is great, but not quite enough to really handle 1440p gaming without some major compromises. Its creative performance is also pretty much non-existent beyond being able to encode AV1 video, which you might find useful if you're a streamer and you want a cheap setup to offload some video processing into.
Otherwise, this is strictly a cheap 1080p gaming graphics card, and for what it is, it's about as good as you're going to find anywhere on the market right now.
Intel Arc B580While not quite a 1080p graphics card thanks to its 12GB VRAM, this card is priced very close at MSRP to the B570, making it a better 'stretch' alternative if you have the money.
Read our in-depth Intel Arc B580 vs B570 analysis
✅ You want native 4K ray-traced gaming: This is one of the few cards that can consistently run full ray tracing at native 4K resolution.✅ You are a 3D graphics professional: If you work with major 3D rendering tools like Maya, Blender, and others, this is your graphics card.
❌ You're on a budget: This card is incredibly expensive, even on sale.❌ You're concerned about power consumption: With a TGP of 450W, this card has a near-bottomless appetite for power.
Yes, the Nvidia GeForce RTX 4090 is expensive. It also requires a 16-pin connector or adapter that many people still don't have, and it is very, very big. But, the first release from Nvidia’s new 4000-series is an absolute powerhouse that can tackle anything you need it to, making it a worthwhile pickup from the last gen if you can find it for cheap.
In my testing, I found it performed significantly better than the Nvidia GeForce RTX 3090 with two to four times the performance in synthetic benchmarks and up to 100% improved framerates with some games. What’s more, DLSS 3 is a game-changer in terms of gaming frame rates, and in games that support it, the RTX 4090 with Frame Generation turned on will absolutely push even the best gaming monitors to the limit of what they can do. Now that DLSS 4 is rolling out to the RTX 40 series, this is an even more powerful card than it was when I reviewed it in 2022.
That said, this is far more GPU than most people will probably ever need, and you have to really, really want this card, especially at the prices it is selling for right now. That said, if you can get it for a reasonable price, it's a solid pickup that you might not want to miss.
Read the full Nvidia GeForce RTX 4090 review
Nvidia GeForce RTX 5090If you're in the market for the best of the best, the RTX 5090 is the best there is, and while more expensive than the RTX 4090, it's certainly worth it.
Read our in-depth Nvidia RTX 5090 vs RTX 4090 analysis
✅ You want outstanding 4K gaming performance: Few games can challenge the RX 7900 XTX at native 4K, and FSR upscaling even makes fast ray-traced 4K gaming possible.✅ You want to future-proof your rig for fast 8K gaming: Thanks to its 24GB VRAM and DisplayPort 2.1 output, when 8K gaming finally does arrive in the next few years, this card has the hardware so it won't be completely out of its league.
❌ You want something for creative work: While the price tag might lead you to think this card can do anything, it really can't keep up with even midrange Nvidia cards in creative workloads like Blender.❌ You're on a budget: While it is well-priced for a premium GPU, this card is not for those who are on tighter budgets.
Asserting its dominance in the 4K gaming realm, the AMD Radeon RX 7900 XTX marks AMD's most exceptional offering in the premium category.
Released at the tail end of 2022, this powerhouse GPU is a couple of years old now, but it's still holding its own against current-gen cards like the RTX 5070 Ti. Especially when it comes to 4K gaming, there are few cards that can match the frame rates of the RX 7900 XTX.
Outside of gaming though, things aren't as rosy for the best AMD graphics card of the last generation. Thanks to Nvidia's proprietary lock on the CUDA instruction set powering most 3D modeling software and AI tools like Pytorch (not to mention Nvidia's advanced Tensor cores), even more budget-friendly Nvidia cards are better for these workloads than AMD's last-gen flagship.
That said, if you're looking for a gaming dynamo that's capable of blazing-fast 4K gaming, this is definitely one of the best graphics cards you can buy and might even be more available than Nvidia's latest RTX 50 series GPUs.
Read the full AMD Radeon RX 7900 XTX review
Nvidia GeForce RTX 5070 TiFor less than the amount of money you'd pay for the RX 7900 XTX, you'd likely be able to pick up the RTX 5070 Ti, which offers very similar performance.
Nvidia GeForce RTX 5080For the same price as the RX 7900 XTX, you can get the current-gen RTX 5080 from Nvidia, which has better performance on every level.
When shopping for the best graphics card for your needs and budget, the first thing you need to do is consider your monitor (or the monitor you plan to buy), as the resolution you're targeting matters quite a bit.
For 1080p, I recommend looking at GPUs with at least 8GB VRAM, like the AMD Radeon RX 7600 and Nvidia RTX 4060, but getting one with 10GB VRAM like the Intel Arc B570 is ideal.
For 1440p, I recommend GPUs with at least 12GB VRAM, like the Nvidia RTX 5070, AMD RX 9070, or Intel Arc B580. This will ensure high framerates at quad HD resolution, which is considered the sweet spot for PC gaming.
For 4K gaming, my testing shows that you'll need a minimum of 16GB VRAM in your GPU, preferably with a 256-bit memory bus to make sure that UHD textures are processed quickly so you can maintain a 60 FPS baseline for your games. This means you should be looking at cards like the Nvidia RTX 4080 Super, RTX 5080, AMD, RX 7900 XTX, and AMD RX 9070 XT, all of which are great picks for 4K gaming.
For creative and AI use, you're going to want to stick to the best Nvidia graphics card you can afford, as it will give you the level of performance professionals need.
If you need more in-depth advice on how to choose a graphics card appropriate for your needs, you can read my in-depth guide to how to choose the right graphics card.
When it comes to the best graphics cards, it's essential that I test every GPU I review on an equal playing field. That's why I test graphics cards with a full suite of benchmark tests covering synthetic tests, creative performance, and a battery of around 7-10 games tested at different settings and across several resolutions, all on current drivers.
I am also constantly retesting graphics cards I've already reviewed so I have the most up-to-date data as drivers are updated. I make sure that all of the cards are tested on the same hardware, which means the same processor, the same memory at the same speed, the same motherboard, and the same SSD.
That way, I can be sure that I'm measuring how the graphics card itself is performing relative to other cards.
If you want to know my testing process in greater detail, you can read more about how I test graphics cards for TechRadar in-depth.
There are a few ways to measure energy efficiency for a GPU, with the two most common being which draws the most power under load, and which gets the best performance per watt.
While maximum power draw is very important (if your graphics card draws more power than your PSU can provide, your PC will crash without warning) since knowing its peak power consumption is essential to knowing whether you can effectively use the card with your PC build.
But if we're talking specifically about efficiency, there's value in knowing which graphics cards get the most performance at the lowest energy use.
For that, performance-per-watt is an excellent metric to look at, especially if you're concerned about your monthly power bill.
Generally speaking, the best graphics card for gaming is going to depend on several factors, but principally, your budget and monitor resolution will dictate the best card for gaming on your system. 
For 4K gaming, the Nvidia RTX 5080 is as good as it gets without spending an absolute mint on the RTX 5090, while the AMD Radeon RX 9070 XT is the best gaming graphics card from AMD, with the Intel Arc B580 offering an excellent budget-friendly alternative. 
For 1080p, the Nvidia RTX 4060 is the way to go, while the AMD Radeon RX 7600 and Intel Arc B570 are also great alternatives.
The most powerful graphics card for gamers and creatives on the market right now is the RTX 5090. That's because it features a staggering 32GB GDDR7 memory pool with a 512-bit memory bus, along with a massive GPU die with more than 20,000 shader cores, 
An RTX graphics card is much better than an older GTX model. Nvidia discontinued the GTX line when brought in the RTX 20 series a handful of years ago. RTX GPUs are capable of real-time ray tracing and can utilize the company's AI upscaling tech DLSS for increased framerates in more demanding games. 
Sign up for breaking news, reviews, opinion, top tech deals, and more.
John (He/Him) is the Components Editor here at TechRadar and he is also a programmer, gamer, activist, and Brooklyn College alum currently living in Brooklyn, NY. 
Named by the CTA as a CES 2020 Media Trailblazer for his science and technology reporting, John specializes in all areas of computer science, including industry news, hardware reviews, PC gaming, as well as general science writing and the social impact of the tech industry.
You can find him online on Bluesky  @johnloeffler.bsky.social

TechRadar is part of Future US Inc, an international media group and leading digital publisher. Visit our corporate site.

©
Future US, Inc. Full 7th Floor, 130 West 42nd Street,
New York,
NY 10036."
"general","article","GeForce RTX 50 Series Graphics Cards & Laptops Powered by NVIDIA","https://www.nvidia.com/en-us/geforce/news/rtx-50-series-graphics-cards-gpu-laptop-announcements/","AI-driven platform for life sciences research and discovery
Fully managed end-to-end AI platform on leading clouds
Build, customize, and deploy multimodal generative AI
Integrate advanced simulation and AI into complex 3D workflows
Guide for using NVIDIA NGC private registry with GPU cloud
Accelerated, containerized AI models and SDKs
Modernizing data centers with AI and accelerated computing
Enterprise AI factory for model development and deployment
Architecture for data centers that transform data into intelligence
A supercomputer purpose-built for AI and HPC
Advanced functional safety and security for edge AI
Accelerated computing with modular servers
Scalable data center infrastructure for high-performance AI
Leading platform for autonomous machines and embedded applications
Powerful in-vehicle computing for AI-driven autonomous vehicle systems
AI-powered computing for innovative medical devices and imaging
RTX graphics cards bring game-changing AI capabilities
Thinnest and longest lasting RTX laptops, optimized by Max-Q
Smooth, tear-free gaming with NVIDIA G-SYNC monitors
Neural rendering tech boosts FPS and enhances image quality
Advanced platform for full ray tracing and neural rendering
Ultimate responsiveness for faster reactions and better aim
AI PCs for gaming, creating, productivity and development
High performance laptops and desktops, purpose-built for creators
RTX-powered cloud gaming. Choose from 3 memberships
Optimize gaming, streaming, and AI-powered creativity
AI-enhanced voice and video for next-level streams, videos, and calls
The engine of the new industrial revolution
High performance, scalability, and security for every data center
Performance and energy efficiency for endless possibilities
RTX graphics cards bring game-changing AI capabilities
Accelerating professional AI, graphics, rendering and compute workloads
Virtual solutions for scalable, high-performance computing
GPU-powered laptops for gamers and creators
High performance laptops purpose-built for creators
Accelerate professional AI and visual computing from anywhere
Accelerated networks for modern workloads
Software-defined hardware accelerators for networking, storage, and security
Ethernet performance, availability, and ease of use across a wide range of applications
High-performance networking for super computers, AI, and cloud data centers
Networking software for optimized performance and scalability
IO subsystem for modern, GPU-accelerated data centers
A Grace Blackwell AI Supercomputer on your desk
The ultimate desktop AI supercomputer powered by NVIDIA Grace Blackwell
Accelerate innovation and productivity in AI workflows
Powerful AI, graphics, rendering, and compute workloads
Accelerate professional AI and visual computing from anywhere
Simplify AI development with NVIDIA AI Workbench on GPUs
Explore NVIDIA's AI models, blueprints, and tools for developers
AI and HPC software solutions for data center acceleration
Monitor and manage GPU performance in cluster environments
Explore NVIDIA developer tools for AI, graphics, and HPC
Discover GPU-optimized AI, HPC, and data science software
Accelerate AI and HPC workloads with NVIDIA GPU Cloud solutions
Enhance multi-display productivity with NVIDIA RTX Desktop Manager
Creative tools and AI-powered apps for artists and designers
Add intelligence and efficiency to your business with AI and machine learning
Build AI agents designed to reason, plan, and act
Powering a new class of enterprise infrastructure for AI
Enables natural, personalized interactions with real-time speech AI
AI-driven solutions to strengthen cybersecurity and AI infrastructure
Iterate on large datasets, deploy models more frequently, and lower total cost
Drive breakthrough performance with AI-enabled applications and services
Powering AI, HPC, and modern workloads with NVIDIA
Bringing enterprise storage into the era of agentic AI
Accelerated computing uses specialized hardware to boost IT performance
On-demand IT resources and services, enabling scalability and intelligent insights
Accelerate the scaling of AI across your organization
High speed ethernet interconnect solutions and services
Save energy and lower cost with AI and accelerated computing
NVIDIA virtual GPU software delivers powerful GPU performance
Streamline building, operating, and connecting metaverse apps
Develop real-time interactive design using AI-accelerated real-time digital twins
Harness the power of large-scale, physically-based OpenUSD simulation
Bring state-of-the-art rendering to professional workflows
Innovative solutions to take on your robotics, edge, and vision AI challenges
Enablies researchers to visualize their large datasets at interactive speeds
AI-defined vehicles are transforming the future of mobility
Transform workflows with immersive, scalable interactions in virtual environments
Discover NVIDIA’s HPC solutions for AI, simulation, and accelerated computing
Boost accuracy with GPU-accelerating HPC and AI
Enables researchers to visualize large datasets at interactive speeds
Fast-tracking the advancement of scientific innovations with QPUs
Innovative solutions to take on robotics, edge, and vision AI challenges
GPU-accelerated advances in AI perception, simulation, and software
Bring the power of NVIDIA AI to the edge for real-time decision-making solutions
Transform data into valuable insights using vision AI
AI-enhanced vehicles are transforming the future of mobility
Essential data center tools for safe autonomous vehicle development
Explore high-fidelity sensor simulation for safe autonomous vehicle development
Develop automated driving functions and immersive in-cabin experiences
State-of-the-art system for AV safety, from the cloud to the car
AI-driven platform for life sciences research and discovery
Fully managed end-to-end AI platform on leading clouds
Build, customize, and deploy multimodal generative AI
Integrate advanced simulation and AI into complex 3D workflows
Guide for using NVIDIA NGC private registry with GPU cloud
Accelerated, containerized AI models and SDKs
Modernizing data centers with AI and accelerated computing
Enterprise AI factory for model development and deployment
Architecture for data centers that transform data into intelligence
A supercomputer purpose-built for AI and HPC
Advanced functional safety and security for edge AI
Accelerated computing with modular servers
Scalable data center infrastructure for high-performance AI
Leading platform for autonomous machines and embedded applications
Powerful in-vehicle computing for AI-driven autonomous vehicle systems
AI-powered computing for innovative medical devices and imaging
RTX graphics cards bring game-changing AI capabilities
Thinnest and longest lasting RTX laptops, optimized by Max-Q
Smooth, tear-free gaming with NVIDIA G-SYNC monitors
Neural rendering tech boosts FPS and enhances image quality
Advanced platform for full ray tracing and neural rendering
Ultimate responsiveness for faster reactions and better aim
AI PCs for gaming, creating, productivity and development
High performance laptops and desktops, purpose-built for creators
RTX-powered cloud gaming. Choose from 3 memberships
Optimize gaming, streaming, and AI-powered creativity
AI-enhanced voice and video for next-level streams, videos, and calls
The engine of the new industrial revolution
High performance, scalability, and security for every data center
Performance and energy efficiency for endless possibilities
RTX graphics cards bring game-changing AI capabilities
Accelerating professional AI, graphics, rendering and compute workloads
Virtual solutions for scalable, high-performance computing
GPU-powered laptops for gamers and creators
High performance laptops purpose-built for creators
Accelerate professional AI and visual computing from anywhere
Accelerated networks for modern workloads
Software-defined hardware accelerators for networking, storage, and security
Ethernet performance, availability, and ease of use across a wide range of applications
High-performance networking for super computers, AI, and cloud data centers
Networking software for optimized performance and scalability
IO subsystem for modern, GPU-accelerated data centers
A Grace Blackwell AI Supercomputer on your desk
The ultimate desktop AI supercomputer powered by NVIDIA Grace Blackwell
Accelerate innovation and productivity in AI workflows
Powerful AI, graphics, rendering, and compute workloads
Accelerate professional AI and visual computing from anywhere
Simplify AI development with NVIDIA AI Workbench on GPUs
Explore NVIDIA's AI models, blueprints, and tools for developers
AI and HPC software solutions for data center acceleration
Monitor and manage GPU performance in cluster environments
Explore NVIDIA developer tools for AI, graphics, and HPC
Discover GPU-optimized AI, HPC, and data science software
Accelerate AI and HPC workloads with NVIDIA GPU Cloud solutions
Enhance multi-display productivity with NVIDIA RTX Desktop Manager
Creative tools and AI-powered apps for artists and designers
Add intelligence and efficiency to your business with AI and machine learning
Build AI agents designed to reason, plan, and act
Powering a new class of enterprise infrastructure for AI
Enables natural, personalized interactions with real-time speech AI
AI-driven solutions to strengthen cybersecurity and AI infrastructure
Iterate on large datasets, deploy models more frequently, and lower total cost
Drive breakthrough performance with AI-enabled applications and services
Powering AI, HPC, and modern workloads with NVIDIA
Bringing enterprise storage into the era of agentic AI
Accelerated computing uses specialized hardware to boost IT performance
On-demand IT resources and services, enabling scalability and intelligent insights
Accelerate the scaling of AI across your organization
High speed ethernet interconnect solutions and services
Save energy and lower cost with AI and accelerated computing
NVIDIA virtual GPU software delivers powerful GPU performance
Streamline building, operating, and connecting metaverse apps
Develop real-time interactive design using AI-accelerated real-time digital twins
Harness the power of large-scale, physically-based OpenUSD simulation
Bring state-of-the-art rendering to professional workflows
Innovative solutions to take on your robotics, edge, and vision AI challenges
Enablies researchers to visualize their large datasets at interactive speeds
AI-defined vehicles are transforming the future of mobility
Transform workflows with immersive, scalable interactions in virtual environments
Discover NVIDIA’s HPC solutions for AI, simulation, and accelerated computing
Boost accuracy with GPU-accelerating HPC and AI
Enables researchers to visualize large datasets at interactive speeds
Fast-tracking the advancement of scientific innovations with QPUs
Innovative solutions to take on robotics, edge, and vision AI challenges
GPU-accelerated advances in AI perception, simulation, and software
Bring the power of NVIDIA AI to the edge for real-time decision-making solutions
Transform data into valuable insights using vision AI
AI-enhanced vehicles are transforming the future of mobility
Essential data center tools for safe autonomous vehicle development
Explore high-fidelity sensor simulation for safe autonomous vehicle development
Develop automated driving functions and immersive in-cabin experiences
State-of-the-art system for AV safety, from the cloud to the car
January 15th, 2025: We have updated and expanded our GeForce RTX 50 Series performance charts as we move closer to launch. These match the charts we showed to press at CES last week during Editors Day, and we wanted to share with everyone. These results include additional testing with the latest hardware and driver revisions.
January 28th, 2025: The NVIDIA RTX Blackwell GPU Architecture whitepaper is now available for download here.
Powered by the NVIDIA Blackwell RTX architecture, GeForce RTX 50 Series graphics cards and GeForce RTX 50 Series laptops accelerate frame rates by up to 8X using NVIDIA DLSS 4 with Multi Frame Generation, reduce latency by up to 75% using NVIDIA Reflex 2, enable next-level graphical fidelity for gamers and creators with NVIDIA RTX Neural Shaders, and much more.
GeForce RTX 50 Series GPUs and laptops deliver game-changing performance, power transformative AI experiences, and enable creators to complete workflows in record time. Rewatch the NVIDIA Keynote by NVIDIA CEO, Jensen Huang, below, and read on for all of the details.
Neural rendering is the next era of computer graphics. By integrating neural networks into the rendering process, we can take dramatic leaps forward in performance, image quality, and interactivity to deliver new levels of immersion.
The very first example of neural rendering was DLSS. We used lower resolution rendered frames as an input to a neural network, which was trained to output a full resolution frame. DLSS has since evolved to the point where it can generate entire frames and understand the composition of a scene including shadows, reflections, and occlusion to generate images that are better than native rendering. With the invention of DLSS 4 with Multi Frame Generation, working in unison with the complete suite of DLSS technologies, we can multiply frame rates by up to 8X over traditional brute-force rendering and provide image quality that is better than native rendering.
We've integrated neural networks inside of programmable shaders to create neural shaders. RTX Neural Shaders will drive the next decade of graphics innovations. They can be used to compress textures by up to 7X, saving massive amounts of graphics memory. And be used to create cinematic-quality textures, and even more advanced lighting effects in games.
RTX Neural Faces offers an innovative, new approach to improve face quality using generative AI. Instead of traditional rendering, Neural Faces takes a simple rasterized face plus 3D pose data as input and uses a real-time generative AI model to infer a more natural face.
The power of neural rendering with DLSS 4 can be seen in our new Zorah tech demo.
The NVIDIA RTX Blackwell architecture has been built and optimized for neural rendering. It has a massive amount of processing power, with new engines and features specifically designed to accelerate the next generation of neural rendering.
With up to 92 billion transistors, Blackwell is the most powerful consumer GPU ever created. The Blackwell streaming multiprocessor (SM) has been updated with more processing throughput, and a tighter integration with the Tensor Cores in order to optimize the performance of neural shaders. Blackwell is enhanced by several hardware and software innovations to improve Shader Execution Reordering. The reorder logic is twice as efficient, increasing the speed and precision of reordering which accelerates the performance of neural shaders.
New Blackwell Tensor Cores have been built with a massive amount of AI processing horsepower, and support accelerated processing of FP4 precision models. With FP4, Blackwell Tensor Cores can not only process models faster, but can do so while using less graphics memory.
To help support the frame pacing requirements of next generation DLSS Multi Frame Generation, the Blackwell architecture was built with enhanced hardware flip metering capabilities to provide the speed and accuracy required for a smooth, high-quality experience.
Blackwell also features brand new RT Cores designed to ray trace massive amounts of detailed geometry. The RT Cores have 2X the ray triangle intersection rate of the previous generation, and enhanced compression designed to reduce memory footprint. This allows Blackwell GPUs to ray trace levels of geometry that were never before possible.
Blackwell has also been enhanced with PCIe Gen5 and DisplayPort 2.1b UHBR20, driving displays up to 8K 165Hz.
For GeForce RTX 50 Series laptops, new Max-Q technologies such as Advanced Power Gating, Low Latency Sleep, and Accelerated Frequency Switching increases battery life by up to 40%, compared to the previous generation.
And in order to feed all this processing power, Blackwell is equipped with the world's fastest memory - GDDR7 with speeds up to 30Gbps. With G7 memory, Blackwell GPUs can deliver up to 1.8TB/s of memory bandwidth.
On January 30th, the GeForce RTX 5090 and GeForce RTX 5080 arrive on store shelves. The GeForce RTX 5070 Ti and GeForce RTX 5070 will be available starting in February.
Limited NVIDIA Founders Edition models of the GeForce RTX 5090, GeForce RTX 5080, and GeForce RTX 5070 graphics cards will be available worldwide. And stock-clocked and factory-overclocked variants of all four GPUs will be available from top add-in card providers such as ASUS, Colorful, Gainward, GALAX, GIGABYTE, INNO3D, KFA2, MSI, Palit, PNY and ZOTAC. And in desktops from system builders, including Falcon Northwest, Maingear, Origin PC, and more.
Thanks to the Blackwell architecture’s innovations and DLSS 4, the GeForce RTX 5090 outperforms the GeForce RTX 4090 by 2X.
Cyberpunk 2077: Phantom Liberty on GeForce RTX 5090 runs two times faster than last generation with the power of DLSS 4
With 32GB of GDDR7 memory, 1792 GB/sec of total memory bandwidth, 21,760 CUDA Cores, 680 5th Generation Tensor, and 170 4th Generation Ray Tracing Cores, it is the ultimate GeForce GPU, with more hardware and power than anything we’ve made previously (see the complete list of specs here). 
In games, DLSS 4 with Multi Frame Generation helps multiply performance in Cyberpunk 2077, Alan Wake 2, and Black Myth: Wukong, enabling GeForce RTX 5090 owners to play at 4K 240Hz/FPS with full ray tracing and every other setting maxed out, for the definitive experience.
In Generative AI applications, image generation is 2X faster with half the memory using FP4 on the GeForce RTX 5090, compared to FP16 on the GeForce RTX 4090. FP4 is a lower quantization method, similar to file compression, decreasing model sizes. 
Black Forest Labs' FLUX.1 [dev] model requires over 23GB of VRAM in plain FP16. On the GeForce RTX 4090, images are generated in 15 seconds. With FP4, less VRAM is used and the GeForce RTX 5090 can reduce image generation to just over five seconds.
The GeForce RTX 5090 will be available from January 30th, starting at $1,999.
Our limited NVIDIA GeForce RTX Founders Edition graphics cards are the pinnacle of design, built with premium materials, boasting striking, unique designs, and cooling innovations.
For the GeForce RTX 5090, our engineers have created an unbelievable 2-slot, 304mm long x 137mm high x 2-slot wide, SFF-Ready Enthusiast GeForce Card capable of quietly cooling the high-performance Blackwell GPU inside thanks to a revolutionary new double flow through design.
Each aspect of the new GeForce RTX 5090 Founders Edition has been purposefully designed to increase cooling capacity and capabilities by leveraging new design innovations, Liquid Metal thermal interface material (TIM), and a unique 3-piece PCB. Stay tuned for further details about the design of these incredible cards ahead of their release.
The GeForce RTX 5080 is up to twice the speed of the GeForce RTX 4080 in games, thanks to the Blackwell architecture and DLSS 4 with Multi Frame Generation. With new 5th gen Tensor Cores, 4th gen RT Cores, and 16GB of GDDR7 memory providing up to 960 GB/sec of total memory bandwidth (a 34% increase compared to the GeForce RTX 4080’s 717 GB/sec), the GeForce RTX 5080 delivers a massive leap in performance for gamers and creators.
At 4K, with full ray tracing and other settings maxed, GeForce RTX 5080 owners can play Alan Wake 2, Black Myth: Wukong, and Cyberpunk 2077 at high frame rates, with performance that’s around twice as fast, on average, compared to the GeForce RTX 4080.
The GeForce RTX 5080 will be available from January 30th, starting at $999.
The GeForce RTX 5070 Ti includes 16GB of GDDR7 memory, and 896 GB/sec of total memory bandwidth, a 78% increase in bandwidth compared to the GeForce RTX 4070 Ti’s 504 GB/sec.
Using the full capabilities of the Blackwell architecture, and the power of DLSS 4 with Multi Frame Generation, game frame rates are 2X faster than the GeForce RTX 4070 Ti’s. At 2560x1440, with full ray tracing and other settings maxed, GeForce RTX 5070 Ti owners can play Alan Wake 2, Black Myth: Wukong, and Cyberpunk 2077 at high frame rates.
The GeForce RTX 5070 Ti will be available starting at $749.
The GeForce RTX 5070 sports the powerful new cores of the NVIDIA Blackwell architecture 12GB GDDR7 memory, and has 672 GB/sec of total memory bandwidth, compared to the GeForce RTX 4070’s 504 GB/sec.
At 2560x1440, with full ray tracing and other settings maxed, and DLSS Multi Frame Generation enabled, GeForce RTX 5070 owners can play Black Myth: Wukong, Alan Wake 2, and Cyberpunk 2077 at high frame rates, with performance that is twice as fast on average compared to the GeForce RTX 4070.
The GeForce RTX 5070 will be available starting at $549.
Starting in March, the GeForce RTX 50 Series comes to laptops. As thin as 14.9mm, GeForce RTX 50 Series laptops boast up to 40% better battery life thanks to new Blackwell Max-Q innovations, and double the performance of previous-generation models.
Game with double the FPS. Create content and complete workflows in half the time. And finish generative AI tasks 2.5X faster.
Blackwell Max-Q is designed from the ground up for efficiency, delivering a massive leap in performance and battery life. There are a number of Max-Q innovations in the Blackwell architecture that combine to make this possible.
Advanced Power Gating technologies greatly reduce power by rapidly toggling unused parts of the GPU.
Blackwell has significantly faster low power states. Low Latency Sleep allows the GPU to go to sleep more often, saving power even when the GPU is being used. This reduces power for gaming, Small Language Models (SLMs), and other creator and AI workloads on battery.
Accelerated Frequency Switching boosts performance by adaptively optimizing clocks to each unique workload at microsecond level speeds.
Voltage Optimized GDDR7 tunes graphics memory for optimal power efficiency with ultra low voltage states, delivering a massive jump in performance compared to last-generation’s GDDR6 VRAM.
Altogether, battery life is boosted by up to 40% compared to previous generation GeForce RTX Laptops.
The GeForce RTX 5090 Laptop GPU is the ultimate GeForce Laptop GPU for gaming and creating. 10,496 CUDA Cores, 1,824 AI TOPS, and 24GB of GDDR7 memory give it unmatched capabilities and performance. Creative and generative AI workloads benefit from the largest ever frame buffer in a laptop, and with the first ever deployment of three NVIDIA NVENC encoders in a laptop, video workflows can be completed in record time.
The GeForce RTX 5080 Laptop GPU boasts 7,680 CUDA cores, 1,334 AI TOPS, and 16GB of GDDR7 memory, with 2X the performance of the GeForce RTX 4080 Laptop GPU. For creators, the 2X increase in memory bandwidth will benefit 3D rendering and video editing.
GeForce RTX 5070 Family Laptop GPUs include two different GPUs. The GeForce RTX 5070 Ti Laptop GPU boasts 5,888 CUDA cores, 992 AI TOPS, and 12GB of GDDR7 memory. And the GeForce RTX 5070 Laptop GPU boasts 4,608 CUDA cores, 798 AI TOPS, and 8GB of GDDR7 memory. 
GeForce RTX 5090, GeForce RTX 5080, and GeForce RTX 5070 Ti laptops will be available starting in March, followed by GeForce RTX 5070 Laptops in April. There will be designs from the world’s top manufacturers, including Acer, ASUS, Dell, GIGABYTE, HP, Lenovo, MECHREVO, MSI, and Razer. Stay tuned to their websites for further details about the GeForce RTX 50 Series Laptops they’re creating.
Leveraging the new capabilities of the Blackwell architecture, GeForce RTX 50 Series users will enjoy record-breaking performance, even more responsive gameplay, movie-quality visuals, and new applications of AI that further enhance games and apps.
NVIDIA DLSS is a suite of neural rendering technologies powered by GeForce RTX Tensor Cores that boosts frame rates while delivering crisp, high-quality images that rival native resolution rendering, as part of over 700 RTX games and apps.
At CES 2025, we’re advancing DLSS once again with the introduction of DLSS 4, featuring Multi Frame Generation for GeForce RTX 50 Series GPUs. DLSS Multi Frame Generation generates up to three additional frames per traditionally rendered frame, working in unison with the complete suite of DLSS technologies to multiply frame rates by up to 8X over traditional brute-force rendering. This massive performance improvement on GeForce RTX 5090 graphics cards unlocks stunning 4K 240 FPS fully ray-traced gaming.
On the GeForce RTX 5090, DLSS 4 with Multi Frame Generation multiplies performance by over 8X versus traditional brute force rendering in this Cyberpunk 2077 scene, PC latency is halved for more responsive gameplay, and image quality is further enhanced
DLSS 4 also introduces the biggest upgrade to its AI models since the release of DLSS 2.0 in 2020.
DLSS Ray Reconstruction, DLSS Super Resolution, and DLAA will now be powered by the graphics industry’s first real-time application of ‘transformers’, the same advanced architecture powering frontier AI models like ChatGPT, Flux, and Gemini. DLSS transformer models improve image quality with improved temporal stability, less ghosting, and higher detail in motion.
Watch NVIDIA’s Bryan Catanzaro and Edward Liu walk through DLSS 4
Get all the details here, in our DLSS 4 announcement article.
In competitive games, a few milliseconds of input lag can mean the difference between victory and defeat.
In 2020, we released NVIDIA Reflex, an innovative technology that reduces PC latency in top competitive games by an average of 50%. NVIDIA Reflex accomplishes this by synchronizing CPU and GPU work, so player actions are reflected in-game quicker, giving gamers a competitive edge in multiplayer games, and making single-player titles more responsive.
At CES 2025, we’re unveiling NVIDIA Reflex 2, which can reduce PC latency by up to 75%. Reflex 2 combines Reflex Low Latency mode with a new Frame Warp technology, further reducing latency by updating the rendered game frame based on the latest mouse input right before it is sent to the display.
Learn more in our NVIDIA Reflex 2 announcement article.
The GeForce RTX 50 Series revolutionizes creative workflows thanks to new NVIDIA Studio tools and features for creators, and even faster hardware.
Added hardware support for encoding and decoding the 4:2:2 pro-grade color format yields a staggering 11X encoding speed increase compared to software encoders.
9th Gen NVENC video encoders include a 5% improvement to HEVC and AV1 encoding quality, and a new AV1 Ultra Quality mode that offers an additional 5% improvement to encoding efficiency. And the 6th Gen NVIDIA decoder is capable of decoding and playing back up to eight 4K60 4:2:2 video streams simultaneously.
Livestreamers get 2 new AI upgrades: an AI Agent from Streamlabs - the Intelligent Streaming Assistant powered by NVIDIA ACE and Inworld AI - capable of joining you on stream as a sidekick, helping you produce your streams, or assisting you with any technical issues. And a new NVIDIA Broadcast upgrade, which offers 2 new effects - Studio voice to improve your microphone quality, and Virtual Key Light to relight you with AI.
DLSS Multi Frame Generation further boosts viewport framerates in creative apps - paired with new-generation RT Cores, GeForce RTX 50 Series GPUs deliver 2X the performance over the previous generation when editing 3D scenes in applications like D5 Render.
NVIDIA RTX Remix is adding RTX Neural Rendering technologies and even more features in the near future.
RTX Video, which upscales video by removing compression artifacts and sharpening edges, and RTX Video HDR, which converts standard dynamic range video into vibrant high dynamic range, is getting an update to decrease GPU usage by 30%. And RTX Video Super Resolution can now upscale HDR10 video content as well.
Get further details in the new GeForce RTX 50 Series NVIDIA Studio blog.
NVIDIA ACE is a suite of digital human technologies that bring game characters and digital assistants to life with generative AI. Powered by GeForce RTX AI PCs and laptops, ACE characters use AI to perceive, plan and act like human players. Play cooperatively with AI companions, battle enemies that continuously learn from player behavior, and interact with autonomous NPCs driven by their own motivations and objectives in persistent living worlds.
Head to our NVIDIA ACE games article to see how NVIDIA ACE technologies are being used to create autonomous AI companions, bosses, and more in inZOI, MIR5, NARAKA: BLADEPOINT MOBILE PC VERSION, PUBG: BATTLEGROUNDS, and more.
Project G-Assist will add an experimental AI assistant to your GeForce RTX AI PC. Accessible via the NVIDIA app overlay, the experimental release of Project G-Assist enables you to command your system and optimize its performance and efficiency with simple commands. Chart frame rates, latency, power consumption, and other performance statistics, and control connected peripherals and accessories, including peripheral lighting, fan speeds, and even home LED lighting.
To demonstrate how RTX enthusiasts and developers can use NIM microservices to build AI agents and assistants, we previewed Project R2X, a vision-enabled PC avatar that can put information at a users’ fingertips, assist with desktop apps and video conference calls, read and summarize documents, and more.
R2X can be connected to cloud AI services such as OpenAI’s GPT4o and xAI’s Grok, and NIM microservices and AI Blueprints, such as PDF retrievers or alternative LLMs, via developer frameworks such as CrewAI, Flowise AI, and Langflow. Sign up to follow along with Project R2X updates.
With double the performance of previous-generation GPUs, 40% longer laptop battery life, and a suite of neural rendering capabilities, GeForce RTX 50 Series products are game changing. Your performance in today’s games and apps multiplies massively with DLSS Multi Frame Generation, FP4, and other enhancements. And in the rapidly approaching future, you can take full advantage of RTX Neural Rendering to game and create with film-quality visuals.
Stay tuned to GeForce.com for further information about the GeForce RTX 50 Series as we approach the release of our new generation of GPUs and laptops, and for all our future game and technology announcements.
In the meantime, check out all of our other GeForce RTX 50 Series announcements to see how we’re further improving your experiences in games and apps, and delivering new innovations that advance the PC industry."
"general","article","NVIDIA Blackwell GeForce RTX 50 Series Opens New World of AI Computer Graphics","https://nvidianews.nvidia.com/news/nvidia-blackwell-geforce-rtx-50-series-opens-new-world-of-ai-computer-graphics","CES—NVIDIA today unveiled the most advanced consumer GPUs for gamers, creators and developers — the GeForce RTX™ 50 Series Desktop and Laptop GPUs.
Powered by the NVIDIA Blackwell architecture, fifth-generation Tensor Cores and fourth-generation RT Cores, the GeForce RTX 50 Series delivers breakthroughs in AI-driven rendering, including neural shaders, digital human technologies, geometry and lighting.
“Blackwell, the engine of AI, has arrived for PC gamers, developers and creatives,” said Jensen Huang, founder and CEO of NVIDIA. “Fusing AI-driven neural rendering and ray tracing, Blackwell is the most significant computer graphics innovation since we introduced programmable shading 25 years ago.”
The GeForce RTX 5090 GPU — the fastest GeForce RTX GPU to date — features 92 billion transistors, providing over 3,352 trillion AI operations per second (TOPS) of computing power. Blackwell architecture innovations and DLSS 4 mean the GeForce RTX 5090 GPU outperforms the GeForce RTX 4090 GPU by up to 2x.
GeForce Blackwell comes to laptops with all the features of desktop models, bringing a considerable upgrade to portable computing, including extraordinary graphics capabilities and remarkable efficiency. The Blackwell generation of NVIDIA Max-Q technology extends battery life by up to 40%, and includes thin and light laptops that maintain their sleek design without sacrificing power or performance.
NVIDIA DLSS 4 Boosts Performance by Up to 8x
DLSS 4 debuts Multi Frame Generation to boost frame rates by using AI to generate up to three frames per rendered frame. It works in unison with the suite of DLSS technologies to increase performance by up to 8x over traditional rendering, while maintaining responsiveness with NVIDIA Reflex technology.
DLSS 4 also introduces the graphics industry’s first real-time application of the transformer model architecture. Transformer-based DLSS Ray Reconstruction and Super Resolution models use 2x more parameters and 4x more compute to provide greater stability, reduced ghosting, higher details and enhanced anti-aliasing in game scenes. DLSS 4 will be supported on GeForce RTX 50 Series GPUs in over 75 games and applications the day of launch.
NVIDIA Reflex 2 introduces Frame Warp, an innovative technique to reduce latency in games by updating a rendered frame based on the latest mouse input just before it is sent to the display. Reflex 2 can reduce latency by up to 75%. This gives gamers a competitive edge in multiplayer games and makes single-player titles more responsive.
Blackwell Brings AI to Shaders
Twenty-five years ago, NVIDIA introduced GeForce 3 and programmable shaders, which set the stage for two decades of graphics innovation, from pixel shading to compute shading to real-time ray tracing. Alongside GeForce RTX 50 Series GPUs, NVIDIA is introducing RTX Neural Shaders, which brings small AI networks into programmable shaders, unlocking film-quality materials, lighting and more in real-time games.
Rendering game characters is one of the most challenging tasks in real-time graphics, as people are prone to notice the smallest errors or artifacts in digital humans. RTX Neural Faces takes a simple rasterized face and 3D pose data as input, and uses generative AI to render a temporally stable, high-quality digital face in real time.
RTX Neural Faces is complemented by new RTX technologies for ray-traced hair and skin. Along with the new RTX Mega Geometry, which enables up to 100x more ray-traced triangles in a scene, these advancements are poised to deliver a massive leap in realism for game characters and environments.
The power of neural rendering, DLSS 4 and the new DLSS transformer model is showcased on GeForce RTX 50 Series GPUs with Zorah, a groundbreaking new technology demo from NVIDIA.
Autonomous Game Characters
GeForce RTX 50 Series GPUs bring industry-leading AI TOPS to power autonomous game characters in parallel with game rendering.
NVIDIA is introducing a suite of new NVIDIA ACE technologies that enable game characters to perceive, plan and act like human players. ACE-powered autonomous characters are being integrated into KRAFTON’s PUBG: BATTLEGROUNDS and InZOI, the publisher’s upcoming life simulation game, as well as Wemade Next’s MIR5.
In PUBG, companions powered by NVIDIA ACE plan and execute strategic actions, dynamically working with human players to ensure survival. InZOI features Smart Zoi characters that autonomously adjust behaviors based on life goals and in-game events. In MIR5, large language model (LLM)-driven raid bosses adapt tactics based on player behavior, creating more dynamic, challenging encounters.
AI Foundation Models for RTX AI PCs
Showcasing how RTX enthusiasts and developers can use NVIDIA NIM microservices to build AI agents and assistants, NVIDIA will release a pipeline of NIM microservices and AI Blueprints for RTX AI PCs from top model developers such as Black Forest Labs, Meta, Mistral and Stability AI.
Use cases span LLMs, vision language models, image generation, speech, embedding models for retrieval-augmented generation, PDF extraction and computer vision. The NIM microservices include all the necessary components for running AI on PCs and are optimized for deployment across all NVIDIA GPUs.
To demonstrate how enthusiasts and developers can use NIM to build AI agents and assistants, NVIDIA today previewed Project R2X, a vision-enabled PC avatar that can put information at a user’s fingertips, assist with desktop apps and video conference calls, read and summarize documents, and more.
AI-Powered Tools for Creators
The GeForce RTX 50 Series GPUs supercharge creative workflows. RTX 50 Series GPUs are the first consumer GPUs to support FP4 precision, boosting AI image generation performance for models such as FLUX by 2x and enabling generative AI models to run locally in a smaller memory footprint, compared with previous-generation hardware.
The NVIDIA Broadcast app gains two AI-powered beta features for livestreamers: Studio Voice, which upgrades microphone audio, and Virtual Key Light, which relights faces for polished streams. Streamlabs is introducing the Intelligent Streaming Assistant, powered by NVIDIA ACE and Inworld AI, which acts as a cohost, producer and technical assistant to enhance livestreams.
Availability
For desktop users, the GeForce RTX 5090 GPU with 3,352 AI TOPS and the GeForce RTX 5080 GPU with 1,801 AI TOPS will be available on Jan. 30 at $1,999 and $999, respectively.
The GeForce RTX 5070 Ti GPU with 1,406 AI TOPS and GeForce RTX 5070 GPU with 988 AI TOPS will be available starting in February at $749 and $549, respectively.
The NVIDIA Founders Editions of the GeForce RTX 5090, RTX 5080 and RTX 5070 GPUs will be available directly from nvidia.com and select retailers worldwide.
Stock-clocked and factory-overclocked models will be available from top add-in card providers such as ASUS, Colorful, Gainward, GALAX, GIGABYTE, INNO3D, KFA2, MSI, Palit, PNY and ZOTAC, and in desktops from system builders including Falcon Northwest, Infiniarc, MAINGEAR, Mifcom, ORIGIN PC, PC Specialist and Scan Computers.
Laptops with GeForce RTX 5090, RTX 5080 and RTX 5070 Ti Laptop GPUs will be available starting in March, and RTX 5070 Laptop GPUs will be available starting in April from the world’s top manufacturers, including Acer, ASUS, Dell, GIGABYTE, HP, Lenovo, MECHREVO, MSI and Razer.
About NVIDIA
NVIDIA (NASDAQ: NVDA) is the world leader in accelerated computing.

Certain statements in this press release including, but not limited to, statements as to: the benefits, impact, performance and availability of NVIDIA’s products, services, and technologies, including GeForce RTX 50 Series Desktop and Laptop GPUs, NVIDIA Blackwell architecture, fifth-generation Tensor Cores, fourth-generation RT Cores, GeForce RTX 5090 Founders Edition GPU, NVIDIA DLSS 4, NVIDIA Reflex, DLSS Multi Frame Generation, DLSS Super Resolution and Ray Reconstruction models, NVIDIA Reflex 2, RTX Neural Shaders, RTX Neural Faces, RTX Mega Geometry, ACE technologies, NVIDIA NIM microservices, Project R2X, RTX 40 Series GPUs, NVIDIA RTX Remix modding platform and D5 Render, NVIDIA Broadcast, Studio Voice, Virtual Key Light, GeForce Blackwell, NVIDIA Max-Q technology, GeForce RTX 5090, GeForce RTX 5080, GeForce RTX 5070 Ti, and GeForce RTX 5070, GeForce RTX 5090 Laptop GPU, GeForce RTX 5080 Laptop GPU, GeForce RTX 5070 Ti Laptop GPU, GeForce RTX 5070 Laptop GPU; and third parties adopting NVIDIA’s products and technologies are forward-looking statements that are subject to risks and uncertainties that could cause results to be materially different than expectations. Important factors that could cause actual results to differ materially include: global economic conditions; our reliance on third parties to manufacture, assemble, package and test our products; the impact of technological development and competition; development of new products and technologies or enhancements to our existing product and technologies; market acceptance of our products or our partners' products; design, manufacturing or software defects; changes in consumer preferences or demands; changes in industry standards and interfaces; unexpected loss of performance of our products or technologies when integrated into systems; as well as other factors detailed from time to time in the most recent reports NVIDIA files with the Securities and Exchange Commission, or SEC, including, but not limited to, its annual report on Form 10-K and quarterly reports on Form 10-Q. Copies of reports filed with the SEC are posted on the company's website and are available from NVIDIA without charge. These forward-looking statements are not guarantees of future performance and speak only as of the date hereof, and, except as required by law, NVIDIA disclaims any obligation to update these forward-looking statements to reflect future events or circumstances.
Many of the products and features described herein remain in various stages and will be offered on a when-and-if-available basis. The statements above are not intended to be, and should not be interpreted as a commitment, promise, or legal obligation, and the development, release, and timing of any features or functionalities described for our products is subject to change and remains at the sole discretion of NVIDIA. NVIDIA will have no liability for failure to deliver or delay in the delivery of any of the products, features or functions set forth herein.
© 2025 NVIDIA Corporation. All rights reserved. NVIDIA, the NVIDIA logo GeForce and NVIDIA NIM are trademarks and/or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Features, pricing, availability and specifications are subject to change without notice.
About NVIDIA
NVIDIA (NASDAQ: NVDA) is the world leader in accelerated computing.

Certain statements in this press release including, but not limited to, statements as to: the benefits, impact, performance and availability of NVIDIA’s products, services, and technologies, including GeForce RTX 50 Series Desktop and Laptop GPUs, NVIDIA Blackwell architecture, fifth-generation Tensor Cores, fourth-generation RT Cores, GeForce RTX 5090 Founders Edition GPU, NVIDIA DLSS 4, NVIDIA Reflex, DLSS Multi Frame Generation, DLSS Super Resolution and Ray Reconstruction models, NVIDIA Reflex 2, RTX Neural Shaders, RTX Neural Faces, RTX Mega Geometry, ACE technologies, NVIDIA NIM microservices, Project R2X, RTX 40 Series GPUs, NVIDIA RTX Remix modding platform and D5 Render, NVIDIA Broadcast, Studio Voice, Virtual Key Light, GeForce Blackwell, NVIDIA Max-Q technology, GeForce RTX 5090, GeForce RTX 5080, GeForce RTX 5070 Ti, and GeForce RTX 5070, GeForce RTX 5090 Laptop GPU, GeForce RTX 5080 Laptop GPU, GeForce RTX 5070 Ti Laptop GPU, GeForce RTX 5070 Laptop GPU; and third parties adopting NVIDIA’s products and technologies are forward-looking statements that are subject to risks and uncertainties that could cause results to be materially different than expectations. Important factors that could cause actual results to differ materially include: global economic conditions; our reliance on third parties to manufacture, assemble, package and test our products; the impact of technological development and competition; development of new products and technologies or enhancements to our existing product and technologies; market acceptance of our products or our partners' products; design, manufacturing or software defects; changes in consumer preferences or demands; changes in industry standards and interfaces; unexpected loss of performance of our products or technologies when integrated into systems; as well as other factors detailed from time to time in the most recent reports NVIDIA files with the Securities and Exchange Commission, or SEC, including, but not limited to, its annual report on Form 10-K and quarterly reports on Form 10-Q. Copies of reports filed with the SEC are posted on the company's website and are available from NVIDIA without charge. These forward-looking statements are not guarantees of future performance and speak only as of the date hereof, and, except as required by law, NVIDIA disclaims any obligation to update these forward-looking statements to reflect future events or circumstances.
Many of the products and features described herein remain in various stages and will be offered on a when-and-if-available basis. The statements above are not intended to be, and should not be interpreted as a commitment, promise, or legal obligation, and the development, release, and timing of any features or functionalities described for our products is subject to change and remains at the sole discretion of NVIDIA. NVIDIA will have no liability for failure to deliver or delay in the delivery of any of the products, features or functions set forth herein.
© 2025 NVIDIA Corporation. All rights reserved. NVIDIA, the NVIDIA logo GeForce and NVIDIA NIM are trademarks and/or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Features, pricing, availability and specifications are subject to change without notice.
Newsroom updates delivered to your inbox."
"general","article","Computex 2025 Live – Our Picks for Best of Show and All the Biggest News About the Future of Computing","https://www.tomsguide.com/news/live/computex-2025","When you purchase through links on our site, we may earn an affiliate commission. Here’s how it works.

Computex 2025 is almost over, and we're on the ground in Taipei going hands-on with the latest and greatest tech — getting an idea for what the future of computing looks like for you.
Now, with over 250 devices being shown off, we've done the legwork and selected our picks for the best of Computex. These are the biggest and most innovative products of the year, so definitely keep these on your radar.
Whether it's a surprise glimpse of the Intel CPUs that will be powering the laptops of 2026, the unveiling of MSI's first AMD-fueled gaming handheld (the Claw A8) or the war between AMD's Radeon RX 9060 XT and Nvidia's RTX 5060 gaming GPUs, this has been a massive year for the biggest computing show on Earth.
For every announcement as it happened (and continues to happen for the final day), keep it locked to Tom's Guide!
It's Day minus-one of Computex 2025 and we're live from Taipei ahead of the Nvidia GTC keynote kicking off this morning at 11am. But before all of that, we're already seeing plenty of news come out.
You can expect much more news to come out today, alongside plenty of hands-on time with the latest announcements (and of course the Qualcomm keynote too). So keep it locked!
While we're already expecting the launch of Nvidia's latest RTX 5060 GPU during the big keynote at Computex 2025, a big question is whether Team Green will have a hidden surprise up its sleeve. Namely, Nvidia's own laptop CPU.
Rumors of the Windows on Arm chip popped up recently, stating that it could come with a modified version of the GB10 Superchip in Nvidia's announced Project DIGITS AI supercomputer (now known as DGX Spark) for desktops.
In partnership with MediaTek, this chip could feature 8 or 12 CPU cores (instead of 20 in the GB10), along with possibly a quarter of the 128GB of RAM, so 32GB or even 16GB. There have been benchmark leaks, too, with speeds at 3.9 GHz, single-core performance reaching 2,960 and multicore at 10,682.
Either way, if Nvidia does deliver its own CPU for laptops, it could be a game-changer. Stay tuned until we find out!
It isn't just Nvidia's big keynote that's happening today, as we're eagerly waiting for what Qualcomm has to say about its state of CPUs. And if we had to guess, a Snapdragon X2 chip would be a a great rival against a possible Nvidia laptop chip.
Not much is known about a next-gen Snapdragon CPU (a leaked shipping manifest states it could happen), but if it surfaces, it's likely to offer greater processing power driven by AI, even further compatibility with AI-driven apps, better battery efficiency and, possibly, better gaming performance.
That's just us being skeptical, but if it's in any way better than the first swoop of Snapdragon X Elite and Plus chips for Copilot+ PCs, Qualcomm will be bringing out a CPU to keep on your radar — and Nvidia will already have some competition if it releases its own CPU.
We're here bright and early for Nvidia's keynote at Computex 2025, and from the looks of things, among the first!
As a reminder, CEO Jensen Huang will be taking center stage on May 19 at 11 a.m. in Taiwan time (May 18 at 11 p.m. ET / 8 p.m. PT). There's still a while to wait, but we'll keep you posted on all the latest happenings.
We've seen the full suite of Nvidia's RTX 50-series GPUs, including everything from RTX 5090 and RTX 5060 Ti GPUs to RTX 5090 gaming laptops playing Doom: The Dark Ages. But there's still another GPU up Team Green's sleeve: The RTX 5060.
The entry-level GPU is already announced to be released sometime this May, and that sets it up quite nicely for Computex 2025. It will be priced from $299 and come with 8GB of GDDR7 VRAM, but DLSS 4 is expected to take on a lot of the legwork to boost those frame rates.
We're sure more will be revelaed once Jensen Huang takes the stage. We'll be keeping an eye on the latest!
So, how does an RTX 5060 perform? Considering it's Nvidia's lower-end GPU with a $299 price tag and 8GB of GDDR7 VRAM, it may not seem like much, but we've already put it to the test in 5 games at 1080p.
And games run buttery smooth! In our RTX 5060 impressions, we tried Avowed, Cyberpunk 2077, Doom: The Dark Ages, Hogwarts Legacy and Marvel Rivals. We saw triple-digit numbers in terms of FPS, even maintaining over 200 FPS with DLSS 4 and multi-frame generation 4x. And that was even at Ultra Nightmare settings in Doom: The Dark Ages at 1080p.
Of course, when playing games that support Nvidia's AI trickery, this is to be expected. It even achieved just over Nvidia's claimed 149 FPS in Cyberpunk 2077. For a more affordable GPU, this won't be a bad shout. Anyway, we're sure to hear more claims about Nvidia's latest graphics card at the keynote!
We've now got a spot for the Nvidia keynote, and a great view of everything CEO Jensen Huang has to say for Nvidia's year ahead. Also, we're liking the GTC Taipei 2025 swag that comes with the seat.
While we wait for the keynote to kick off, Nvidia has a ""Pregame"" chat with Acer and Asus going on right now. It's all about AI, robotics and accelerated computing, and may even offer a sneak peek at what's to come. Plus, a little insight from leaders across Taiwan’s tech can go a long way!
Nvidia clearly has a lot to show off this year, and it's not all to do with RTX 50-series GPUs. During Nvidia GTC 2025, we got a taste of Nvidia Isaac Groot N1, a new model for robotics, and we even got to see this Galbot robot in action serving popcorn not too long ago.
Will Nvidia's robotics be serving more than just popcorn at the keynote? Hoepfully yes, but we'd still like the popcorn, please.
The pregame is still going strong, but Jensen Huang popped up talking about how long it takes to seat everyone. Apparently, in the ballpark or two hours! And we can confirm considering we arrived their well beforehand just to get a good seat.
Now, Jensen, get prepped for the show — you've got less than an hour!
We got a look at Nvidia's AI supercomputer, Project DIGITS (now known as DGX Spark), during CES, boasting the GB10 Grace Blackwell superchip. That's expected to set you back around $3,000, but if Nvidia makes its own chip for laptops, there could be a chip here that's far more affordable. That is, if Nvidia announces something along these lines.
We're less than 5 minutes away from the livestream kicking off, and we're in clear view of everything that's coming. In the meantime, check out what else Nvidia has in store, including Nvidia Reflex 2, which cuts latency in half (and then some). We tried it out while playing The Finals, and seeing those single digits was a treat.
Only 15 minutes until Nvidia takes to the stage, and the room is piling up (as you can see below). Now's the time to get prepared and put on the livestream (it's currently treating us to some nice music and graphics on display).
That's our own Jason England at the keynote! And yes, we very much meant that this place was packed. 
Wait, is Nvidia using AMD Wi-Fi? Anyway, get ready for all things to kick off in under a minute!
We're starting off with an inspirational speech from an AI face, all about what Nvidia can do with its AI and robotics. Including a special appearance from Blue at Disney!
And there he is, CEO Jensen Huang on the stage (with some great music). In fact his parents are even in the audience. Nice shout out.
""We're going to talk about great partners...and we're at the center of the computer ecosystem."" In better news, there are some surprises for us! Nvidia laptop CPU, by chance? Also expect AI and robotics, by the way
A GPU chip to AI infrastructure, and ""new products that creates new markets"" for Nvidia? Yeah, there's something big up Huang's sleeve
Jensen talks about how Nvidia's roadmap so everyone in the world can go off to build data centers, saying Team Green is an essential for AI all around the globe. Even calling back to the industrial revolution and the makings of electricity. Well, Nvidia wants to be the same.
""We need AI EVERYWHERE!"" says Jensen Huang, just like the internet and electricity. That'll be what we all say in 10 years time, he says.
What makes Nvidia special is the fusion of its AI and accelerated computing. And now we're getting a sneak peek at what's set to be announced, including graphics that are all artificially made (including a sweet Mercedes).
Jensen has an RTX 5060 GPU in his hand and a new MSI laptop with a RTX 5060 in it! We heard it would arrive in May, but now it's looking like a fore-sure launch
Now Huang is talking about accelerated computing with its CUDA-X libraries, with it being one of the ""many children"" under the CEO's wing. As, you know, it isn't all about GPUs, as he claims! This includes Earth-2 for weather analytics, Megatron Dynamo for deep learning and more.
That's the RTX 5060 in hand, and that background? That's all through graphics.
Those mentioned CUDA libraries can help bring AI to 5G, and even 6G networks, as well as computing, no less. But all through Nvidia!
Forget agentic AI (not really), Nvidia is talking about Physical AI. The physical embodiment of this is through robots, using AI to navigate obstacles and using it for thinking. That all needs a lot of computing power, hence Nvidia's Grace Blackwell: (A Thinking Machine).
Here's a look at the Grace Blackwell Ultra Superchip, which came up through the floor as Jensen summoned.
The internet as a whole shifts 900 terabits of information per second. In Jensen's words, this NV Link Spine, the backbone of what powers Nvidia's power, moves more traffic with 9 NV Link switches.
Now a little movie that Jensen (apparently) made for us about how Blackwell is made, from factory to finish, including at TSMC and Foxconn.
Nvidia is now building AI for Taiwan, the first giant AI supercomputer, for the AI infrastructure and ecosystem of Taiwan.
Nvidia announces NV Link Fusion. It will aid in building AI infrastructure instead of just singular, specific super chips. It will be able to integrate all companies' custom CPUs and needs, fitting ""right into an ecosystem"" to incorporate Nvidia GPUs.
DGX Spark, that Project DIGITS Nvidia talked about during CES, will be available in the coming weeks. It's for those who want to have their own supercomputer, and companies will have their own prices for the PC. Everyone can have one for Christmas, Huang says (hoorah!).
Here's a look at the RTX PRO enterprise server. This is a computer for AI agents, and it aims to power through IT infrastructures. And yes, Crysis works on here, too.
Now Huang talks about its open AI models, with an example of Vast using the Nvidia blueprint to build its own AI
Now we're onto robots! And the ability to ""learn to be a good robot."" Huang talks about the Newton engine, which will be available in July. And we're getting a look at how it can ""bring these robots to life.""
What I find interesting is how the Nvidia Isaac Groot platform is using simulations for robots to work, like walking through particles to simulate how it would walk in real life. Isaac Groot is also open source for anyone to use!
We got a taste of Groot during GTC 2025, and now we're seeing what Groot Dreams can do for pre- and post-training for robots, along with 3D action trajectories for robots to learn human-like actions!
Robots will be the next multi-trillion dollar industry, but it will take a lot. But Huang believes Nvidia's robotics division is the one that's able to do it, and that's simply due to scalability. So, little robots in our kitchens doing odd jobs soon? I'll wait and see.
Nvidia is even using the simulation of Omniverse to effectively run cities in Taiwan!
Nvidia Constellation!! A new office is opening in Taipei, and no doubt this is a great choice to bring the company closer to its respective chipmakers. Huge cheers in the hometown crowd.
And it's over! We'll post a wrap up soon of everything announced.
On the show floor at Computex, things are already super busy with plenty of last-minute construction underway.
We still got a chance to check out Asus, MSI, Acer and loads of other companies ' booths while getting a sneak peak at some cool new monitors, desktops, laptops, keyboards and other cool new tech.
When we come back tomorrow bright and early, everything will be ready for the first official day of Computex 2025.
So we got a lot going on there — plenty of dense information when it comes to enterprise computing, AI and robotics, so let's try to break it all down:
But one thing Nvidia didn't announce is the rumored new Arm CPU. I thought this was going to be the ""one last thing,"" but it never came. That leaves Qualcomm with a pretty big opportunity here to deliver something significant in the form of a Snapdragon X2 tease.
Our own Darragh Murphy is in the crowd for this one, and I'll be your tour guide of everything Qualcomm has to say!
We're a little delayed starting this one! Let's see what Qualcomm's got in store
The show has started! James Huang of the Taiwanese trade council has come out to discuss how significant this show will be for the future of phones and computing!
Cristiano Amon is here, and turns out this is a significant year for Qualcomm as a whole with its 40th anniversary kicking off.
Well, this was a rather forboding quote from Amon. X2 seems unlikely...
Microsoft's CEO Satya Nadella stopped by (virtually) to big up Snapdragon-armed Copilot+ PCs too! Expect many more AI features to get announced for Windows at Microsoft Build 2025.
One of the more pesky frustrations with Snapdragon X Elite has always been the app compatibility. Sure, there's an emulator for x86 apps, but it's always been slower and less power efficient.
Now, that seems to be fading away, as 750 apps are now native to X, and 1,400 games can be played — including Fortnite coming to the platform!
Now, Asus has taken the stage to talk about its partnership with Qualcomm. If they could make the fantastic Asus Zenbook A14 cheaper, that would be splendid!
BREAKING: Doom: The Dark Ages will melt your Nvidia GPU with Path Tracing and DLSS Ray Reconstruction in June
Just in time for Nvidia’s RTX 5060, Doom: The Dark Ages is getting two significant upgrades. It was already a lightning quick performer and a brutally gorgeous game too, but that’s not enough for ID games.
Path tracing is the net big thing over the ray tracing that we’ve seen over the past few years — rather than making the light sources themselves and a couple of reflections look pretty, path tracing calculates every single light source reflection off every surface.
It’s super demanding, and it’s looking incredible in Doom. Fingers crossed it is optimized well. To optimize with that, though, Nvidia’s DLSS Ray Reconstruction is coming to the game.
Basically, instead of using the rendering cores and video memory to brute force the reflections, the AI cores are pretty good at ray and path tracing calculations, so machine-learned trickery is the way to go.
One thing you don't really realize until you see it all in front of you is just how many places Snapdragon is, and how many gadgets Qualcomm is in around you.
In a time where every company is looking for that same level of interoperability that you see between Apple devices, Qualcomm is probably in the best place to take Cupertino on.
The cake is most definitely not a lie anymore. Portal with RTX has just received a huge visual upgrade — DLSS 4 and all the transformer model visual smoothness of multi-frame generation are now available in an update.
On top of that, RTX Neural Radiance Cache is heading on over too. This could be good news for 5060 players, as the essential purpose is to use neural processing techniques to render light sources and dramatically reduce the size of them on the video memory of a GPU.
This is just one of nine games set to attend the DLSS 4 party:
Do you see that number next to the 16-inch Omnibook 5? Up to 34 hours!? Who cares about AI features when your laptop could last more than a literal day!!
Sorry, that felt mean. But we're deep into enterprise use cases here for AI — including agentic features like creating reports and uniting datapoints collected from many sources.
""This is going to fundamentally change how we do work,"" Amon says.
Meanwhile, Lenovo has just spoken about it's AI-interconnected ecosystem, and how it's working when built around Snapdragon! Things like fast sharing.
So right at the end of the Qualcomm keynote, Cristiano Amon confirmed that ""the revolution continues at Summit 2025."" Basically, he just said that the Snapdragon X2 is set to launch in September!
I sweat that with every new generation of Intel chip, the company's naming conventions sound more and more like Mario Kart levels...want proof? OK then.
Intel has given Tom's Hardware a tease of its upcoming Panther Lake Core Ultra 300 series CPUs — launching early 2026. Packing Cougar Cove performance cores and Darkmont efficiency cores, this looks set to get the next generation iGPU too.
Now, as we already know, the graphics on these Intel chips have already been surprisingly impressive, so to get the next-gen architecture is exciting to say the least!
Hi there everyone, Jeff Parsons in London taking over our Computex live blog for awhile and the first piece of news I have to bring you is a couple of new AI-powered laptops from HP, just announced at Computex.
The HP OmniBook 5 Series with Snapdragon arrive with either a 14-inch or 16-inch 2K OLED display, Snapdragon X or X Plus processors offering up a 45 TOPS NPU powering the AI performance. These are Copilot+ PCs so you'll get features like Recall (preview), Click-to-Do (preview), improved Windows Search and Cocreator in Paint. Plus HP's own AI Companion.
HP promises up to 34 hours of battery life and impressive fast charge capabilities that brings the battery up to 50% in 30 minutes with a 65W mini adapter.
Pricing for the 14-inch model starts at $799 while the 16-inch model starts at $849. Both are expected to be available in July.
The Tom's Guide team was present at Nvidia's Computex keynote earlier today, where CEO Jensen Huang took to the stage to officially reveal the RTX 5060 — along with a lot of other stuff.
We're tracking where to buy the RTX 5060 on a separate live blog to help you secure yours, but if you want to see some of the other stuff Huang spoke about (there was a lot of AI) then the entire keynote is now available to view below.
Probably the coolest part is Huang's digital background which, he said, had nine out of every 10 frames generated by DLSS.
Well, Asus' ROG brand has just gone large at Computex 2025 with details about all its next-gen gaming laptops.
The company unveiled the next-gen ROG Zephyrus, ROG Strix and TUF lineups. Let's dive into what's available.
The new ROG Zephyrus G14 is that sleek, silver beast that's just a scant 0.63-inches thin but packs an AMD Ryzen AI 9 270 CPU alongside an RTX 5060 GPU, up to 16GB of RAM and a 1TB SSD.
The ROG Strix G16 and G18 come with a choice of either an Intel Core Ultra 9 Processor 275HX or AMD Ryzen 9 9955HX Processor. They'll both get an Nvidia RTX GPU, up to 32GB of RAM and that sweet ROG NEbula Display with up to a 240Hz refresh rate and 3ms response time.
Finally, there's a big announcement from the TUF side of things. The TUF Gaming A18 marks the first TUF-branded 18-inch gaming laptop from the brand and it'll boast an AMD Ryzen 7 260, RTX 5060 GPU, up to 16GB of DDR5 RAM, and 1TB of storage. Meanwhile, the TUF Gaming F16, A16 and A14 are also getting refreshes.
Those are just the laptops. We've also got pre-built gaming PCs and some handheld news from Asus as well...
Here's Team TG (from left: Paul, Jason and Anthony) at Asus HQ in Taiwan after going hands-on with the brand's entire new range. I'm getting strong rock-rap album cover vibes from this picture.
Expect plenty of thoughts from them on all of Asus' new tech over the next couple of days and make sure you're checking out our TikTok for fresh videos straight from Computex as the team get onto the show floor tomorrow.
Back to business now and Intel has unveiled a new lineup of GPUs aimed at professionals and developers.
The Arc Pro B60 and B50 GPUs are based on the Xe2 architecture and feature Intel Xe Matrix Extensions (XMX), AI cores and advanced ray tracing units.
Designed for ""demanding AI inference workloads and workstation applications"", the cards feature 24GB and 16GB of memory respectively and multi-GPU scalability.
Asus has arrived at Computex with this mongrel of a machine: it's the TUF Gaming T500 desktop that pairs an Nvidia RTX 5060 Ti desktop GPU with an Intel Core i7-13620H laptop CPU.
A laptop CPU and a desktop GPU? There's no explanation as to why Asus has opted for an Intel mobile processor in the TUF Gaming T500, but our computing editor Darragh Murphy believes it'll have something to do with its cost.
""While there's no official word on when it will be available yet, this Asus desktop is sure to cater towards budget gaming,"" Darragh says.
This right here is the Asus ROG Strix Ace XG248QSG and it's committed to one thing and one thing only: being as fast as possible with a refresh rate 100 times quicker than the average human can blink. That's right, it has a refresh rate of up to 610Hz.
We got some eyes-on time with it at Computex and suffice to say, this thing will probably be north of a grand when we finally learn the price.
Let's stick with monitors for a second. MSI has shown off its new MPG 271QR QD-OLED X50 gaming monitor at Computex which uses AI smarts to prevent burn-in.
The company calls it the ""AI Care Sensor"" and it enables “real-time human detection, dynamically adjusting settings to protect the monitor and boost efficiency.”
Basically, there’s an always-on CMOS sensor built into the bezel that captures images every 0.2 seconds. This is doing two things – real-time human detection so it can dim or even power down when you leave, and it can even analyze the color hue of the light in your room and reduce blue light to match. Clever!
If you haven't started yet, make sure you follow Tom's Guide on TikTok. We'll be showing you the coolest things from the show floor all this week.
Like that giant mechanical keyboard from Red Bull...
While we wait for the RTX 5060 cards to drop, we've been looking at this monster of a machine — the limited-edition Nvidia RTX 5080 GPU that Asus has created to mark the launch of Doom: The Dark Ages. Only 666 units will ever exist.
My colleague Darragh Murphy has spent a little time with Doom on both an MSI Stealth A16 AI+ with an RTX 5070 Ti and an Asus ROG Zephyrus G16 with an RTX 5080. Find out how he got on when he put them to the test.
Howdy! Alyse Stanley in Chicago taking over our Computex live blog for a bit, and the first piece of news I have for you is that Project G-Assist, Nvidia's handy locally-run AI digital assistant for PCs, is getting a serious upgrade. Tools for developers to build their own plugins are now publicly available, so we should see even more functionality emerge. It definitely has the scope to be a big deal for fans of the best PC games.
Nvidia announced during Computex 2025 that its RTX 5060 is going on sale today, both in desktop and laptop GPU form, promising frugal buyers frame rates in the 100s for the 100-plus games that support its DLSS 4 optimization techniques.
But stock is selling fast. Be sure to keep an eye on our guide to buying an RTX 5060 to snag yours while you can.
Asus has finally refreshed the budget TUF gaming lineup with improved thermals, more power and up to an Nvidia RTX 5060 GPU. We went hands-on with the new Asus TUF A14 at Computex 2025, and it's shaping up to be the best budget gaming laptop to beat. Check out our impressions below.
Nvidia made it clear during its presentation that it's betting big on AI. The company has launched a new version of its NVLink tech called NVLink Fusion to build powerful custom AI systems with multiple chips linked together. The program was previously closed to chips made by Nvidia, but now customers and chip designers can use non-Nvidia central processing units and graphics processing units together with Nvidia’s products and its NVLink.
“NVLink fusion is so that you can build semi-custom AI infrastructure, not just semi-custom chips,” Nvidia CEO Jensen Huang said at Computex 2025 in Taiwan. With NVLink Fusion, AI infrastructures can combine Nvidia processors with different CPUs and application-specific integrated circuits. “In any case, you have the benefit of using the NV link infrastructure and the NV link ecosystem,"" Huang said.
Asus had a lot of exciting announcements at Computex 2025, but easily our favorite is this upgraded Asus TUF A14 gaming laptop. We've been eagerly waiting for Asus to update its TUF line-up of budget gaming laptops, and this one does not disappoint with its improved thermals, AI-powered upgrades, and up to an Nvidia RTX 5060 GPU.
Good morning from Taipei! There's already been so much news at Computex that it may be weird hearing me say it's officially day one today. The exhibition center opens and we get to try all the latest and greatest in the world of computing.
Today is going to be a significant one for gaming handhelds, laptops and all kinds of brand new tech, so keep it locked on Tom's Guide as we will be your tour guides through it all.
Here's everything we've talked about so far:
We headed into Computex 2025 believing we'd finally see the Asus ROG Ally 2 (with a dedicated Xbox button) boasting the hugely anticipated AMD Ryzen Z2 Extreme, but it turns out the MSI Claw A8 beat it to the punch.
In case you missed it, AMD's Ryzen Z2 Extreme is set to deliver a significant bump in performance to gaming handhelds, and we'll finally see it firsthand with the new Claw A8. Oh, and it comes with up to 24GB of RAM, an 8-inch display with a 120Hz refresh rate and an 80Wh battery for good measure.
We'll be keeping an eye out for a better look at the handheld while we're on the ground, so stay tuned!
Computex always has a few surprises up its sleeve, but a smart ring from Acer? That's unexpected, but we're happy to see the Acer FreeSense Ring — especially with that $199 price tag and subscription-free health data (watch out, Oura Ring 4).
After going hands-on with the smart ring, it's clear Acer has done its homework on all the best smart rings out there. It's lightweight (and stylish in black and rose gold), offers plenty of health monitoring and comes with a 4-day battery life after a 1-hour charge.
We'll have more thoughts once we finally give it a proper try, with it aiming to arrive sometime in August.
Well, I said we'd be on the lookout for the new MSI Claw A8 BZ2EM, and here it is! And it's coming in that eye-catching neon colorway. We're just checking it out, but one thing is for sure: that AMD Ryzen Z2 Extreme already makes gaming mighty impressive.
If you blink while using Asus' 610Hz gaming monitor, you'll actually miss 164 frames of gameplay. That's the kind of ridiculous frame rates you can expect when using the world's fastest gaming monitor — and we gave it a try.
No, the Asus ROG Strix Ace XG248QSG isn't the brightest, nor are its colors too accurate. But for esports players? It's that 610Hz refresh rate that matters. Check out how it runs!
A laptop or a work of art? How about both! We just set our eyes on MSI's Prestige 13 AI+ Ukiyo-e Edition featuring Katsushika Hokusai’s “Great Wave off Kanagawa” artwork, and you'll also find traditional handcrafted lacquerware using pure gold leaf powder on this special edition model.
There's only 1,000 of these being made, and we can imagine its price will be fairly costly, but there's not denying this is easily one of the most beautiful laptops around.
Why not turn your gaming PC into a robot that's brought to life thanks to an RTX 5090? That's what Asus thought, and the ROG Omni Case Mod stared us down as we walked by it on the show floor!
It notably sports an Intel Core Ultra 5-245K CPU and an RTX 5090 GPU (interesting mix there), and aside from its face animations, don't expect this robot to walk around or give out high-fives. With an RTX 5090, though? This robot can game.
We finally got some hands-on time with the MSI Claw A8, boasting an AMD Ryzen Z2 Extreme, but it's more than just the improved chip that impresses. Thanks to some tweaks to its ergonomics, it's far more comfortable to hold than previous models (even better than the Steam Deck) and buttons have that pronounced, tactile feel.
We'd love to see SteamOS on this gaming handheld, and we're a little worried how much it will end up costing, but gamers should keep this on their watchlist. Get a deeper dive in our MSI Claw A8 BZ2EM hands-on review.
While the Qualcomm keynote yesterday didn't give us a look at what may be the next-gen Snapdragon X CPUs, CEO Cristiano Amon did give us a tease that something big is in store in the Snapdragon Summit 2025 in September. Now, it's looking like possible Snapdragon X2-powered PCs will be landing in early 2026!
Qualcomm also noted that it's still putting gaming into the mix, so a Snapdragon X2 chip is sure to see performance shine even more (along with extended battery life much, like its lineup of Copilot+ PCs).
There are already plenty of big announcements (and more on the way) from Computex 2025 but my favorite thing about being on the ground here in Taipei is having a chance to check out everything on the show floor. I’ve seen quite a few new gadgets and devices but I’ve also had a chance to see some I missed when they first came out. Case in point, this tiny Cybertruck that houses a full mini PC.
I remember seeing plenty of videos about this unique mini PC last year but I finally got to see it for myself. While the hype around the real Cybertruck has certainly died down, this mini PC with moving wheels, doors and even working lights still looks just as cool. With an AMD Ryzen 7 processor under the hood, it’s powerful enough for most tasks and even some light gaming. My favorite part though is that all of its ports are housed at the back where the Cybertruck’s bumper is. While it might not be one of the best mini PCs, it’s certainly one of the coolest looking ones I’ve seen yet and it almost tops the spaceship-inspired mini PC that caught my eye at last year’s show.
Nvidia has finally launched the most affordable graphics card in its new RTX 50 series in the form of the RTX 5060. While we spent some time putting the desktop version of this card through its paces over the weekend, at Computex 2025, we got a chance to test the laptop version. If you were worried about the mobile version of the RTX 5060 being underpowered, fortunately, DLSS 4 is able to do some very serious lifting when it comes to frame rates and performance. In Cyberpunk 2077 for instance, the RTX 5060 laptop we tested hit just 33 fps at a resolution of 1920 x 1200 at ultra settings with ray tracing enabled. Once we turned on DLSS though, we saw a massive improvement immediately and the game jumped up to 193 fps.
We’ll have more testing data once we get some RTX 5060 laptops in for review but with DLSS 4 on board, you may soon be able to get a gaming laptop with some stellar performance for even less.
The best mechanical keyboards come in all sorts of different sizes and configurations but there’s one type you don’t see all that often: split keyboards.
Unlike a traditional keyboard, you can pull apart a split one or even use each side on their own. Most of the split keyboards I used in the past were from smaller or more niche brands which is why it was quite the surprise when we got to see the first one from Asus during our visit to the company’s headquarters yesterday.
Not only is the new ROG Falcata a split mechanical keyboard, but it also features magnetic switches. Since you can fine tune their accusation point down to 0.01mm intervals, magnetic switches have become quite popular in gaming keyboards. Unfortunately though, many magnetic keyboards need to be hardwired to your PC.
Well, that’s not the case with the ROG Falcata which can be used both wired and wirelessly. You can also customize it to get the perfect fit for typing or gaming since Asus includes a number of detachable feet which you can use to lift the whole keyboard up or even to have the two halves slope at a downward angle.
A good split mechanical keyboard is hard enough to come by as it is, which is why the news that Asus is making one — and one with wireless mechanical switches to boot — is such a big deal. Stay tuned as this is definitely one we’re going to get in for review so that we can try it out for ourselves.
Companies are seemingly a little bit hesitant to reveal the specifics of pricing at Computex this year due to the lingering uncertainty over U.S. tariff policies.
Language like ""intending"" to launch a product at a given price is often a factor, as is leaving the dollar amounts out of written press releases that could then be labelled as misleading later on.
There's good reason for brands to be cautious. The excellent Asus Zenbook A14 was unveiled with a promised $899 price tag. As a result of the various macroeconomic factors, that laptop now costs $1,099 — which makes it much harder to recommend as one of the best laptops you can buy.
The team on the ground in Taipei are wrapping up their first day on the show floor now. Don't worry — we've got a round up of the best things we saw from the opening of Computex 20205 on the way!
But in the meantime, scouting some of the weird and wonderful PC builds is always one of the highlights of attending computing events like this.
Check out the one above — how would you feel about putting that on your desk?
As the sun goes down over in Taiwan, we've rounded up the top 5 gadgets from Day 1 at Computex 2025.
There have been a couple of no-shows (we predicted new CPUs from Nvidia that never came to pass) and a distressing lack of new gaming handhelds so far.
But some real bright spots have shone through, like MSI finally ditching Intel for AMD in the new Claw 8 and Acer giving Oura and Samsung the finger — with its new FreeSense smart ring, of course.
It's been a busy 24 hours at Computex already and we're still digesting everything we've seen at the show today. So stay tuned as, frankly, there's loads more to come.
Here's a look at the ROG XG Station 3 dock which made its debut at Computex this year.
Users can connect any desktop GPU to any Thunderbolt 5-enabled system with this external dock and transfer speeds, thanks to Thunderbolt 5, can hit speeds of up to 80GBps.
Asus says it's fully compatible with AMD Radeon RX 9000 series and Nvidia RTX 50 series GPUs.
We got to go hands on with the new MSI Titan 18 HX, a gaming laptop with ludicrous levels of performance that wowed us...at first.
However, despite all the bells and whistles we should note that this behemoth of a PC comes with an equally huge $5,000 price tag which is just for the base model. The high-end configuration we tested tops out close to seven grand!
Acer announced at Computex 2025 that is producing two new Android-based Iconia tablets; the Iconia V11 and V12.
The 11-inch and 12-inch devices look to be geared for budget-conscious buyers who still want a pretty good tablet.
We've got full details of the new tablets here.
While we're tracking all of the cool stuff being announced at Computex 2025 we can't forget that Microsoft and Google are also throwing huge events this week.
Tom's Guide is keeping tracking of all it.
Right now Google I/O is going on with Google sharing all of it's latest AI and Android developments and what's upcoming from the Mountain View giant.
Meanwhile, Microsoft's Build 2025 developer event is also happening this week. Like Google, it's all about AI.
Our tails are on fire but we're covering all three major tech events so check it out if you want the latest information.
We got some hands-on time with the new MSI VenturePro A14 gaming laptop which features an RTX 5060 GPU.
It's pretty solid and could be among this year's best laptops.
MSI has a new 500Hz QD-OLED gaming monitor that doesn't sacrifice picture quality and brightness for speed.
We went hands-on with the new monitor and its various systems that meant to prevent burn-in despite the speed.
We got to see the Asus TUF A14 at Computex 2025 a refresh of Asus' budget gaming laptop.
The big upgrade is the RTX 5060 graphics it rocks, while still fighting into a friendly budget.
Computex roars on with more gadgets, tech and shiny new device to marvel over.
Look forward to that as we continue our coverage of one of the biggest computing expos in the world.
Later today we'll highlight some of the most interesting things we saw on Day 2 and share more information on brands and devices that are being announced this week.
AMD's keynote is happening today at 11am Taiwan time / 11pm ET, and we're expecting significant announcements in both gaming and AI.
What will they be? Well, rumors are pointing to an RX 9060 XT – setting its sights firmly on the Nvidia RTX 5060. I'll be live at the keynote, so once we know, you'll know!
The wait is finally over as MSI has finally added an AMD-powered version of to its Claw handheld gaming PC lineup. Check out the video above to see it in action and you can read our full hands-on with the new MSI Claw A8 BZ2EM for even more details. Spoilers, it's as comfortable to hold as it is to look at in this bright new colorway.
We're just minutes away from AMD's Computex 2025 keynote where we're expecting a ton of gaming, AI and workstation news. Of course we're most excited about the company's new Radeon RX 9060 XT graphics card which we think might give the Nvidia RTX 5060 a run for its money. Stay tuned as we're heading into the press conference now and we'll have all the updates live as they're announced.
And we're in. Our own Jason England is in the auditorium right now getting ready for AMD's big Computex 2025 keynote. It starts in just a minute, so get ready for all the big announcements from Team Red.
Jack is starting off AMD's keynote with gaming while highlighting the company's partnership with Sony. Through Project Amethyst, the two were able to bring amazing graphics to the PS5 while paving the way for AMD's FSR 4 frame-gen tech. FSR Redstone is set to come out in the second half of this year and will be a game changer for FS4 4.
PC gaming is about to get even more affordable with the launch of AMD's new RX 9060 XT GPU. Available in an 8GB version for $299 or a 16GB version for $360, this new graphics card will launch on June 5 worldwide.
Jack is now highlighting the power of AMD's Ryzen AI Max+ chips which are 15% faster than Apple's M4 Pro ones. The company says it isn't just building chips to break records but for the future where as we've seen, AI will play an increasingly larger role in our everyday computing.
We got to see AMD's RX 9060 XT graphics card in action and it's everything we we're hoping for and more, especially at just $350 for the 16GB version.
Lenovo's Luca Rossi has joined AMD on stage to talk about how the company's chips make a massive difference when it comes to local AI in its Yoga and ThinkPad laptops along with with its handheld gaming PCs.
If you're watching AMD's Computex 2025 keynote at home, then you too are likely experiencing glitches and the dreaded spinning loading wheel. Before the stream glitched out, Lenovo was talking about how one of its customers used a ThinkStation P8 with a Threadripper processor to map an entire island. Let's hope they get this issue fixed soon. Fortunately though, Jason is still in the auditorium seeing everything as it happens live, so we'll be leaning on him for our next few updates.
The livestream may be down but Asus has joined AMD on stage to discuss bringing AI to everyone. Asus has announced 4 new Asus ExpertCenter enterprise PCs along with 2 laptops, a new all-in-one and my favorite, a new mini PC.
If you're trying to watch the livestream at home, your probably seeing the dreaded spinner. Unfortunately, the issue hasn't been fixed. I've seen it jump ahead a few times but fortunately, we're on the ground covering the news as it happens so stay tuned.Actually, AMD just cut the livestream entirely but hopefully they relaunch it soon.
While we wait for AMD's Computex 2025 livestream to come back online, here are the specs of the company's new RX 9060 XT graphics card in both its 8GB and 16GB variants. It's a powerful card for sure and if you can get one at MSRP, you're going to get a long bang for your buck.
AMD has announced its Threadripper Pro 9000 series which packs 96 cores, 192 threads, 384MB of L3 cache, 128 PCIe Gen 5 lanes and can run up to 5.4GHz with max boost. This is one fast chip.
We're now getting some details on how Hollywood will be using the new AMD Threadripper Pro 9000 as the CEO of weta FX has joined AMD on stage. Apparently, the next Avengers movie (along with several new Avengers projects) and even the next 3-4 Avatar movies will be made on Threadripper Pro 9000 chips. This is huge news and weta FX's CEO said that AMD's new Threadripper chip is ""60% faster than any other chip we tried.""
Computex is one of the most exciting tech events of the year but would you go to the show yourself or have lunch with James Cameron? Well, the CEO of weta FX was presented with that exact choice and he turned down the director of Terminator, Titanic and Avatar for AMD.
To update you on what's happening in the room here at the AMD keynote (since the stream has gone down), AMD has just announced a monster of a GPU — the Radeon AI Pro 9700.
This is built for AI developers to build their models and run them, so not necessarily for the regular folk. But the beastly power paired with Threadripper makes this physical build he brought out the most powerful PC on the planet!
I wonder if the camera guy knows that the stream is down... Anyway, cool rig mate!
And that's the AMD keynote over! Here's what got announced.
Just like clockwork, once the keynote ended, the RX 9060 XTs were wheeled out into company booths. We managed to find a 16GB TUF Gaming version from Asus — overclockable and sporting triple fan cooling.
If you're looking at this thinking ""that's massive,"" don't worry! There are smaller options.
The Zotac Zone pleasantly surprised us when we first tried it last year. But with a fresh update for 2025, things are really going up to the next level.
As Anthony found out in his hands-on time with the Zone, you're still getting that same gorgeous AMOLED panel and similar specs. But two big things have changed:
The Zotac Zone turned into a Steam Deck OLED on speed!
Another day, another incredibly creative (and kind of terrifying) PC build. Terrifying primarily because, well...yea that's a full water fountain display so close to your components!
This packs an Intel Core Ultra 5 225, Intel Arc A770, 64GB DDR5 RAM and a 1TB SSD — not too shabby for all its classiness!
Technically, RTX 5070 laptops have been around for a while, but ever wonder why nobody has really tested it yet? Well that's because manufacturing of systems with the GPU took quite a while longer than anticipated.
So long, in fact, that what you're looking at here is an MSI prototype laptop running the GPU. Once I set my sights on it, you know I had to benchmark it!
Cyberpunk 2077 (Path Tracing ultra settings)
This falls largely in line with what I expected — sandwiched nicely between 5080 and 5060. Fun fact: Cyberpunk 2077's benchmarking doesn't recognise the multi-frame gen on it's own frame counter. So while it looks buttery smooth at nearly 100 FPS, it looks like it's struggling when it actually isn't.
The newly-announced AMD 9060 XT GPU is the company's mid-range GPU that looks set to go right up against the Nvidia 5060 Ti with almost identical specs.
However, while AMD's card will ship with 8GB or 16GB of GDDR6 RAM, the Nvidia option uses the faster GDDR7 variant which delivers roughly 40% more bandwidth.
We've already seen some of these 9060 XT cards out in the wild, and it'll be interesting to see how it performs when we get it in for some real world testing. AMD's GPU is packing 32 compute units, 64 dedicated AI accelerators and 32 ray-tracing cores. Prices start at $299 for the 8GB variant.
After giving the desktop RTX 5060 some hands-on time, our own Jason England has gone and done the same with its laptop counterpart. He locked down both an Asus TUF Gaming A14 and an Acer Predator Helios Neo 14 AI with the new card and got some quality time with Cyberpunk 2077 and the soon-to-be-released FBC Firebreak.
You can read his full thoughts here, but suffice to say, the AI-driven frame generation is where things are heading right now (at least for those of us who don't want to drop thousands on a fully-specced gaming rig) and Nvidia's card seemingly nails it.
Usual caveat applies: this limited testing was done under Nvidia's optimised scenarios so things could be different when we hit it with something more taxing.
Samsung is showing off some prototypes of a new laptop display it's calling ""UT One"" where the UT stands for Ultra Thin. It's a new OLED structure that replaces the top glass substrate with ultra-thin organic and inorganic thin films, while retaining the bottom glass layer.
As a result, the panel is 30% thinner and 30% lighter than conventional dual-glass OLED panels, with a weight reduction equivalent to a single standard laptop battery cell (approximately 50g), allowing for either increased battery capacity or improved portability.
Since the market is rapidly moving from LCD to OLED, expect to see this make a splash in the years to come.
Gaming monitors are pushing the envelope at Computex this year with three new models from MSI, Asus and Acer all going hard on refresh rates.
We've already spoken about the Asus ROG Strix Ace XG248QSG which, at 610Hz, is the fastest of the bunch. But the Acer Predator X27U F5 and the MSI MPG 271QR X50 aren't slouches either. The former is only launching in China and Europe (starting at €899) to begin with but will come to the U.S. eventually.
Meanwhile, the latter throws in an interesting AI feature that uses a small sensor at the bottom of the screen to detect when you move away from the screen. Once you step away, the NPU will dim the screen to prevent burn-in. Oh, it also has a maximum refresh rate of 500Hz.
This right here is the iGame G Helmet PC case and it's crafted with 0.8mm aluminium panels that have been cut using precision lasers.
If you've got one of the best desks in your home office and feel the need for a PC designed like a ""spaceship command module"" then this is the case for you.
According to the blurb on the show floor, it's got ""silver-black nano-brushed surfaces that reflect quantum-inspired light effects for a futuristic aesthetic.""
We've wrapped up our best of day 2 roundup at Computex 2025 and, perhaps unsurprisingly, the most important news from our second 24 hours at the show was AMD's official reveal of the Radeon 9060 XT GPU.
It's set to go toe-to-toe with Nvidia's 5060 and we've already seen some evidence this is a very capable midmarket card. It comes in 8GB and 16GB variants with prices set at $299 and $349 respectively.
Elsewhere we got a look at the upgraded Zotac Zone gaming handheld and Samsung was showing off some of it's ultra-thin new display panels. They're just prototypes at this stage, but Samsung says they're 30% thinner and 30% lighter than conventional dual-glass OLED panels.
Hey, all! Tony Polanco here to update this blog for the next few hours. Though I'm not at Computex, I've been keeping up with the latest news, which I'll now present to all of you.
One of the most interesting things was the MSI Claw A8. Why? It will be the first gaming handheld powered by the new AMD Ryzen Z2 Extreme chip. The other big deal here is that this machine won't be using an Intel Core Ultra CPU like previous Claw models. Because AMD chips have a better track record with handhelds than Intel (see the Steam Deck and Asus ROG Ally for examples), this could be the best Claw handheld yet.
Computex isn't just about CPUs and GPUs! Our own racing sim afficionado Jason England got to test out a new racing sim wheel at the big show, and it looks like it could be an awesome peripheral to play games like Gran Turismo on. We can't tell you more about this yet, but stay tuned to find out more!
When your desktop tower is also a monitor! We got to check out MSI's MEG Vision X AI PC powered by RTX 5090, featuring a 13.6-inch touch panel display with access to almost every mainstream AI tool under the sun. And all it's going to cost you is $7,500!
Acer's been pulling out all the stops at Computex 2025, and not just with its sweep of all-new laptops. It revealed an all-new pair of earbuds, but these aren't the kind you listen to music on. Instead, they break down language barriers — and we got to have a full-blown conversation in a different language.
The Acer AI TransBuds offer real-time translation through earbuds, using AI-based speech recognition and semantic analysis, so you can have a two-way conversation in two completely different languages and know what the other is saying. And there's no overly long, awkward delay, either.
The speediness of the AI being used in Acer's earbuds is impressive, and we can see why the company is aiming for buyers to use these for business meetings, livestreams or online study sessions.
The AI TransBuds present many use cases for multilingual conversations, and from what we could tell, real-time translation with these on is fairly snappy and accurate. But they also offer live captioning and transcription that can be seen on the connected smartphone or tablet, meaning you can record conversations and check them out later on.
While we didn't get any word on pricing or availability, the Acer AI TransBuds are sure to be useful earbuds for education, work or simply traveling.
Depending on how much these will cost, the Acer AI TransBuds could be a handy set of earbuds to have in your pocket while going abroad (but for listening to music, probably stick with one of the best wireless earbuds instead).
In the worst-kept secret of Computex 2025, AMD has just announced its new GPU — the Radeon RX 9060 XT. The self-proclaimed “world’s best graphics card under $350” will be available on June 5th with a starting price of $299.
That lower price is for the model with 8GB of video memory (the storage space for all those shiny in-game graphics). For $349, you can double that to 16GB — a much better number given how demanding AAA games like Indiana Jones and the Great Circle are.
And if AMD’s numbers are reflective of real-world testing, then this is looking like the ultimate combo breaker of performance and value for money that's ready to take on the Nvidia RTX 5060 and RTX 5060 Ti.
Good morning from Taipei! And we start with some new mind blowing display concepts from Samsung Display. Constantly on the forefront of seeing what OLED can do, there are all kinds of foldables, rollables and even the highest-resolution ultrawide monitor on the planet.
These prototypes are a good indication of the kinds of tech we'll be using in the future, and we got to see what's what.
You know those portable projectors you keep seeing on sale online? Well, this mini PC I spotted on the show floor at Computex 2025 takes direct inspiration from them. Like a projector, the whole case can swivel and flip around and without anything plugged in, you might really think this was a projector.
Besides its looks, my favorite thing about this mini PC is how its motherboard and basically the whole computer can slide out at the back. This makes it easy to access its internals for when you want to upgrade its memory or storage. Computex is all about the weird and wacky and this mini PC definitely checks that box. However, I can't imagine how cool it would look on a side table in my living room.
One technology that keeps showing up in the background of all things we've seen at Computex this year is Thunderbolt 5. From faster external graphics cards to even better read and write speeds for storage, Thunderbolt 5 looks to be a game changer for a lot of different device types. And with a baseline of 80 gigabytes per second and a max speed of 120, it's easy to see why.
Who needs a modern PC case when retro beige ones are finally back in fashion? At Computex, SilverStone is showing off its 80's inspired FLP02 PC case. While it may look old on the outside, it has all the modern conveniences like USB-A and USB-C ports on the front and enough space inside to accommodate a RTX 5080 or even an RTX 5090 graphics cards.
Computex is all about seeing sick custom PC builds and as we've been working way back and forth between the two buildings that are the heart of this show (TaiNEX 1, TaiNEX 2), we've been on the hunt for some cool ones. This racing car themed custom PC from Tcomas certainly fits the bill.
While you can see its graphics card and some other components on the side, you can also open the hood and take an even closer look inside. We'll be on the search for even more cool PC builds for the rest of the week, so keep checking back.
Speaking of creative custom PC builds, is your PC built like a tank? Well, it probably isn't quite as literal as this custom tank mod case from Thermalright. And yes, those wheels are moveable.
All of its ports and components are situated on the back, including its power switch, but the real fun is how it was made. Taking over 3 months to manufacture, it's built with 500 individual CNC parts and comes with that gamer-ready RGB glow on the railguns. The wheels are also moveable, although we didn't get to see that in action (we wouldn't want a PC rolling away from us, after all).
That's just one of the plentiful custom PC builds we've seen on the showfloor. Although, compared to race car and water fountains, it's hard to beat a tank!
We've got Anthony on the ground checking out all of Samsung Display's latest tech, and that includes a glimpse at the UT One OLED panel that's 30% lighter and thinner, crazy 500Hz QD-OLED prototypes and plenty of foldable and rollable AI-powered smart displays.
If this is what the future of displays looks like, we're in for a treat. Find out everything we saw!
""Yes, I beat the Monza lap record, because of course I did,"" says our resident sim racing champ Jason.
This is Nitro Concepts' E-Racer setup — a new level of immersion with haptic actuators in the seat and pedals to help you feel the whole car around you. Really helps you identify moments of understeer around chicanes.
And there are wind boxes mounted under the monitor that blow air at you at varying degrees of intensity depending on how fast you're going.
It's €500 — a steep price for the GOAT experience of sim racing. But Nitro Concepts is a modular company, so it can start small (frame is €399) and build out from there. It could be a great choice if you're trying to build the best sim racing rig.
Did you know that there are 2 different Asus ROG Zephyrus G14s? One is actually 2mm thicker than the other.
That's because of the GPU used in it. The first picture is what's called ""Board 1."" This is reserved for the higher graphics cards — your RTX 5090, 5080 and 5070 Ti.
To fit this card and all the video memory chips around the outside, other stuff had to be moved around to make room for it. That, paired with needing bigger fans to cool it, means the laptop is the tiniest bit thicker.
Meanwhile, ""Board 2"" is reserved for RTX 5070, RTX 5060 and the other one Nvidia is obviously cooking up that everyone is too scared to mention the name of (cough RTX 5050 cough). This is smaller and doesn't run as hot, so thinness is the name of the game here.
One of the most interesting gaming rigs Asus revealed is its Asus TUF Gaming T500, which sports an RTX 5060 Ti GPU and an Intel Core i7-13620H CPU. The latter is a mobile processor made for laptops, so it's strange to find this in a gaming desktop. As for how Asus pulled it off? That's what the motherboard looks like to deal with a laptop CPU and a desktop GPU.
There is good reason behind bringing these two components together, though. Not only does it keep the cost down ($1,099), but it also gives Asus the chance to use other high-quality components, unlike other budget gaming PCs, including better fans and the like.
We'll wait and see how this gaming PC turns out, and whether that laptop CPU and desktop GPU combo works out.
Here's a better look at the Zotac Zone with Paul! We've already called it a true Steam Deck rival, and we can see why with a switch from Windows 11 to Linux. Will it be the go-to gaming handheld thanks to the update? We'll have to get some more hands-on time with it — and we'd be very happy to do so.
We've got a better look at Asus' bonkers robot PC casemod that actually fitted with two RTX 5090 GPUs. The ROG Omni Casemod is surprisingly also powered by an Intel Core Ultra 5 245K CPU, but that's enough for it to give you looks while gaming away. We've already seen a fountain, race car and tank gaming PC, but a full on robot? Now that's a
We got some exclusive hands-on time with this all-new racing rig from Asetek, and even though it's still months away from release that we can't reveal its name, we can say that its already looking to be the best sim racing rig of 2025 (so far).
It feels incredible to sit and drive in, but the real kick here is its price. With everything included (such as its wheel, wheel base, pedals and the seat to connect it all to), it'll be $1,000. Pricey? Yes, but from what it offers, you'd find other rigs cost twice as much — if not more.
Check out our thoughts on this Asetek sim racing rig, and we'll be on the lookout for even more racing setups from our resident sim racing guy, Jason, while we explore the floors of Computex.
It's been a wild few days of checking out the latest and greatest pieces of tech to come out of the world's biggest computing show, and we're now ready to dish out some well-earned awards — our picks for the best of Computex 2025 are here!
Here's an overview of all the devices you'll want to put on your radar:
For an in-depth look at all our best of Computex 2025 picks, we've got you covered.
The MSI Claw A8 is making waves, but not just because of its AMD Ryzen Z2 Extreme chip. It comes in two colors: a sleek white and lime green. We're big fans of the latter, and MSI has a few more concept colors it's having fun with, including purple and blue.
Here we thought that this MSI 500Hz QD-OLED monitor and 610Hz gaming monitor were as fast as can be. We've now spotted the world's first 750Hz monitor from manufacturer HKC, which, as the company boldly claims, really does push ""esports performance to new heights.""
The HKC ANT257PF 24.1-inch gaming monitor has a panel made from the company itself, and sports a 1ms response rate, 95% DCI-P3 color gamut coverage and even HDR 400 certification. That 750Hz refresh rate is insane, though, and while many gamers won't need to take advantage of these blistering frame speeds, it shows just how far frame rate tech in displays is going. And I'm very sure 750Hz is plenty fast for esport gamers.
Since they’re designed to move up and down with the touch of a button, standing desks rarely provide you with enough storage space. And the few that do use drawers or a pegboard back that needs to be clamped onto your desk. With its Revon Elite standing desk, Taiwan’s own Dezctop has solved this problem in a very elegant way.
The Revon Elite features a track that runs along the sides and back of the desk that can be used to almost instantly add a cable management tray, as well as metal side panels. With a whole host of magnetic accessories, you can add storage for your pens, flash drives and other work from home essentials. The best thing, you can rearrange this whenever and as often as you want to quickly transform your workspace.
The desk features a U-shaped frame for increased stability, a magnetic keypad you can position wherever you want and a chamfered edge for a more comfortable typing experience. The Revon Elite offers an easy way to customize your workspace without sacrificing storage.
In a sea of lookalike laptops, the MSI Prestige 13 AI+ Ukiyo-e Edition makes real waves — quite literally. Most laptops play it safe or scream for attention. This one strikes a rare balance: subtle, calming, and genuinely artistic.
As part of MSI’s new Artisan Collection, it’s crafted in collaboration with Japanese lacquerware company OKADAYA, featuring the iconic Great Wave off Kanagawa by Hokusai. But this isn’t just a print — it’s handcrafted.
Each lid goes through an eight-layer lacquer process, finished with pure gold leaf powder and individually numbered. Only 300 are made each month. This isn’t a “sticker on a lid” kind of deal. The gold-dusted wave, piano gloss accents, and gold-lettered keys all show just how far MSI went. It’s easily the most distinctive laptop I saw at Computex this year.
And yes — it’s powerful too. At just 2.2 pounds and 0.7 inches thick, it still packs a 13.3-inch 2.8K OLED (16:10), Intel Core Ultra 9 chip, and a stellar port selection including Thunderbolt 4, USB-A, HDMI 2.1, and a microSD slot.
Pricing isn’t final, but rumors say around $1,599. For something this light, capable, and genuinely beautiful? That might just be a steal.
14-inch gaming laptops are the hottest thing going at Computex for the past few years, and Acer has taken the crown with its elegant-yet-powerful Predator Triton 14 AI. In our hands-on review, we said it's “one of the most beautiful laptops” we’ve ever seen, gaming or otherwise.
The anti-fingerprint coating keeps it looking clean and streak-free, while giving it a nice premium feel in the hand. The port array is generous, and that 14.5-inch 2880 x 1800 OLED display is a real stunner that is sure to make your games look incredible with all that vividity and HDR depth.
But don’t let those slender looks fool you — this is a real performer too with Intel Core Ultra 9, Nvidia’s RTX 5070, 32GB of RAM and a 2TB SSD. Oh, and that thinness doesn’t get in the way of keeping it cool, as the re-engineered fans boost airflow while making it quieter too!
Morning all! The temperatures are high and the sun is shining over the final day of Computex, and as the Managing Editor of computing, I wanted to take the time to simply say ""thank you.""
Computex 2025 has been a wild ride. While some of the big announcements that were rumored didn't necessarily materialize (looking at you, Asus ROG Ally 2, Snapdragon X2 and Nvidia's consumer CPU), this has been a dense show of new releases and gadgets to test.
And my team and I wouldn't be able to do what we love without you. We have much more coming over the next few days, but as the biggest computing show on the planet winds down, it's time for me to sign off.
Besides, I need to get to my tradition — visiting the Guang Hua Digital Plaza!

Tom's Guide is part of Future US Inc, an international media group and leading digital publisher. Visit our corporate site.

©
Future US, Inc. Full 7th Floor, 130 West 42nd Street,
New York,
NY 10036."
"general","article","The Sequel to Nvidia's Most Popular GPU Hits Shelves Today—With No Reviews","https://www.wired.com/story/nvidia-rtx-5060","All products featured on WIRED are independently selected by our editors. However, we may receive compensation from retailers and/or from purchases of products through these links.
Nvidia’s RTX 4060 is the most popular graphics card as of April 2025, according to Steam  ’s Hardware Survey. Now there’s finally a successor, the RTX 5060, rounding out the 50-series graphics cards that the company has been releasing since their debut at CES 2025. The 5060 was announced at Computex, a trade show in Taiwan, alongside the laptop version, which are immediately available to purchase today.
All of that should be reason to be excited, especially since we've been waiting years for midrange gaming laptops to receive a meaningful bump in GPU performance. The only problem? Nvidia apparently wants you to buy this new GPU and laptops without letting reviewers test them first.
It's been around a month since the RTX 5060 Ti launched, and now Nvidia has followed it up with the RTX 5060, rounding out the lineup on the low-end graphics cards.
There's a lot we still don't know about the RTX 5060, including specs like clock speed or the actual CUDA core counts. All of that will have to wait, along with real testing, even though you can buy this cards starting today.
The RTX 5060 keeps the same pricing as last generation's RTX 4060. There was a price decrease for the RTX 5060 Ti, but Nvidia has kept things steady here, as well as the 8 GB of VRAM. Video random-access memory stores graphics data for the graphics card and boosts performance, and is increasingly an important spec for playing modern AAA games.
Maintaining supply has been a consistent problem for the rest of the RTX 50-series, inflating prices way beyond where they should be. For the RTX 5060, Nvidia says it’s expecting a “good supply” of these cards, but we'll have to wait and see how that plays out. In general, these lower-tier cards aren't affected as much by the scalper market, so they should be easier to get hold of.
Like with the RTX 5090, 5080, 5070 Ti, 5070, and 5060 Ti, the primary selling points of the RTX 5060 is the DLSS 4 and a feature called Multi-Frame Generation. Standard frame generation lets AI create an artificial frame between other frames, allowing for much higher frame rates, which was introduced in DLSS 3.5. With DLSS 4, though, that's expanded to 2X, 3X, or 4X frame generation, pushing frame rates even higher. As you'll notice in the table above, the RTX 5060 includes a lot more AI power in the form of Tensor Cores to power these AI capabilities.
Multi-Frame Generation, however, is not a feature I've been particularly impressed with, though, as the implementation has been sloppy. Nvidia has been overriding the feature into games through the Nvidia app, and relatively very few games support the feature natively. While the higher frame rates are impressive on paper, the larger problem is that the more artificial frames you add, the more input lag becomes an issue.
None of that is exclusive to the RTX 5060. As for this GPU in particular, Nvidia says you’ll see a 20 percent performance improvement over the RTX 4060 without the aid of AI trickery. If that turns out to be true, that’ll be a decent uplift over the popular GPU from 2023, especially since there isn't much competition on the market outside of Intel's Arc B580.
Along with the new desktop card, several other companies have launched new gaming laptops with the RTX 5060. Most notably, Razer announced the refreshed Blade 14, the company’s hybrid laptop aimed at MacBook owners and content creators, plus gamers.
The new model is thinner than before, at 0.62 inch thick and 3.59 pounds, which is 11 percent more portable than its predecessor. It has an upgraded 3K OLED display with a 120-Hz refresh rate, as well as a new six-speaker audio system. The RTX 5060 configurations start at $2,299 and will be available this month, also packing the AMD Ryzen AI 9 365.
Nvidia says laptops with the RTX 5060 will start at $1,099, though I haven't seen any laptops yet that hit that price just yet. But again, no reviews were provided for any of these new GPUs or laptops, meaning for now we're just going to have to take Nvidia's word for it.
In your inbox: Upgrade your life with WIRED-tested gear
“Wi-Fi keeps going down”: Trump’s RTO mandate is going terribly
Big Story: The worm that no computer scientist can crack
Yuval Noah Harari: “Prepare to share the planet with AI superintelligence”
Uncanny Valley: An insider look at the influence of Silicon Valley
$50 Off In-Person Tax Prep When You Switch From Your Tax Current Provider
Exclusive: Up To 50% Off 6 Boxes With Factor Promo Code
© 2025 Condé Nast. All rights reserved. WIRED may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Condé Nast. Ad Choices"
"general","article","I Don't Understand the RTX 50-Series Hype","https://linustechtips.com/topic/1595927-i-dont-understand-the-rtx-50-series-hype/",""
"general","article","AMD Unveils Next-Generation AMD RDNA™ 4 Architecture with the Radeon RX 9000 Series","https://www.amd.com/en/newsroom/press-releases/2025-2-28-amd-unveils-next-generation-amd-rdna-4-architectu.html","– The new AMD Radeon™ RX 9000 Series graphics cards deliver enthusiast-level gaming experiences supercharged by AI –
SANTA CLARA, Calif., Feb. 28, 2025 (GLOBE NEWSWIRE) -- AMD (NASDAQ: AMD) today unveiled the highly-anticipated AMD RDNA™ 4 graphics architecture with the launch of the AMD Radeon™ RX 9070 XT and RX 9070 graphics cards as a part of the Radeon™ RX 9000 Series. The new graphics cards feature 16GB of memory and extensive improvements designed for high-quality gaming graphics, including re-vamped raytracing accelerators and powerful AI accelerators for ultra-fast, cutting-edge performance, and breakthrough gaming experiences.
In a YouTube livestream, David McAfee, CVP and GM, Ryzen CPU and Radeon Graphics AMD, was joined by Andrej Zdravkovic, SVP of GPU Technologies and Engineering and Chief Software Officer, AMD, as well as Andy Pomianowski, CVP of Silicon Design Engineering, AMD, to discuss the outstanding performance and value proposition of the Radeon RX 9000 Series. In a related event in Zhuhai, China, Jack Huynh, SVP of the Client and Graphics Group, AMD, led a regional event for the new products. Huynh was joined by David Wang, SVP of GPU Technology and Engineering, AMD, and Lanzhi Wang, Senior Director of Product Management,  AMD. The celebration was also marked by a customer celebration with Darren Grasby, EVP and Chief Sales Officer, AMD; Spencer Pan, President of AMD China, and partners including Asrock, ASUS, Gigabyte, Sapphire, Tul, Vastarmor, Veston, and XFX.
""Today, we're thrilled to unveil the AMD Radeon™ RX 9000 Series, a significant leap forward in graphics performance powered by our next-generation AMD RDNA™ 4 architecture,"" said McAfee. ""These GPUs are designed to meet the demands of today's games, delivering enthusiast-class gaming experiences to gamers everywhere, while ready to support tomorrow's innovations. Through the power of advanced AI and Raytracing accelerators, we're not just improving frame rates – we're fundamentally enhancing the gaming experience. Offering incredible performance, AI-powered features, and next-gen display support at competitive price points, the Radeon RX 9000 Series delivers exceptional value for gamers looking to upgrade their systems.”
The RX 9000 Series, powered by the new AMD RDNA™ 4 architecture, offers gamers and creators a powerful blend of performance, visuals, and value. These advanced graphics cards redefine incredibly fast, high-resolution gaming with third-generation raytracing technology enabling realistic lighting, shadows, and reflections to deliver immersive gaming experiences while integrating a suite of AMD features to maximize hardware utilization. Beyond gaming, the RX 9000 Series GPUs leverage new second-generation AI accelerators with up to 8x INT8 throughput per AI accelerator (for sparse matrices) to enhance creative applications and effectively run generative AI applications (vs. RDNA 3).1 The RX 9000 Series GPUs also implement the newly redesigned AMD Radiance Display™ Engine & Enhanced Media Engine for broad display support and elevated quality in both recording and streaming.
Gaming For Today and TomorrowThe Radeon RX 9000 Series unlocks new levels of performance while delivering a suite of new and enhanced features that improve the gaming experience. The Radeon RX 9070 Series offers 16GB of GDDR6 memory, allowing gamers to render the most exciting games of today and tomorrow at max settings. Compared to the previous generation RX 7900 GRE, the latest AMD Radeon RX 9070 is able to deliver over 20% more performance on average when gaming at 1440,2 with the AMD Radeon RX 9070 XT extending that lead to over 40% on average.3
Both graphics cards make smart upgrades for gamers looking to future-proof their systems with a suite of next-gen features that will keep their experiences feeling fresh for years to come. Key features include:
ML-Powered AMD FidelityFX™ Super Resolution 4 (AMD FSR 4) Upgrade
AMD Radeon RX 9000 Series Product Specifications 
Pricing and Availability AMD Radeon RX 9000 Series graphics cards are expected to be available from leading board partners including Acer, ASRock, ASUS, Gigabyte, PowerColor, Sapphire, Vastarmor, XFX and Yeston beginning March 6th, 2025. The AMD Radeon RX 9070 XT has an SEP of $599 USD, while the AMD Radeon RX 9070 has an SEP of $549 USD.
About AMDFor more than 50 years AMD has driven innovation in high-performance computing, graphics and visualization technologies. Billions of people, leading Fortune 500 businesses and cutting-edge scientific research institutions around the world rely on AMD technology daily to improve how they live, work and play. AMD employees are focused on building leadership high-performance and adaptive products that push the boundaries of what is possible. For more information about how AMD is enabling today and inspiring tomorrow, visit the AMD (NASDAQ: AMD) website, blog, LinkedIn and X pages.
Cautionary Statement This press release contains forward-looking statements concerning Advanced Micro Devices, Inc. (AMD) such as the features, functionality, performance, availability, timing and expected benefits of AMD products including the AMD Radeon™ RX 9000 Series graphics cards, which are made pursuant to the Safe Harbor provisions of the Private Securities Litigation Reform Act of 1995. Forward-looking statements are commonly identified by words such as ""would,"" ""may,"" ""expects,"" ""believes,"" ""plans,"" ""intends,"" ""projects"" and other terms with similar meaning. Investors are cautioned that the forward-looking statements in this press release are based on current beliefs, assumptions and expectations, speak only as of the date of this press release and involve risks and uncertainties that could cause actual results to differ materially from current expectations. Such statements are subject to certain known and unknown risks and uncertainties, many of which are difficult to predict and generally beyond AMD's control, that could cause actual results and other future events to differ materially from those expressed in, or implied or projected by, the forward-looking information and statements. Material factors that could cause actual results to differ materially from current expectations include, without limitation, the following: Intel Corporation’s dominance of the microprocessor market and its aggressive business practices; Nvidia’s dominance in the graphics processing unit market and its aggressive business practices; competitive markets in which AMD’s products are sold; the cyclical nature of the semiconductor industry; market conditions of the industries in which AMD products are sold; AMD’s ability to introduce products on a timely basis with expected features and performance levels; loss of a significant customer; economic and market uncertainty; quarterly and seasonal sales patterns; AMD's ability to adequately protect its technology or other intellectual property; unfavorable currency exchange rate fluctuations; ability of third party manufacturers to manufacture AMD's products on a timely basis in sufficient quantities and using competitive technologies; availability of essential equipment, materials, substrates or manufacturing processes; ability to achieve expected manufacturing yields for AMD’s products; AMD's ability to generate revenue from its semi-custom SoC products; potential security vulnerabilities; potential security incidents including IT outages, data loss, data breaches and cyberattacks; uncertainties involving the ordering and shipment of AMD’s products; AMD’s reliance on third-party intellectual property to design and introduce new products; AMD's reliance on third-party companies for design, manufacture and supply of motherboards, software, memory and other computer platform components; AMD's reliance on Microsoft and other software vendors' support to design and develop software to run on AMD’s products; AMD’s reliance on third-party distributors and add-in-board partners; impact of modification or interruption of AMD’s internal business processes and information systems; compatibility of AMD’s products with some or all industry-standard software and hardware; costs related to defective products; efficiency of AMD's supply chain; AMD's ability to rely on third party supply-chain logistics functions; AMD’s ability to effectively control sales of its products on the gray market; long-term impact of climate change on AMD’s business; impact of government actions and regulations such as export regulations, tariffs and trade protection measures; AMD’s ability to realize its deferred tax assets; potential tax liabilities; current and future claims and litigation; impact of environmental laws, conflict minerals related provisions and other laws or regulations; evolving expectations from governments, investors, customers and other stakeholders regarding corporate responsibility matters; issues related to the responsible use of AI; restrictions imposed by agreements governing AMD’s notes, the guarantees of Xilinx’s notes and the revolving credit agreement; impact of acquisitions, joint ventures and/or strategic investments on AMD’s business and AMD’s ability to integrate acquired businesses; our ability to complete the acquisition of ZT Systems; impact of any impairment of the combined company’s assets; political, legal and economic risks and natural disasters; future impairments of technology license purchases; AMD’s ability to attract and retain qualified personnel; and AMD’s stock price volatility. Investors are urged to review in detail the risks and uncertainties in AMD’s Securities and Exchange Commission filings, including but not limited to AMD’s most recent reports on Forms 10-K and 10-Q.
© 2025 Advanced Micro Devices, Inc. All rights reserved. AMD, the AMD Arrow logo, AMD Software: Adrenalin Edition, AMD RDNA, FidelityFX, Radeon, Ryzen, and combinations thereof are trademarks of Advanced Micro Devices, Inc. Other product names used in this publication are for identification purposes only and may be trademarks of their respective owners. Certain AMD technologies may require third-party enablement or activation. Supported features may vary by operating system. Please confirm with the system manufacturer for specific features. No technology or product can be completely secure. 
1 Based on specifications of AMD RDNA 4 architecture compared to AMD RDNA 3 architecture as of December 2024. RX-11432 Testing done by AMD performance labs February 2025, on a test system configured with Ryzen 7 9800X3D CPU, 32 GB DDR5-6000 Memory, Windows 11 Pro and Radeon RX 9070 (Driver 25.3.1 RC 31) vs. a similarly configured system with an RX 7900 GRE (Driver 25.3.1 RC31) comparing gaming performance at 4K in the following applications: Cyberpunk 2077 (DX12, Ultra), Cyberpunk 2077 (DX12, RT Ultra), Assassin's Creed Mirage (DX12, Ultra High), F1 24 (DX12, Ultra High), F1 24 (DX12, Ultra High RT), Starfield (DX12, Ultra), Far Cry 6 (DX12, Ultra), Far Cry 6 (DX12, Ultra RT), Forza Horizon 5 (DX12, Extreme), Forza Horizon 5 (DX12, RT Extreme), Watch Dogs Legion (DX12, Ultra), Watch Dogs Legion (DX12, RT Ultra), Horizon Forbidden West (DX12, Maxed), Horizon Zero Dawn Remastered (DX12, Maxed), God of War: Ragnarok (DX12, Ultra), Call of Duty: Black Ops 6 (DX12, Extreme), DOOM Eternal (Vulkan, Ultra Nightmare), DOOM Eternal (Vulkan, Ultra Nightmare RT), Total War: Warhammer 3 (DX11, Ultra), Dying Light 2 (DX12, High), Dying Light 2 (DX12, High Raytracing), Alan Wake 2 (DX12, High), Alan Wake 2 (DX12, High w/Med RT), Avatar: Frontiers of Pandora (DX12, Ultra), Hitman 3 (DX12, Ultra), Hitman 3 (DX12, Ultra RT), The Witcher 3 (DX12, Ultra+), The Witcher 3 (DX12, RT Ultra), Metro Exodus Enhanced Edition (DX12, Extreme), Black Myth: Wukong (DX12, Cinematic), Black Myth: Wukong (DX12, Cinematic RT) Baldur's Gate 3 (DX11, Ultra), Ghost of Tsushima (DX12, Very High), Star Wars Outlaws (DX12, Ultra RT), Warhammer 40,000: Space Marine 2 (DX12, Ultra), Control (DX12, High), Control (DX12, High RT), Dragon Age: The Veilguard (DX12, Ultra), Dragon Age: The Veilguard (DX12, Ultra RT), Resident Evil 4 (DX12, Max), Resident Evil 4 (DX12, Max RT), Marvel’s Spider-Man 2 (DX12, Maxed), Marvel’s Spiderman 2 (DX12, Maxed RT), Microsoft Flight Simulator 2024 (DX12, Ultra), The Last of Us: Part 1 (DX12, Ultra), S.T.A.L.K.E.R. 2: Heart of Chornobyl (DX12, Epic), Final Fantasy XVI Demo (DX12, Ultra). Testing conducted with latest game builds as of February 5, 2025 (Marvel’s Spider-Man 2, Microsoft Flight Simulator 2024, The Last of Us: Part 1, and Forza Horizon 5 using latest builds as of February 14th, 2025). System manufacturers may vary configurations, yielding different results. RX-1176.3 Testing done by AMD performance labs February 2025, on a test system configured with Ryzen 7 9800X3D CPU, 32 GB DDR5-6000 Memory, Windows 11 Pro and Radeon RX 9070 XT (Driver 25.3.1 RC 31) vs. a similarly configured system with an RX 7900 GRE (Driver 25.3.1 RC31) comparing gaming performance at 4K in the following applications: Cyberpunk 2077 (DX12, Ultra), Cyberpunk 2077 (DX12, RT Ultra), Assassin's Creed Mirage (DX12, Ultra High), F1 24 (DX12, Ultra High), F1 24 (DX12, Ultra High RT), Starfield (DX12, Ultra), Far Cry 6 (DX12, Ultra), Far Cry 6 (DX12, Ultra RT), Forza Horizon 5 (DX12, Extreme), Forza Horizon 5 (DX12, RT Extreme), Watch Dogs Legion (DX12, Ultra), Watch Dogs Legion (DX12, RT Ultra), Horizon Forbidden West (DX12, Maxed), Horizon Zero Dawn Remastered (DX12, Maxed), God of War: Ragnarok (DX12, Ultra), Call of Duty: Black Ops 6 (DX12, Extreme), DOOM Eternal (Vulkan, Ultra Nightmare), DOOM Eternal (Vulkan, Ultra Nightmare RT), Total War: Warhammer 3 (DX11, Ultra), Dying Light 2 (DX12, High), Dying Light 2 (DX12, High Raytracing), Alan Wake 2 (DX12, High), Alan Wake 2 (DX12, High w/Med RT), Avatar: Frontiers of Pandora (DX12, Ultra), Hitman 3 (DX12, Ultra), Hitman 3 (DX12, Ultra RT), The Witcher 3 (DX12, Ultra+), The Witcher 3 (DX12, RT Ultra), Metro Exodus Enhanced Edition (DX12, Extreme), Black Myth: Wukong (DX12, Cinematic), Black Myth: Wukong (DX12, Cinematic RT) Baldur's Gate 3 (DX11, Ultra), Ghost of Tsushima (DX12, Very High), Star Wars Outlaws (DX12, Ultra RT), Warhammer 40,000: Space Marine 2 (DX12, Ultra), Control (DX12, High), Control (DX12, High RT), Dragon Age: The Veilguard (DX12, Ultra), Dragon Age: The Veilguard (DX12, Ultra RT), Resident Evil 4 (DX12, Max), Resident Evil 4 (DX12, Max RT), Marvel’s Spider-Man 2 (DX12, Maxed), Marvel’s Spiderman 2 (DX12, Maxed RT), Microsoft Flight Simulator 2024 (DX12 Ultra), The Last of Us: Part 1 (DX12, Ultra), S.T.A.L.K.E.R. 2: Heart of Chornobyl (DX12, Epic), Final Fantasy XVI Demo (DX12, Ultra). Testing conducted with latest game builds as of February 5, 2025 (Marvel’s Spider-Man 2, Microsoft Flight Simulator 2024, The Last of Us: Part 1, and Forza Horizon 5 using latest builds as of February 14th, 2025). System manufacturers may vary configurations, yielding different results. RX-1179.4 Testing by AMD, as of February 2025 using Amuse 2.3.15 and Procyon 2.10.1542 64. Models used: SD 1.5, SDXL, ComputerVision FP16, and FLUX Schnell. System configuration: AMD Ryzen 7 9800X3D, 32GB 6000 MT/s DDR5 RAM, 2TB SSD with an AMD Radeon RX 9070 XT GPU vs. a similarly configured system with a Radeon RX 7900 GRE GPU. Driver 25.3.1 RC 31. Performance may vary. RX-1168.5 AMD FreeSync/FreeSync Premium/FreeSync Premium Pro technology requires AMD Radeon graphics and a display certified by AMD. See www.amd.com/freesync for complete details. Confirm capability with your system or display manufacturer before purchase. GD-127.6 Boost Clock Frequency is the maximum frequency achievable on the GPU running a bursty workload. Boost clock achievability, frequency, and sustainability will vary based on several factors, including but not limited to: thermal conditions and variation in applications and workloads. GD-151.
Contact:Stacy MacDiarmid AMD Communications+1 512-658-2265 Stacy.MacDiarmid@amd.com
Matt Ramsay AMD Investor Relations+1 512-496-0197Matthew.Ramsay@amd.com
A photo accompanying this announcement is available at https://www.globenewswire.com/NewsRoom/AttachmentNg/d4e6957f-0945-483c-9795-cf97039270b9
Stacy MacDiarmid
AMD Communications
+1 512-658-2265
Stacy.Macdiarmid@amd.com
Liz Stine
AMD Investor Relations
+1 720-652-3965
liz.stine@amd.com
Find and download the latest AMD corporate and product logos, images, and b-roll footage."
"general","article","AMD RDNA 4 and Radeon RX 9000-Series GPUs Start at $549","https://www.tomshardware.com/pc-components/gpus/amd-rdna4-rx-9000-series-gpus-specifications-pricing-release-date","Everything you need to know about AMD's RDNA 4 GPU architecture and graphics cards.

When you purchase through links on our site, we may earn an affiliate commission. Here’s how it works.

AMD has announced the Radeon RX 9070 at $549 and the RX 9070 XT at $599, both of which ship on March 6. The AMD RDNA 4 architecture and Radeon RX 9000-series GPUs were partially revealed at CES 2025, except they weren't part of AMD's keynote. Very little was known (officially) other than the names of the first two graphics cards for the family. That changes today, with AMD detailing many of the architectural upgrades, specifications, and more, during a video presentation. These will go up against the Nvidia Blackwell RTX 50-series GPUs and the Intel Battlemage Arc B-series GPUs and will likely join the ranks of the best graphics cards in the coming days.

Like Nvidia's RTX 50-series graphics cards, AMD's RDNA 4 launch seems to have been delayed, though perhaps for different reasons. There were rumors that the cards would be revealed at CES 2025 and launched in January, then February, and finally March. That last is no longer a rumor, with the RX 9070 XT and RX 9070 set to go on sale on March 6 — and in typical fashion, the ""MSRP"" or base model cards will have reviews go up the day before, followed by the overclocked non-MSRP models on the launch date. Nvidia's RTX 5070 will likely land right around the same time, just to make things even more exciting.
But if you look at graphics card availability right now, what becomes immediately clear is that virtually everything is sold out or, at the very least, seriously overpriced. AMD has had difficulties with GPU transitions in the past, with the prior generation hanging around for too long and competing with the new parts. This time, it seems to have gone the opposite way, with RX 7000-series GPUs mostly having disappeared from retail shelves in December and January. Only the lower tier RX 7600 and RX 7600 XT are still in stock at MSRP (or close to it).

The result has been dramatically increased demand for everything from mainstream to high-end graphics cards, and Nvidia's RTX 5090, RTX 5080, and RTX 5070 Ti all sold out almost instantly at launch. Will AMD's 9070 XT and 9070 fare better? We can hope so, but we suspect there's so much pent-up demand that even with another two months' worth of production and supply, it will still be insufficient. Hopefully, things will settle down later this year, but in the near term, we expect inadequate supplies and increased retail prices — and, yes, scalping.

No doubt Nvidia's record profits driven by AI are a big contributor, and while AMD isn't selling quite as many data center GPUs, a lot of its wafer allocation from TSMC is likely going to data center CPUs and GPUs as well. Gamers are no longer the top priority for either company, in other words; for the time being, they just get the scraps that fall from the AI table.

But enough sad talk. Let's check out the specifications for AMD's RDNA 4 GPUs, talk about architectural updates, and dig into all the other details. We even have pricing information, though as you can guess, that's worth about as much as the paper this is printed on. We'll continue updating this article as additional details become available, but for now, here's everything you need to know about the AMD RDNA 4 and Radeon RX 9000-series GPUs.
Here are the known specifications for the RX 9070 series GPUs, along with placeholder information on the RX 9060 series. AMD did share the 9060 name at CES 2025, but no other details have been shared. There are rumors, however, which we've used to flesh out the table — these are indicated by question marks in the various cells.
The RX 9070 XT and 9070 columns should be fully accurate. We're reasonably sure there will be a trimmed-down RX 9060 XT using the same Navi 48 die as the 9070 cards, just with fewer CUs (Compute Units) and memory controllers enabled. Below that, things get murky.

The RX 9060 could use a further binned Navi 48, or it could use Navi 44. Most of the details on Navi 44 are questionable at best, but we'll certainly find out more in the coming months. There might even be RX 9050-class GPUs at some point, but we've avoided listing those for the time being.

Looking at the RX 9070 XT, it uses a fully enabled Navi 48 die that includes 64 RDNA 4 CUs. Combined with a 2.97 GHz boost clock and a 256-bit memory interface with 20 Gbps GDDR6 VRAM, the other specifications mostly come from straight mathematical calculations. The RX 9070 is mostly the same configuration, just with 56 CUs and a 2.52 GHz boost clock — substantially lower than its bigger sibling, though we'll have to wait and see what real-world clocks actually look like.

Power targets also play a role in the final clock speeds, and where the 9070 XT has a 304W TBP (Total Board Power), the 9070 cuts that all the way down to 220W. That's probably a big factor in the 450 MHz difference in boost clocks.

Raw compute works out to 48.7 TFLOPS FP32 on the 9070 XT and 36.1 TFLOPS on the 9070. On paper, that makes the XT up to 35% faster. In practice, we suspect the two chips will be quite a bit closer and that the actual clocks in most games may only be a couple hundred MHz apart, despite what the specs suggest.

AMD has also given the Ray Accelerators and AI Accelerators in the CUs a massive overhaul compared to RDNA 3. For AI, each can do twice as many FP16 operations per cycle and they now support sparse operations. Sparsity can skip up to half of the zero multiply operations to potentially double performance, and it's a feature Nvidia has supported since its second-generation RTX 30-series GPUs. (AMD has also supported sparse operations on its CDNA family of GPUs for several years.)

Moreover, the AI units also support FP8, INT8, BF8, and INT4 operations, with the 8-bit calculations being twice as fast as 16-bit, and 4-bit integers double that again. Put it all together, and you get 389 TFLOPS of sparse FP16 compute and up to 1557 TOPS of sparse INT4 compute.

Keep in mind that the previous generation RDNA 3 architecture featured GPUs with up to 96 CUs and a 384-bit memory interface on the RX 7900 XTX, so while RDNA 4 GPUs are faster on a per-CU basis, AMD doesn't expect the RX 9070 XT to beat the RX 7900 XTX in all workloads.

There's a lot more going on than the raw specs will tell you. First, let's cover the pricing and launch date, then move on to the architectural deep dive.
How fast are the RX 9070 XT and RX 9070 graphics cards? We can't share our own internal benchmarks yet, but AMD provided comparisons against its previous generation RX 7900 GRE. In it's testing of 11 rasterization games and nine ray tracing games, the 9070 XT was on average 42% faster at 4K and 38% faster at 1440p, while the 9070 was 21% faster at 4K and 20% faster at 1440p.

Breaking things down into pure rasterization and pure ray tracing performance, the 9070 XT delivered 37% and 33% improvements in performance at 4K and 1440p in rasterization mode. In ray tracing games, it was 53% and 49% faster on average, again at 4K and 1440p, respectively. The RX 9070 meanwhile provided 18% and 17% higher rasterization performance at 4K and 1440p, while in ray tracing it was 22% and 20% faster on average.

These are AMD's own numbers, so we can't fully vouch for them, but we can use them to get a reasonable idea of where the new AMD cards might land relative to the RTX 5070 Ti and RTX 4070 Ti Super (both 16GB cards).

In our GPU benchmarks hierarchy, at 4K in our rasterization test suite the RTX 4070 Ti Super beats the 7900 GRE by 13% at 4K and 7% at 1440p. The RTX 5070 Ti on the other hand was 14% faster than the 4070 Ti Super at 4K and 11% faster at 1440p. Taken together, our existing test results indicate the 5070 Ti would be around 29% faster than the 7900 GRE for 4K rasterization and 19% faster at 1440p.

Flip over to ray tracing and our data has the 4070 Ti Super beating the 7900 GRE by 64% at 4K and 60% at 1440p. The new RTX 5070 Ti leads the 4070 Ti Super by 12% at both 4K and 1440p. That would thus put the 5070 Ti potentially 84% and 79% ahead of the 9070 XT.

Clear as mud? Let's put it this way: Our existing data combined with AMD's data suggests the 9070 XT will beat the 5070 Ti by perhaps 6% at 4K for rasterization and 12% at 1440p. In ray tracing, however, Nvidia would appear to still have the edge and be around 20% faster at 4K and 1440p. We can't be precise just because there are other factors in play — different testing suites, different platforms, and two levels of extrapolation (because we haven't tested the 7900 GRE on our new test suite yet), but overall it appears the 9070 XT will land reasonably close to the 5070 Ti in perforamnce.

Doing the same calculations for the vanilla RX 9070, it would be a decent 12 to 17 percent slower than the 9070 XT, which would be about where the RTX 4070 Ti sits in our testing. But we don't know precisely where the RTX 5070 will land yet, so we can't really come to any conclusion. With only 12GB of VRAM on the Nvidia 5070, however, it seems more likely that the 9070 will take an overall lead. We'll find out for certain next week, so stay tuned.
How much will the RX 9000-series GPUs cost? AMD has announced the Radeon RX 9070 will start at $549 and the RX 9070 XT will start at $599, placing them firmly in the ""mainstream"" segment. Given the current market conditions, however, it probably doesn't matter what AMD has given as the MSRP. Short-term, certainly, we expect the cards will all sell out and end up costing much more than the MSRP.

As we said in the Nvidia Blackwell overview, for dedicated desktop graphics cards, we're now living in a world where ""budget"" means around $250–$300, ""mainstream"" means $400–$700, ""high-end"" is for GPUs costing $800 to $1,000, and the ""enthusiast"" segment targets $1,500 or more. AMD is going after the mainstream segment with the 9070 series, and possibly the lower mainstream and upper budget segments with future 9060 series parts.

Depending on supply, as well as performance, the RDNA 4 series should be worth the price AMD is asking. More likely is that there simply won't be enough cards to satisfy the current demand, not for many months. Given the reasonably low-ish prices, don't be surprised if scalpers and retail markups step in and push the prices up.

It's basically a repeat of the cryptocurrency GPU mining shortages, only this time it's caused by AI and demand from that sector may not go away for many years. Let's hope we're wrong, but the RTX 50-series launches so far have not been promising.
We listed the March 6, 2025 release date for the RX 9070 cards already, but AMD has also at least partially teased an RX 9060 family of GPUs. Will there be multiple cards or only one? Will there be lower-tier RX 9050 cards as well? The short answer: We don't know. The nebulous answer: Sometime between April and the end of the year, hopefully sooner than later.

We've seen rumored die sizes for Navi 44 that suggest it's a much smaller chip, like more of a replacement for the current Navi 33 (RX 7600 series). If that's correct, it may not come out any time soon. There still appear to be plenty of RX 7600 and RX 7600 XT GPUs floating around, and that's because when those launched there were still a lot of similar performing Navi 23 (RX 6650 XT / RX 6600 XT / RX 6600) cards still available, at lower prices.

The naming scheme from AMD suggests that the RX 9060 will compete with the RTX 5060 family. That would perhaps require a larger chip than what's indicated. But RX 7600 does technically compete with the RTX 4060, and there's no RTX 4050 and probably won't be an RTX 5050.

Will AMD be making a ""true budget"" RDNA 4 chip? Again, rumors suggest that's at least possible, perhaps even likely. At less than half the size of Navi 48, AMD may try to create a $200~$250 graphics card to go after budget-minded gamers — and OEMs. Certainly it could get a lot more chips per wafer with the rumored 150~160 mm^2 die size.

But if the cards then only sell for $250 or less? That hardly seems worth the effort, not when companies can charge tens of thousands for data center GPUs.
Get Tom's Hardware's best news and in-depth reviews, straight to your inbox.
The above slide gallery covers the architectural briefing AMD provided in advance of today's reveal, including some of the specifications discussed above as well as finer details. AMD worked to refine the underlying architecture to improve per-CU performance in all major workloads: rasterization, ray tracing, and AI.

Rasterization performance sees the smallest generational gains, but it's still about 40% faster than RDNA 3 according to AMD. Ray tracing performance is basically doubled, and AI performance is doubled for dense FP16 compute, with lower precision formats delivering even higher performance.

The specific details of the rasterization improvements are a bit nebulous. RDNA 4 supports out of order memory requests, which AMD specifically notes as being helpful for ray tracing, but it can help rasterization tasks as well — we just don't have any details on how much. The other major change involves dynamic register allocation. RDNA 3 (and earlier) allocated registers for the worst case for shaders. By dynamically allocating extra registers only when needed, AMD provides an example use case where it could have an extra wave in flight. The slides show three waves versus four waves, which would be a 33% increase, but we don't know if that's representative of real workloads or just for illustrative purposes.

Moving on to ray tracing, this is where AMD spent a significant amount of effort. It doubled the ray/triangle and ray/box intersection rates per RT unit as a start. Then it offers some enhancements including hardware instance transforms (rather than doing a lot of the work via GPU shaders), oriented bounding boxes, an improved BVH (Bounding Volume Hierarchy) structure and traversal, the above-mentioned out of order memory returns, and better ray hardware stack management.

Most of the improvements come from the doubling of intersection rates and BVH compression, but the other aspects combine to deliver a solid improvement as well. How does RDNA 4 compare with Nvidia's latest hardware? That's not fully clear, but certainly it's going to do better per CU than what we saw with RDNA 3 and RDNA 2. It likely won't match Blackwell, but it might be better than Ampere and at least closer to Ada levels of performance.

AI, as already noted, sees the biggest changes. Nvidia has been iterating on its AI tensor cores since the RTX 20-series, and even before that the Volta data center GPU had tensor cores. So Nvidia is on its fifth generation of AI matrix cores while AMD is mostly on its second generation — mostly because it looks like AMD took a lot of the work that's been happening in its CDNA GPUs and brought it over to RDNA 4.

RDNA 3 CUs could do 512 FP16 operations per cycle, with no sparsity support, or 1024 INT4 operations per cycle. With RDNA 4, AMD doubles the baseline FP16 throughput for dense operations, doubles that again for sparse operations, and doubles that again for FP8 workloads — which are proving useful in the AI space. That's up to 8X higher AI throughput for FP8 on RDNA 4 compared to FP16 on RDNA 3, and the INT4 throughput sees a similar up to 8X improvement.

AMD gave a real-world example of how this affects AI performance using Stable Diffusion XL. The RX 9070 XT with 64 CUs took on the RX 7900 XT with 84 CUs. That gives the older GPU a 31% advantage in compute units, but the 9070 XT ended up delivering very close to 2X the performance. That will prove very helpful for other AI and machine learning workloads, including ML-based upscaling and frame generation (see FSR 4 below).

Alongside these changes, AMD has reworked some of the cache and memory hierarchy with RDNA 4. It didn't provide any clear details on what has changed, but it notes that this is the third generation of Infinity Cache. The capacity remains 64MB, the same as what was present on the 256-bit 7900 GRE and 7800 XT, but now the cache is again part of the monolithic chip, so it likely has better latencies and throughput.
RDNA 4 isn't just about core architecture upgrades. Along with the above rasterization, ray tracing, and AI enhancements, AMD has also upgraded a few other areas. One of the big changes is with the media encoding hardware. Last time we checked video encoding performance and quality, AMD came in last place, clearly behind Nvidia and Intel. It looks like RDNA 4 will close the gap.

AMD says it has improved H.264 (AVC) quality by up to 25%, H.265 (HEVC) by 11%, and improved the AV1 encoding efficiency. It also has better support for AV1 and VP9 decoding and reduced memory accesses.

Besides the quality improvements, RDNA 4 adds a dual media engine. Nvidia did this with its Ada architecture, and AMD seems to be taking a similar approach. It likely doesn't help all workloads equally, but AMD says it doubles the AV1 encoding throughput.

Realistically, there's only so far you can go with improving video encoding quality, particularly with hardware encoders. Intel and Nvidia are pretty comparable, but AMD was behind on quality while being ahead on performance with RDNA 3. It sounds like RDNA 4 will continue to be faster while offering similar quality to the competition, which is a good thing.

Another change with RDNA 4 is that AMD has added hardware flip queue support, which offloads video frame scheduling to the GPU. While Nvidia discussed something similar for MFG (Multi Frame Generation), it sounds like AMD's solution is focused on improving video playback by reducing CPU load, as opposed to being something to improve the scheduling of generated frames.

Radeon Image Sharpening (RIS) has also been updated, to RIS2. This is a driver level sharpening solution that's based on AMD's CAS algorithm (Contrast Aware Sharpening), only now the quality is supposed to be better. It's a single click toggle to apply RIS2 across all APIs.

Finally, RDNA 4 GPUs will support PCIe 5.0 interfaces. That doubles the throughput over the x16 link, though in practice most workloads likely won't see much benefit. Gaming in particular doesn't tend to need more than PCIe 3.0, or perhaps 4.0, when using a full x16 connection. However, AI and certain content creation tasks can benefit from the added bandwidth. Don't be surprised if the future Navi 48 chips and possibly even the RX 9060 XT cut the interface down to x8 or even x4 widths.
One thing that isn't changing from RDNA 3 is the memory support. While Nvidia has moved all of the announced Blackwell RTX 50-series solutions to GDDR7 memory, AMD will continue to use GDDR6 memory, clocked at up to 20 Gbps. Coupled with a 256-bit interface on the 9070 XT and 9070 GPUs, that results in 640 GB/s of memory bandwidth. That's the same VRAM capacity as the RX 7900 GRE and RX 7800 XT, and also the same as Nvidia's RTX 5070 Ti and RTX 4070 Ti Super.

The 64MB Infinity Cache will improve the effective bandwidth, though AMD didn't elect to provide any estimates of cache hit rates so far. The RX 7900 GRE and RX 7800 XT both had 64MB Infinity Caches, and AMD provided effective bandwidth rates that were about 4X the base memory bandwidth with those GPUs, so we'd anticipate the Navi 48 GPUs will see similar results.

It's also possible that further improvements to the Infinity Cache have made it less critical for AMD to move to GDDR7 at present. Considering that Nvidia gets a 40% improvement in raw bandwidth from 28 Gbps GDDR7 compared to AMD's 20 Gbps GDDR6, that might seem like a sizeable advantage. However, effective bandwidth after factoring in the large caches may not be all that different.

Plus, there's only so much bandwidth needed to drive a 64 CU GPU. Nvidia's RTX 5070 Ti for example has 70 SMs (Streaming Multiprocessors), which are roughly analogous to AMD's CUs, and the 5070 Ti has a 48MB L2 cache. Putting a larger 64MB L3 cache with fewer GPU processing clusters could reduce the need for higher memory speeds.

AMD continues to use 16Gb (2GB) GDDR6 modules, and we're unaware of any companies currently pursuing 24Gb (3GB) capacities. That's one area where GDDR7 support could prove beneficial for Nvidia in the future, though so far only the RTX 5090 Laptop GPU is using the higher capacity chips.
One of the interesting changes with RDNA 4 is that AMD is, at least for now, ditching the GPU chiplets approach. It may come back to that in the future, but the Navi 48 and presumably the rumored Navi 44 will be monolithic chips. Along with that design choice, AMD is also upgrading from TSMC's N5 process node used on RDNA 3 to the N4P node for RDNA 4.

N4P provides for modest improvements in performance and efficiency compared to the N4 node, which in turn refines the base N5 node. Our understanding is that N4P may introduce some additional metal layers, and N4 used more EUV than N5. What's not entirely clear is how N4P compares to 4N and 4NP — the ""for Nvidia"" variants that are used with Hopper, Ada, and Blackwell. It's probably pretty similar in most respects, which means that AMD will be on node parity with Nvidia this round.

But AMD isn't really trying to take down Nvidia's top GPUs. The lack of GDDR7 memory and the lack of a larger design prove this. The Navi 48 chip will house 53.9 billion transistors in a 356.5 mm^2 die. Nvidia's GB203 used in the RTX 5080 and 5070 Ti contains 45.6 billion transistors in a 378 mm^2 die... which might suggest AMD actually has a superior process node and/or design. But we can't really conclude that.

While die sizes are pretty straightforward, transistor counts are not. They're more of a mathematical estimate, and there are different ways of counting what constitutes a ""transistor."" Perhaps AMD does have a denser design with more transistors, perhaps not. Ultimately, we'll have to see how the various GPUs perform.

One interesting side note here is that Navi 31, the top solution from the RDNA 3 family, had a 300 mm^2 GCD (Graphics Compute Die) with six 37.5 mm^2 MCDs (Memory Cache Dies). I wondered when AMD revealed the specs just how much it was actually saving by going the chiplet route. The GCD had 45.6 billion transistors, which means the overall transistor density — looking at the RDNA 3 GCD compared to the RDNA 4 monolithic design — is basically identical (152 MTrans/mm^2 on Navi 31 GCD compared to 151.2 Mtrans/mm^2 on Navi 48).

But let's not get too carried away. It's known that scaling on external interfaces — like the GDDR6 memory controllers — is quite poor with newer process nodes. Navi 31 used twelve 32-bit controllers while Navi 48 has eight 32-bit controllers. If AMD had attempted to make a 384-bit interface on a monolithic design, it would have certainly required a larger chip. Putting that on an older process node for the prior generation did make financial sense at the time, and may yet prove a smart approach for a future AMD product.
If it wasn't clear yet, AMD will not be making or selling its own reference model RX 9070 series graphics cards. Despite providing some slides of what appear to be MBA (Made By AMD) cards, these are merely graphical renders rather than photos of actual hardware. There were certainly prototype cards created during the design, testing, and validation process, but what those looked like isn't really important.

All of the RX 9070 series graphics cards will be made by AMD's add-in board (AIB) partners. That means two things. First, we'll see a lot of variation in final clock speeds and power draw, not to mention things like the number of fans and RGB lighting. But more importantly, it means AMD has a lot less say in the actual retail graphics card prices.

Very likely AMD has a requirement that all of the AIBs have at least one model for each GPU that will be nominally priced at the stated MSRP. Beyond that, however, all bets are off. ""Here's our RX 9070 XT Red Herring for $599... and we sold all of those. Sorry! But you can pick up our Redder Herring OC model for $799!"" We saw something like this with the RTX 5070 Ti cards, where there also isn't a reference model from Nvidia.

Long-term, if there's insufficient supply to meet the demand, most AIBs are going to produce higher tier models with a few minor extras and drastically inflated prices. If on the other hand the supply catches up to demand, then it's easy enough to drop prices as needed.
Besides the hardware, AMD has been working on a variety of feature improvements. The biggest one is undoubtedly FSR 4. The fourth iteration of AMD's FidelityFX Super Resolution (FSR) algorithm, it will break with tradition in a couple of key ways.

First, FSR 4 will leverage the more potent AI accelerators in the RDNA 4 GPUs. At launch, it will require an RDNA 4 GPU. Down the road a few months, AMD may try back-porting the algorithm so that it can run on RDNA 3 and maybe even RDNA 2 GPUs... but it seems unlikely.

Instead, FSR 4 will basically co-exist with FSR 3.1, or rather, the non-AI upscaling will continue to be offered. It's not entirely clear exactly how this will play out, but keeping everything unified under one name makes more sense. What we do know is that AMD plans to allow gamers to use the more potent FSR 4 algorithm on games that have FSR 3.1 support. Will that happen automatically or require a driver settings toggle? It seems like the latter but we'll have to wait and see.

FSR 4 isn't just for upscaling, either; it also has frame generation. From our understanding, both upscaling and framegen will use the AI accelerators of the RX 9000-series GPUs. AMD also says RDNA 4 is ""neural rendering ready"" without really going into further detail. Presumably that's related to Microsoft's new Cooperative Vectors feature, which is something Nvidia also talked about with Blackwell.

We've asked for additional details on how FSR 4 works, in terms of the computations. AMD hasn't responded, but one slide does note that the RX 9070 XT offers ""up to 779 TOPS AI Acceleration"" while talking about FSR 4. Now, that's either dense INT4 operations or sparse INT8 operations, as the 9070 XT hits double that figure for sparse INT4, but we don't have a direct answer on whether the algorithms are using INT4 or INT8 yet. Either way, that's a lot more theoretical compute than what you can get from prior generation AMD GPUs, which is why we don't anticipate the AI upscaling and framegen models getting backported.

We also asked if FSR 4 was using a transformers-based network or a convolutional neural network. DLSS 4 offers better image fidelity than DLSS 3 by using transformers, and AMD may have skipped the CNN approach since it's late to the AI-powered upscaling and framegen party. However, we don't have a direct answer yet. We do have some image quality comparisons from AMD, in the slides above, and FSR 4 definitely looks better than FSR 3.1.

As with Nvidia's use of performance mode upscaling with framegen, we don't generally focus on the promised performance after all these extras. Framegen in particular is very heavy on marketing in our experience. It's less problematic when you already have a high base framerate, but then it's also less necessary when you're already getting 100+ FPS.

AMD says it will have over 30 games with FSR 4 enabled for the RX 9070 series launch, with 75+ games coming in 2025.

AMD also talked about HYPR-RX, which combines a variety of driver-level performance boosting features and can be enabled with a single click. We've poked at it a bit in the past, and it can be useful in some cases, but we prefer sticking with apples to apples comparisons. If you're just playing games, however, enabling HYPR-RX to apply all of the features including FSR/RSR, Anti-Lag, Radeon Boost, and AFMF 2 could be useful.

AMD also has a new AFMF 2.1 release that imprves the quality of the algorithm, reducing ghosting, improving fine features, and detecting and handling overlays better.
The last item AMD discussed is its new Adrenalin 25.3.1 drivers along with some new software. While most of the driver interface will be familiar to AMD GPU users, there are some new additions along with some behind the scenes changes. AI plays a role in both areas. We've already discussed FSR 4 upscaling and framegen, so let's talk about the other AI uses.

First, AMD is using AI to help find rendering errors and to detect instability and other issues. AMD claims its new 25.3.1 drivers will be perhaps the best and most stable drivers it has ever released, with fewer rendering errors. We'll have to wait and see how that goes...

Moving on, similar to Nvidia's Chat RTX and other tools, AMD is providing some easy to access AI-powered features. These are all managed by a new AMD Install Manager that sits alongside the usual AMD Software in your system tray. Besides your GPU drivers, it can also detect if you have an AMD platform and keep your chipset drivers updated. And then there are some new extras: AMD Chat, AMD Image Inspector, and the AI Apps Manager (among others).

AMD Chat is a chatbot designed to answer questions specifically about your PC hardware and GPU. You can ask it about GPU temperatures, performance, and more. It's a hefty 25GB download, though, so you might not want to install if it you're low on space — or if chatting with your PC isn't something you plan on doing.

The AI Apps Manager provides a list of software and utilities that can use AI that are installed or available to install. So if you have Adobe CC, some of those apps might show up. Or you can use it to install Amuse, AMD's tuned AI image and video generation tool.

Finally, the Image Inspector is a feature to help with finding and reporting rendering errors and bugs. AMD is already using AI to help it find issues internally, and the Image Inspector is an opt-in feature that allows you to participate. Using spare GPU resources (so it won't go crazy and use all your GPU power if you're in a demanding game), it can automatically capture rendering errors and submit them to AMD, should you enable the feature. It sounds interesting, but we suspect there might be a performance hit still, even if it's small.
Frankly, RDNA 4 feels like what AMD should have been doing with RDNA 3 rather than pursuing the abandoned-for-now GPU chiplets approach. AMD has finally decided to put serious effort into ray tracing hardware and AI in its consumer product line. We can understand why RDNA 2 was lacking in these areas — Nvidia's GeForce RTX feature set probably caught the company off guard — but when RDNA 3 arrived in 2022, it really needed to do more and be more.

What's interesting is that all of these new hardware features haven't caused a massive bloat in the die size. Navi 48 is 357 mm^2 on a 5nm-class node (N4P). Navi 31 was 300 mm^2 on a 5nm-class node (N5), with Infinity Fabric links to the external memory and cache chiplets. Rip out those links, rework the cores, and this was all possible several years ago. Which is obvious, since Nvidia already did that, but it felt like RDNA 3 doubled down on the ""ray tracing and AI aren't really that important"" marketing and got left behind. RDNA 4 finally rights that misstep, or at least attempts to. Now we just need to see how the actual hardware performs in a variety of tasks.

AMD's RDNA 4 GPUs will have to compete with Nvidia Blackwell RTX and Intel Arc Battlemage solutions. As we discussed already, supply and availability of all graphics cards has become quite poor of late. (Yes, that's a sarcastic understatement.) Every recently launched GPU has sold out quickly, with many cards then selling at prices far above the original MSRP. It started with the Arc B580 and became especially painful with Nvidia's Blackwell launches.

Things aren't going to get better in the near term. The big issue is that there are a lot of companies competing for a limited supply of silicon manufacturing. TSMC only has the ability to process so many wafers in a month. Right now, AMD, Apple, Intel, and Nvidia are all using TSMC for various chips, and there are plenty of other companies as well — Broadcom, Facebook, Google, Amazon... the list can get quite large.

Even if a company pays for a certain number of wafers in a given month, what to do with those wafers is still up for debate. Just looking at the main PC companies, AMD could make RDNA 4 GPUs on TSMC's N4P node, sure. Or it could make Zen 5 CPU chiplets for both Ryzen and EPYC CPUs, CDNA 3 data center GPUs (MI300X), other Ryzen APU designs for laptops and handhelds, or the future CDNA 4 GPUs that are likely coming this year. Nvidia has Grace CPUs, Hopper and Blackwell data center GPUs, NVLink processors, Ada Lovelace previous generation GPUs, and the new Blackwell RTX GPUs that are all using variants of TSMC's 5nm-class nodes. And Intel has leveraged TSMC for all or part of its Arrow Lake, Lunar Lake, and Battlemage product lines.

Nvidia made record profits last year of $130 billion, primarily driven by AI. Its consumer gaming division only accounted for $11.35 billion, 8.7% of the total. And going forward, Nvidia will likely invest even more heavily in data center GPUs. That will eat up a lot of wafers, needless to say, and gaming will have to compete for its share.

The good news is that more manufacturing capacity is coming online. A lot of that will likely go to create more AI processors, but the more capacity that exists, the more likely it is for other, less profitable chips — like consumer GPUs — to get made. And maybe AMD and Intel will try to grow their gaming GPU divisions while Nvidia is otherwise occupied. Or maybe Nvidia will treat gaming as a passion project that started the company and so it will try to ensure at least a reasonable number of chips get made. Maybe, maybe...

Whatever happens, what's clear right now is that, as long as AI continues to grow as an industry, gaming GPUs are now a lower priority item for most of the biggest players in the graphics space. Let's hope that, like cyptocurrency mining, this turns out to be just a passing phase. But we wouldn't bet the farm on that.
Jarred Walton is a senior editor at Tom's Hardware focusing on everything GPU. He has been working as a tech journalist since 2004, writing for AnandTech, Maximum PC, and PC Gamer. From the first S3 Virge '3D decelerators' to today's GPUs, Jarred keeps up with all the latest graphics trends and is the one to ask about game performance.

Tom's Hardware is part of Future US Inc, an international media group and leading digital publisher. Visit our corporate site.

©
Future US, Inc. Full 7th Floor, 130 West 42nd Street,
New York,
NY 10036."
"general","article","What You Need to Know About AMD's RX 90 Series","https://www.xda-developers.com/amd-rx-9000-series-facts-pc-building/",""
"general","article","AMD Teases Radeon RX 9000 Series Launch Event, Date Not Specified","https://www.techpowerup.com/331197/amd-teases-radeon-rx-9000-series-launch-event-date-not-specified",""
"general","article","AMD Radeon RX 9060 XT Graphics Cards Could Launch Shortly After Computex 2025","https://www.techpowerup.com/335547/amd-radeon-rx-9060-xt-graphics-cards-could-launch-shortly-after-computex-2025",""
"general","article","AMD Radeon RX 9060 XT Launches on June 5, Starting at $299","https://www.tomshardware.com/pc-components/gpus/amd-radeon-rx-9060-xt-launches-on-june-5-starting-at-usd299","The 8GB version starts at $299, and the 16GB version at $349.

When you purchase through links on our site, we may earn an affiliate commission. Here’s how it works.

AMD has seized the opportunity presented by Computex 2025 to announce the chipmaker's third addition to the Radeon RX 9000 series. The newly unveiled Radeon RX 9060 XT complements the previously released Radeon RX 9070 and Radeon RX 9070 XT, strengthening AMD's RDNA 4 gaming portfolio to compete with the best graphics cards.
The Radeon RX 9060 XT serves as the successor to the Radeon RX 7600 XT, which was introduced in January 2024. Although it has only been a little less than a year and a half since the previous RDNA 3 graphics card has been out, the release of Nvidia's GeForce RTX 5060 Ti, coupled with the recent coming of the GeForce RTX 5060, means AMD needs a contender to compete within that market segment. Thus, the Radeon RX 9060 XT couldn't have arrived at a better time.
AMD provided renders of the Radeon RX 9060 XT solely for artistic and illustration purposes. Similar to the Radeon RX 9700 series situation, AMD isn't releasing any reference or MBA (Made by AMD) Radeon RX 9060 XT cards. However, the chipmaker's partners will offer reference-clocked models that should stick to or close to the MSRP.
Up until this point, AMD has been using the Navi 48 silicon for its Radeon RX 9070-series graphics cards; however, given the Navi 48's size and the Radeon RX 9060 XT's lower specifications, it wouldn't be a wise business decision to use the aforementioned silicon. Instead, AMD has introduced a smaller piece of RDNA 4 silicon shaped like Navi 44. As a result, the Radeon RX 9060 XT is the first SKU to leverage this silicon, and it probably won't be the last.
The Radeon RX 9060 XT transitions to a complete PCIe 5.0 x16 interface, opposite the Radeon RX 7600 XT, which was restricted to a PCIe 4.0 x8. For the past couple of generations, AMD has used the x8 interface for its Radeon RX x600-series graphics cards, such as the Radeon RX 7600 XT or Radeon RX 6600 XT. These SKUs typically use smaller silicon, and sometimes even silicon for mobile graphics cards, so a x16 interface usually doesn't offer substantial benefits. However, the latest GeForce RTX 5060 Ti 8GB debacle has shown up to a 10% performance hit when used on a PCIe 4.0 interface. Sticking with an x16 interface for the Radeon RX 9060 XT is a sound move.
Navi 44 shares characteristics similar to Navi 48 in design and manufacturing processes. AMD's new silicon continues to sport a monolithic design and is manufactured using the same TSMC's N4P (4nm) FinFET process node. The Navi 44 silicon has a die size of 199 mm² with up to 29.7 billion transistors.
In contrast, Navi 33, which is used in the current Radeon RX 7600 XT, is built on TSMC's N6 (6nm) FinFET process. The silicon for Navi 33 is 204 mm² and contains up to 13.3 billion transistors, resulting in a density of 65.2 million transistors per mm². AMD decreased the die size of Navi 44 by approximately 2% relative to Navi 33, which is not a major reduction. However, Navi 44's transistor count has substantially improved, housing up to 2.2X more transistors than Navi 33. It works out to an impressive density of 149.2 million transistors per mm².
Get Tom's Hardware's best news and in-depth reviews, straight to your inbox.
In addition to enhancing performance and transistor count, TSMC's N4P FinFET manufacturing process also boasts improvements in power efficiency. However, we'll have to wait for a full review of the Radeon RX 9060 XT to measure how efficient the graphics card is compared to the Radeon RX 7600 XT.
The Radeon RX 9060 XT utilizes the full Navi 44 silicon, thus having access to all 32 Compute Units (CUs) equivalent to 2,048 Stream Processors (SPs). That's the exact core specifications as the Radeon RX 7600 XT. Therefore, the Radeon RX 9060 XT's performance uplift comes from the prowess of the new RDNA 4 CUs.
The graphics card also has 32 third-generation Ray Tracing cores, which can double the throughput compared to the second-generation counterparts inside the Radeon RX 7600 XT. Meanwhile, the 64 onboard second-generation AI accelerators can deliver up to 821 TOPS.
The Radeon RX 9060 XT boasts a 3,130 MHz boost clock, even higher than the Radeon RX 9070 series. Compared to its predecessor, the Radeon RX 9060 XT has a 14% higher boost clock speed, translating to around 13% higher FP32 performance on paper.
According to AMD's provided gaming benchmarks, the Radeon RX 9060 XT 16GB is up to 6% faster than the GeForce RTX 5060 Ti 8GB across 40 tested titles at 1440p (2560x1440) resolution with Ultra settings. AMD's selection does raise some eyebrows. It would have been a fairer comparison if the chipmaker had used the 16GB variant of the GeForce RTX 5060 Ti instead of the 8GB variant.
In contrast to the Radeon RX 7600 XT, which was only available with 16GB of GDDR6 memory, AMD offers the Radeon RX 9060 XT in 8GB and 16GB flavors. Regardless of the variant, the memory operates at 20 Gbps across a 128-bit interface. The Radeon RX 9060 XT delivers a memory bandwidth of up to 320 GB/s, 11% more than the existing Radeon RX 7600 XT. Meanwhile, the capacity of Infinity Cache remains the same on the Radeon RX 9060 XT.
The Radeon RX 9060 XT's TBP (Total Board Power) varies between the 8GB and 16GB models. The former is rated for 150W, and the latter is slightly higher at 160W. They're 21% and 16% lower than the Radeon RX 7600 XT, respectively. In any event, gamers can rest easy as the Radeon RX 9060 XT only employs a single 8-pin PCIe power connector.
RDNA 4 has permitted the Radeon RX 9060 XT to feature upgraded display outputs. It offers DisplayPort 2.1a and HDMI 2.1b support, contrary to the Radeon RX 7600 XT's DisplayPort 2.1 and HDMI 2.1a. The Radeon RX 9060 XT only provides two DisplayPort 2.1a outputs and a single HDMI 2.1 port, one less DisplayPort 2.1a output than the Radeon RX 9070 series.
FidelityFX Super Resolution 4 (FSR 4) launched alongside the Radeon RX 9000 series with initial support for 30 games. AMD expects the list to extend beyond 60 by the Radeon RX 9060 XT's launch date. The chipmaker is also preparing FSR Redstone, powered by machine learning, for a 2H launch. FSR Redstone will bring some cool features, such as neural radiance caching, machine learning ray regeneration, and machine learning frame generation.
The Radeon RX 9060 XT is available in 8GB and 16GB versions, priced at $299 and $349, respectively. The 8GB model is 9% less expensive than the Radeon RX 7600 XT, whereas the 16GB model costs just 6% more.
AMD is positioning the Radeon RX 9060 XT 16GB, priced at $349, against the GeForce RTX 5060 Ti 8GB, which is priced at $379. Consequently, the former features a price that is 8% lower. By AMD's estimate, the Radeon RX 9060 XT 16GB, on average, seemingly offers gamers 15% better gaming performance per dollar compared to the GeForce RTX 5060 Ti 8GB.
The Radeon RX 9060 XT will be available on June 5. You can expect a diverse offering of custom models from vendors such as Acer, ASRock, Asus, Gigabyte, PowerColor, Sapphire, Yeston, and XFX.
Follow Tom's Hardware on Google News to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button.
Zhiye Liu is a news editor and memory reviewer at Tom’s Hardware. Although he loves everything that’s hardware, he has a soft spot for CPUs, GPUs, and RAM.

Tom's Hardware is part of Future US Inc, an international media group and leading digital publisher. Visit our corporate site.

©
Future US, Inc. Full 7th Floor, 130 West 42nd Street,
New York,
NY 10036."
"general","article","AMD's New RX 9060 XT Looks Set to Challenge Nvidia's RTX 5060 GPUs","https://www.theverge.com/news/670285/amd-radeon-rx-9060-xt-announcement-computex","AMD’s latest RDNA 4 graphics card starts at $299 and launches in June.
AMD’s latest RDNA 4 graphics card starts at $299 and launches in June.
AMD is officially announcing its Radeon RX 9060 XT GPU at Computex today. Like the number implies, this graphics card will challenge Nvidia’s recently released RTX 5060 and RTX 5060 Ti, with AMD offering models with 8GB or 16GB of VRAM. AMD is launching both models on June 5th, with the 8GB variant priced at $299, with the 16GB version priced at $349.
AMD is following Nvidia’s controversial choice to ship a modern GPU with just 8GB of VRAM in the year 2025. The 8GB of VRAM debate has been raging for months now, particularly because of the latest games that can be very demanding on the memory side. AMD is following in Nvidia’s footsteps, though, so it’ll be interesting to see what reviewers make of both cards in this important part of the market.
The RX 9060 XT will ship with 32 RDNA 4 compute units, a boost clock of 3.13GHz, and support for DisplayPort 2.1a and HDMI 2.1b. The total board power is between 150 watts and 182 watts, depending on the model. AMD claims its 16GB version of the RX 9060 XT will be around 6 percent faster than Nvidia’s RTX 5060 Ti at 1440p resolution, based on 40 games that AMD has tested itself.
We’re still waiting to hear how the RTX 5060 stacks up, because oddly, Nvidia launched its latest 50-series GPU yesterday without any reviews available. The GPU maker had reportedly prevented reviewers from obtaining the necessary driver to test the RTX 5060 ahead of the release date, presumably because it’s worried about the paltry 8GB of VRAM spec.
While the 8GB of VRAM choice for both Nvidia and AMD is controversial, Nvidia has managed to spark a further wave of outrage from PC gaming YouTubers over comments it has made to Gamers Nexus. In a 22-minute video, Gamers Nexus discusses the pressure from Nvidia to include Multi Frame Generation (MFG) in benchmarks against competitor cards that don’t have a similar feature. Gamers Nexus (GN) alleges that Nvidia has even implied that it would revoke access to interview Nvidia engineers unless the channel discussed MFG more.
Update, May 21st: Article updated with pricing and release date information that AMD didn’t share with The Verge ahead of its press conference.
A weekly newsletter by David Pierce designed to tell you everything you need to download, watch, read, listen to, and explore that fits in The Verge’s universe.
© 2025 Vox Media, LLC. All Rights Reserved"
"general","article","AMD Insists It Was Right to Make an 8GB Version of RX 9060 XT GPU, but PC Gamers Are Finding It Easy to Be Cynical About This Model","https://www.techradar.com/computing/gpu/amd-insists-it-was-right-to-make-an-8gb-version-of-rx-9060-xt-gpu-but-pc-gamers-are-finding-it-easy-to-be-cynical-about-this-model","What’s in a name? Quite a lot when it comes to AMD’s latest GPU

When you purchase through links on our site, we may earn an affiliate commission. Here’s how it works.

AMD has shot back at critics after coming under fire for producing a version of its newly revealed RX 9060 XT graphics card that has an 8GB loadout of video RAM (VRAM).
The RX 9060 XT was revealed earlier this week in both 16GB and 8GB versions. The latter is causing anger, as some argue it is not enough for modern PC gaming, and there are other worries here, too.
Michael Quesada, who runs a Spanish YouTube channel on the topic of PC gaming, aired an indignant post on X asking why AMD (and Nvidia) keep making GPUs with 8GB of VRAM, questioning how that’s justified in 2025.
VideoCardz noticed that Frank Azor, AMD’s head of consumer and gaming marketing, was drawn to reply, as you can see below.
Majority of gamers are still playing at 1080p and have no use for more than 8GB of memory. Most played games WW are mostly esports games. We wouldn't build it if there wasn't a market for it. If 8GB isn't right for you then there's 16GB. Same GPU, no compromise, just memory…May 22, 2025
Azor observes that most gamers are still running at 1080p resolution and, therefore, don’t need more than 8GB of VRAM. The AMD exec notes that the most popular games are esports titles, which are less demanding, and that Team Red wouldn’t make an 8GB graphics card if there wasn’t a demand for it.
Azor concludes: “If 8GB isn’t right for you then there’s 16GB. Same GPU, no compromise, just memory options.”
To be fair to Azor, there’s some truth to what the executive says here. Certainly, for a more casual level of gaming, as well as esports titles that are built for fluid frame rates in general, as that’s more important than graphical bells and whistles to competitive players, 8GB is likely enough.
Sign up for breaking news, reviews, opinion, top tech deals, and more.
As others point out, it’s not enough for all PC games, even at 1080p resolution. Although tweaking graphics details suitably and making some compromises, you can generally get by, albeit there are notable exceptions even at 1080p.
But despite the noise made by the ‘8GB just isn’t enough these days’ camp on social media – and it is a fair old racket, make no mistake – some of the negative feeling here is more about deceptive naming.
Rather than having the RX 9060 XT 8GB and RX 9060 XT 16GB, there should have been a clear naming delineation between these two variants. The most prevalent suggestion is that AMD should’ve called the 8GB spin the plain old RX 9060, dropping the XT suffix.
Why is making that naming distinction important? Because what can happen with both graphics cards being called the ‘RX 9060 XT’ is that system builders simply list that as the GPU in any given PC, with no accompanying memory details. Less informed consumers may not even be aware that there are two different variants of the RX 9060 XT.
They may have perused opinions or reviews of the 16GB flavor and assume that’s what they are getting in their shiny new PC, when in fact it has the somewhat inferior 8GB GPU.
PC builders may deliberately not make that clear, because the system is cheaper to produce with the RX 9060 XT 8GB, but they won’t drop the price to consider that. In other words, this is a knowledge trap for the unwary and a way for system makers to take advantage of them. And it’s an avenue AMD could have shut off with different names for the 8GB and 16GB cards.
AMD might argue that it intends to have an RX 9060 vanilla GPU in the future, so it couldn’t use that name, but surely it could’ve found some suitable way of denoting the difference. Such as calling the 16GB version the 9060 XTX (although that’s a suffix reserved for the flagship GPU, you get the idea).
There’s a level of unhappiness and cynicism around the naming here, in short, and we should note this applies to Nvidia as well as AMD (with Team Green’s xx60 Ti models that have both 8GB and 16GB versions in the same vein).
AMD does get some credit here for ensuring it hasn’t further hamstrung the RX 9060 XT for some gamers with older motherboards by halving the number of supported PCIe lanes. Still, I won’t go into that here, as it’s getting sidetracked really (and it’s something I’ve discussed elsewhere).
To summarize: 8GB should be okay for a lot of games at 1080p resolution, with some down-tuning of graphics details as appropriate – but it won’t work well for everything, and the level of future-proofing feels wonky indeed.
On top of that, be careful of prebuilt PCs that list an RX 9060 XT graphics card with no accompanying spec info – it’s almost certainly going to be the 8GB version, and you may be paying more for it than you should.
For those buying a standalone RX 9060 XT, it makes sense to pay the premium for the 16GB version. It’s worth doing so for future-proofing alone, and it promises to be an excellent graphics card for the money overall.
That said, this assumes the premium is roughly 15% extra as per the MRSPs and that demand for the 9060 XT 16GB doesn’t considerably inflate the price. If it does, then that muddles the value equation a lot more. Hopefully, stock won’t be a problem, though, if the rumors are right. It’s only if supply is thin that jacked-up prices start to rear their ugly heads.
If another rumor is correct, the 16GB board will be the RX 9060 XT model predominantly stocked at retailers, so that’ll be the one you mostly see if you’re on the hunt for an AMD GPU, anyway.
Although that also brings the suggestion that the 8GB flavor is being kept more to PC builders, which could fan the aforementioned flames of cynicism around this whole affair – assuming this is anything more than empty chatter.
Darren is a freelancer writing news and features for TechRadar (and occasionally T3) across a broad range of computing topics including CPUs, GPUs, various other hardware, VPNs, antivirus and more. He has written about tech for the best part of three decades, and writes books in his spare time (his debut novel - 'I Know What You Did Last Supper' - was published by Hachette UK in 2013).
Please logout and then login again, you will then be prompted to enter your display name.

TechRadar is part of Future US Inc, an international media group and leading digital publisher. Visit our corporate site.

©
Future US, Inc. Full 7th Floor, 130 West 42nd Street,
New York,
NY 10036."
"general","article","Wired – The Essential CPU and GPU Field Guide for 2025","https://www.wired.com/story/intel-amd-qualcomm-nvidia-new-cpus-and-gpus-ces-2025","At CES 2025, AI is everywhere—just as expected—in the CPU and GPU announcements from the major players in the biz, where every last company is touting how their processors will be the end-all, be-all of AI-driven productivity. Never mind if you don’t know your Stable Diffusion from your DeepL Translate: Your next PC is going to be tuned for all of it.
Need help making sense of it all? We sifted through the keynotes and other announcements from the four big chipmakers at CES this year to get the lowdown on what’s in store for the PC space in the months to come. It’s all digested below but, spoiler alert, every one of them says their chips are going to be the fastest and bestest on the market.
As a foreword, note that we’re not delving into non-PC announcements here, including things like the Snapdragon Digital Chassis for automotive platforms, Nvidia’s homegrown AI foundational model Cosmos, or chips specifically designed for use in robotics or various smart home technologies.
Intel has launched a glut of new Core and Core Ultra processors—a total of eight different series across three major product lines—along with some minor upgrades to existing chips. While Intel is in the unenviable position of playing catch-up to much of the market on performance, its efficiency-focused messaging isn’t overwhelmingly convincing that things are going to change quickly—though it does have some muscle hitting the market. Here’s what’s new, and you can get the full low-down from Intel here.
Intel’s Core Ultra Series 2 (Arrow Lake) launched in October to a collective groan. The chips were supposed to make Intel competitive with other platforms, but for the most part performance was static over the previous generation of Core Ultra chips, with particularly weak gaming performance. They also aren’t Copilot+ PCs: Only Intel’s Lunar Lake–based machines are able to be classified as Copilot+ PC systems, thanks to their improved neural processing unit.
The new Core Ultra laptop chips target gamers and enthusiasts (200HX), premium laptops (200H), and ultralights (200U)—with a total of 15 new SKUs announced across the Ultra 5, 7, and 9 series. Chips will have between 10 and 24 cores and do not have memory on the die. Only the 200H line will use Intel’s upscaled integrated Arc graphics; the others will opt for lower-end silicon (though 200HX systems will surely pair the chip with discrete graphics processing).
As they’re built on Arrow Lake and not the newer Lunar Lake, all three new chips will still miss out on Microsoft’s Copilot+ PC designation. Despite boasting overall performance of 99 TOPS, the NPUs on these chips won’t have enough juice to hit the requirement of 40 TOPS (Trillions of Operations Per Second) delivered on their own. For what it’s worth, Intel’s messaging strongly stresses that “TOPS alone does not define AI performance.”
This series is a desktop-class chip collection denoted by the lack of an “Ultra” in the name. The 200S is a new design, formally code-named Bartlett Lake, while the 200H and 100U are updates of the 13th- and 14th-generation Intel Core “Raptor Lake” platform, which launched in October 2022. Intel didn't share much on what distinguishes these chips or where they’re likely to end up.
Codenamed Twin Lake, the Intel Core 3 and the Intel (no model name) processor are both very low-power chips (as low as 6 watts) targeted at “low-power, low-cost edge systems.” Imagine these showing up in bare-bones laptops and embedded systems like storage devices, televisions, and so on.
This is an existing Lunar Lake chip that is getting Intel vPro features, which is an enterprise management and security system.
Qualcomm had a lot of news, but its PC-centric announcement list was relatively tame, limited to these two updates. Complete details can be found here.
This is the fourth platform in the Snapdragon series and the lowest cost of them all, designed to be installed in Copilot+ PCs in the rock-bottom $600 range. It’s a single-SKU, 8-core chip with stated performance of 45 TOPS. Qualcomm is also targeting mini desktop PCs with this chip for the first time. Devices are expected in “early 2025.”
Snapdragon Copilot+ PCs famously can’t run a number of applications, including most VPN software and various cloud storage apps. That’s been changing in recent weeks (many of these apps are now in beta), and Qualcomm says it’s only going to get better.
Given AMD’s recent gains in the market—it has over a third of the x86 market today—I was expecting some big news from the chipmaker, but that wasn’t the case. AMD announced a handful of new CPUs in a fairly no-nonsense presentation focused largely on its performance leadership and how much the company loved video games. Notably absent at the keynote: No announcement or even much of a mention of new Radeon GPUs, though a new series was revealed after the presentation. (About half of the presentation was dedicated to corporate devices and the company’s many enterprise partnerships.)
Here’s what’s coming, all in Q1 and Q2 of 2025. Complete details can be found here and here.
This is AMD’s fastest and most advanced CPU to date, featuring 16 or 12 cores and purpose-built for creators and (especially) gamers. AMD says the new chip will provide an average 8 percent performance boost on gaming framerates and a 10 percent improvement on other tasks.
The Ryzen AI chip isn’t an NPU but rather a CPU that’s tuned for overall performance, including AI workloads. The first Ryzen AI CPU—the Ryzen AI 300—launched in late 2024, and a collection of much faster follow-ups are arriving soon, topping out at the Ryzen AI Max (available in seven different SKUs). With up to 16 cores and support for 128 GB of memory, AMD says the AI Max offers NPU performance of 50 TOPS. AMD naturally says the Max is embarrassingly fast at all kinds of tasks—and boldly says it can compete with the Apple M4 CPU on some of them. Ryzen AI chips are also set to show up in mini PC designs.
AMD hasn’t revealed much detail about what would distinguish its latest GPU, except that the 9070 line is targeted at midrange users. More notable is that AMD’s naming scheme is evolving to align more closely with how Nvidia names its products. The 9070 and 9070 XT are coming this quarter.
Nvidia CEO Jensen Huang took to the stage in a sparkly jacket and spent close to two hours outlining the company’s upcoming plans—and as expected it’s virtually all about AI. But rather little of the keynote covered Nvidia’s bread-and-butter GPUs, which include this new stuff. Complete details can be found here and here.
Surprising no one, Nvidia announced a new graphics processor, the GeForce RTX 50 series. On the desktop, these GPUs will launch at the end of January 2025. The big advance is a technology called Deep Learning Super Sampling (DLSS) 4 and Multi Frame Generation, which uses AI techniques to generate part of the pixel stream instead of doing so via brute force tactics, dramatically improving performance. Huang says the new graphics card has up to 4,000 TOPS of AI power. The top-tier card in the series—the 5090—will run a mere $1,999.
The RTX 50 series is also being shrunk down for laptop implementations, starting in March 2025. You’ll get a bit less than half the performance from the mobile version vs. the desktop, but Huang says AI will ensure your laptop won’t melt while you’re using it. Prices for RTX 50-equipped laptops will run up to $2,899, providing a maximum power of 1,850 AI TOPS.
This is the backbone for Nvidia Project Digits, “a personal AI supercomputer” that will bring Nvidia’s Blackwell AI platform to the masses. Want to run inference on your desktop, offline? Digits and the GB10 “AI superchip” are going to make that possible—with 1 petaflop (1,000 TOPS) of performance. The system will cost a minimum of $3,000, with availability in May 2025.
In your inbox: Upgrade your life with WIRED-tested gear
“Wi-Fi keeps going down”: Trump’s RTO mandate is going terribly
Big Story: The worm that no computer scientist can crack
Yuval Noah Harari: “Prepare to share the planet with AI superintelligence”
Uncanny Valley: An insider look at the influence of Silicon Valley
$50 Off In-Person Tax Prep When You Switch From Your Tax Current Provider
Exclusive: Up To 50% Off 6 Boxes With Factor Promo Code
© 2025 Condé Nast. All rights reserved. WIRED may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Condé Nast. Ad Choices"
